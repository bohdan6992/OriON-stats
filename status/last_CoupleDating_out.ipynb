{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ec98a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-28\"\n",
    "output_dir = \"C:\\\\datum-api-examples-main\\\\OriON\\\\signals\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e5e95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-01\"  # papermill replacement\n",
    "import os\n",
    "output_dir = os.environ.get(\"ORION_SIGNALS_DIR\", \"../signals\")\n",
    "config_path = os.environ.get(\"DATUM_API_CONFIG_PATH\", \"../ops/datum_api_config.json\")\n",
    "dry_run = False\n",
    "\n",
    "# ensure output exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b328423",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import pandas as pd\n",
    "from datum_api_client import DatumApi\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pip install xlrd\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4562fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "def _floor_time_to_bucket(dt: pd.Series, minutes: int) -> pd.Series:\n",
    "    if minutes <= 1:\n",
    "        return dt.dt.floor(\"min\")\n",
    "    return dt.dt.floor(f\"{minutes}min\")\n",
    "\n",
    "def _connected_components(nodes: List[str], edges: Dict[Tuple[str, str], float]) -> List[List[str]]:\n",
    "    adj = {n: [] for n in nodes}\n",
    "    for (a, b) in edges.keys():\n",
    "        adj[a].append(b)\n",
    "        adj[b].append(a)\n",
    "    seen = set()\n",
    "    comps = []\n",
    "    for n in nodes:\n",
    "        if n in seen:\n",
    "            continue\n",
    "        stack = [n]\n",
    "        seen.add(n)\n",
    "        comp = []\n",
    "        while stack:\n",
    "            cur = stack.pop()\n",
    "            comp.append(cur)\n",
    "            for nb in adj[cur]:\n",
    "                if nb not in seen:\n",
    "                    seen.add(nb)\n",
    "                    stack.append(nb)\n",
    "        comps.append(sorted(comp))\n",
    "    return comps\n",
    "\n",
    "def _count_divergence_reversion(z: np.ndarray, enter: float, exit: float) -> Tuple[int, int, List[float]]:\n",
    "    \"\"\"\n",
    "    divergence event: first time |z| >= enter\n",
    "    converged: later returns to |z| <= exit (within same day window)\n",
    "    strength: max |z| during the event\n",
    "    \"\"\"\n",
    "    z = z[np.isfinite(z)]\n",
    "    if z.size < 30:\n",
    "        return 0, 0, []\n",
    "    events = 0\n",
    "    succ = 0\n",
    "    strengths = []\n",
    "    i = 0\n",
    "    n = z.size\n",
    "    while i < n:\n",
    "        if abs(z[i]) >= enter:\n",
    "            events += 1\n",
    "            max_abs = abs(z[i])\n",
    "            j = i + 1\n",
    "            reverted = False\n",
    "            while j < n:\n",
    "                max_abs = max(max_abs, abs(z[j]))\n",
    "                if abs(z[j]) <= exit:\n",
    "                    reverted = True\n",
    "                    break\n",
    "                j += 1\n",
    "            strengths.append(float(max_abs))\n",
    "            if reverted:\n",
    "                succ += 1\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return events, succ, strengths\n",
    "\n",
    "def find_stack_clusters_and_pairs_full_stats(\n",
    "    path: str,\n",
    "    *,\n",
    "    file_type: str = \"parquet\",     # \"parquet\" | \"csv\"\n",
    "    dt_col: str = \"dt\",\n",
    "    ticker_col: str = \"ticker\",\n",
    "    stack_col: str = \"Stack%\",\n",
    "    beta_col: str = \"beta\",\n",
    "    sigma_col: str = \"sigma\",\n",
    "    bench_col: str = \"bench\",\n",
    "\n",
    "    # window:\n",
    "    t_from: str = \"04:00\",\n",
    "    t_to: str = \"09:30\",\n",
    "    bucket_minutes: int = 5,\n",
    "\n",
    "    # edge constraints (для \"щільно ходять разом\"):\n",
    "    corr_threshold: float = 0.70,\n",
    "    sign_agree_threshold: float = 0.60,\n",
    "    beta_tol: float = 0.50,\n",
    "    sigma_tol: float = 0.80,\n",
    "\n",
    "    # sufficiency:\n",
    "    min_points_per_ticker: int = 800,\n",
    "    min_overlap_points_pair: int = 400,\n",
    "\n",
    "    # divergence/reversion (within 04:00–09:30):\n",
    "    z_enter: float = 2.0,\n",
    "    z_exit: float = 0.5,\n",
    "    min_div_events_pair: int = 3,        # відсіяти \"noise\" пари\n",
    "    min_div_events_cluster: int = 10,    # відсіяти слабкі кластери\n",
    "\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      clusters_df: 1 row per cluster with aggregated arb stats\n",
    "      pairs_df:    1 row per pair with full arb stats + corr/sign/meta\n",
    "    Both are bench-separated and sorted by succ_to_fail_ratio desc.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- load minimal cols ---\n",
    "    use_cols = [dt_col, ticker_col, stack_col, beta_col, sigma_col, bench_col]\n",
    "    if file_type.lower() == \"parquet\":\n",
    "        df = pd.read_parquet(path, columns=use_cols)\n",
    "    else:\n",
    "        df = pd.read_csv(path, usecols=use_cols)\n",
    "\n",
    "    df = df.dropna(subset=[dt_col, ticker_col, stack_col, bench_col]).copy()\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[dt_col]).copy()\n",
    "\n",
    "    df[stack_col] = pd.to_numeric(df[stack_col], errors=\"coerce\")\n",
    "    df[beta_col] = pd.to_numeric(df[beta_col], errors=\"coerce\")\n",
    "    df[sigma_col] = pd.to_numeric(df[sigma_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[stack_col]).copy()\n",
    "\n",
    "    # --- filter time window ---\n",
    "    df[\"date\"] = df[dt_col].dt.date\n",
    "    df[\"time\"] = df[dt_col].dt.time\n",
    "    t_from_t = pd.to_datetime(t_from).time()\n",
    "    t_to_t = pd.to_datetime(t_to).time()\n",
    "    df = df[(df[\"time\"] >= t_from_t) & (df[\"time\"] <= t_to_t)].copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # bucket to reduce noise\n",
    "    df[\"dt_bucket\"] = _floor_time_to_bucket(df[dt_col], bucket_minutes)\n",
    "\n",
    "    all_pairs_rows = []\n",
    "    all_clusters_rows = []\n",
    "\n",
    "    # ===== bench loop =====\n",
    "    for bench, df_b in df.groupby(bench_col):\n",
    "        # per-ticker meta (robust)\n",
    "        meta = (\n",
    "            df_b.groupby(ticker_col)[[beta_col, sigma_col]]\n",
    "            .median(numeric_only=True)\n",
    "            .reset_index()\n",
    "        )\n",
    "        meta_map = meta.set_index(ticker_col).to_dict(orient=\"index\")\n",
    "\n",
    "        # pivot for this bench: (date, bucket) x ticker\n",
    "        piv = (\n",
    "            df_b.groupby([\"date\", \"dt_bucket\", ticker_col])[stack_col].mean()\n",
    "            .reset_index()\n",
    "            .pivot(index=[\"date\", \"dt_bucket\"], columns=ticker_col, values=stack_col)\n",
    "            .sort_index()\n",
    "        )\n",
    "        if piv.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # drop tickers with too few points\n",
    "        counts = piv.notna().sum(axis=0)\n",
    "        keep = counts[counts >= min_points_per_ticker].index.tolist()\n",
    "        piv = piv[keep]\n",
    "        if piv.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        tickers = piv.columns.tolist()\n",
    "        dpiv = piv.diff()\n",
    "        dates = piv.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "        # ---- build edges and compute pair stats (incl arb) ----\n",
    "        edges: Dict[Tuple[str, str], float] = {}\n",
    "        pair_rows_bench = []\n",
    "\n",
    "        for i, a in enumerate(tickers):\n",
    "            xa_all = piv[a].to_numpy(dtype=float)\n",
    "            dxa_all = dpiv[a].to_numpy(dtype=float)\n",
    "            ma = meta_map.get(a, {})\n",
    "\n",
    "            for j in range(i + 1, len(tickers)):\n",
    "                b = tickers[j]\n",
    "                mb = meta_map.get(b, {})\n",
    "\n",
    "                beta_a, beta_b = ma.get(beta_col, np.nan), mb.get(beta_col, np.nan)\n",
    "                sig_a, sig_b = ma.get(sigma_col, np.nan), mb.get(sigma_col, np.nan)\n",
    "\n",
    "                # meta similarity filter\n",
    "                if np.isfinite(beta_a) and np.isfinite(beta_b) and abs(beta_a - beta_b) > beta_tol:\n",
    "                    continue\n",
    "                if np.isfinite(sig_a) and np.isfinite(sig_b) and abs(sig_a - sig_b) > sigma_tol:\n",
    "                    continue\n",
    "\n",
    "                xb_all = piv[b].to_numpy(dtype=float)\n",
    "                dxb_all = dpiv[b].to_numpy(dtype=float)\n",
    "\n",
    "                # overlap check on levels\n",
    "                m = np.isfinite(xa_all) & np.isfinite(xb_all)\n",
    "                overlap = int(m.sum())\n",
    "                if overlap < min_overlap_points_pair:\n",
    "                    continue\n",
    "\n",
    "                corr = np.corrcoef(xa_all[m], xb_all[m])[0, 1]\n",
    "                if not np.isfinite(corr) or corr < corr_threshold:\n",
    "                    continue\n",
    "\n",
    "                # sign agreement on diffs\n",
    "                md = np.isfinite(dxa_all) & np.isfinite(dxb_all)\n",
    "                if int(md.sum()) < int(min_overlap_points_pair * 0.6):\n",
    "                    continue\n",
    "                agree = np.mean(((dxa_all[md] > 0) & (dxb_all[md] > 0)) | ((dxa_all[md] < 0) & (dxb_all[md] < 0)))\n",
    "                if not np.isfinite(agree) or agree < sign_agree_threshold:\n",
    "                    continue\n",
    "\n",
    "                # passed \"tight movement\" filters -> edge in graph\n",
    "                edges[(a, b)] = float(corr)\n",
    "\n",
    "                # --- pair-level arb stats: sum per-day divergence/reversion in spread zscore ---\n",
    "                total_events = 0\n",
    "                total_succ = 0\n",
    "                strengths_all: List[float] = []\n",
    "\n",
    "                for d in dates:\n",
    "                    day_slice = piv.loc[d]\n",
    "                    xa = day_slice[a].to_numpy(dtype=float)\n",
    "                    xb = day_slice[b].to_numpy(dtype=float)\n",
    "                    mm = np.isfinite(xa) & np.isfinite(xb)\n",
    "                    if int(mm.sum()) < 30:\n",
    "                        continue\n",
    "\n",
    "                    spread = xa[mm] - xb[mm]\n",
    "                    mu = spread.mean()\n",
    "                    sd = spread.std(ddof=0)\n",
    "                    if sd <= 0 or not np.isfinite(sd):\n",
    "                        continue\n",
    "                    z = (spread - mu) / sd\n",
    "\n",
    "                    ev, su, strengths = _count_divergence_reversion(z, z_enter, z_exit)\n",
    "                    total_events += ev\n",
    "                    total_succ += su\n",
    "                    strengths_all.extend(strengths)\n",
    "\n",
    "                fails = total_events - total_succ\n",
    "                succ_rate = (total_succ / total_events) if total_events > 0 else np.nan\n",
    "                ratio = (total_succ / fails) if fails > 0 else (np.inf if total_succ > 0 else np.nan)\n",
    "\n",
    "                if total_events < min_div_events_pair:\n",
    "                    # можна не відсікати, але так чистіше для репорту\n",
    "                    continue\n",
    "\n",
    "                strength_avg = float(np.mean(strengths_all)) if strengths_all else np.nan\n",
    "                strength_med = float(np.median(strengths_all)) if strengths_all else np.nan\n",
    "                strength_p90 = float(np.quantile(strengths_all, 0.90)) if strengths_all else np.nan\n",
    "\n",
    "                # a simple pair score: prioritize reversion, then tightness\n",
    "                # (ти можеш підкрутити ваги)\n",
    "                score = (\n",
    "                    0.55 * (succ_rate if np.isfinite(succ_rate) else 0.0)\n",
    "                    + 0.25 * float(corr)\n",
    "                    + 0.20 * float(agree)\n",
    "                )\n",
    "                # reward bigger divergence (more edge) but softly\n",
    "                if np.isfinite(strength_avg):\n",
    "                    score += 0.03 * min(5.0, strength_avg)\n",
    "\n",
    "                pair_rows_bench.append({\n",
    "                    \"bench\": bench,\n",
    "                    \"a\": a, \"b\": b,\n",
    "                    \"corr\": float(corr),\n",
    "                    \"sign_agree\": float(agree),\n",
    "                    \"overlap_points\": overlap,\n",
    "                    \"beta_a\": float(beta_a) if np.isfinite(beta_a) else np.nan,\n",
    "                    \"beta_b\": float(beta_b) if np.isfinite(beta_b) else np.nan,\n",
    "                    \"sigma_a\": float(sig_a) if np.isfinite(sig_a) else np.nan,\n",
    "                    \"sigma_b\": float(sig_b) if np.isfinite(sig_b) else np.nan,\n",
    "                    \"beta_diff\": abs(beta_a - beta_b) if np.isfinite(beta_a) and np.isfinite(beta_b) else np.nan,\n",
    "                    \"sigma_diff\": abs(sig_a - sig_b) if np.isfinite(sig_a) and np.isfinite(sig_b) else np.nan,\n",
    "\n",
    "                    \"pair_div_events\": int(total_events),\n",
    "                    \"pair_div_converged\": int(total_succ),\n",
    "                    \"pair_div_not_converged\": int(fails),\n",
    "                    \"pair_success_rate\": float(succ_rate) if np.isfinite(succ_rate) else np.nan,\n",
    "                    \"pair_succ_to_fail_ratio\": float(ratio) if np.isfinite(ratio) else np.nan,\n",
    "                    \"pair_strength_avg_max_abs_z\": strength_avg,\n",
    "                    \"pair_strength_median_max_abs_z\": strength_med,\n",
    "                    \"pair_strength_p90_max_abs_z\": strength_p90,\n",
    "\n",
    "                    \"pair_score\": float(score),\n",
    "                })\n",
    "\n",
    "        if not pair_rows_bench:\n",
    "            continue\n",
    "\n",
    "        pairs_df_b = pd.DataFrame(pair_rows_bench)\n",
    "        all_pairs_rows.append(pairs_df_b)\n",
    "\n",
    "        # ---- clusters from edges ----\n",
    "        if not edges:\n",
    "            continue\n",
    "\n",
    "        comps = _connected_components(tickers, edges)\n",
    "        comps = [c for c in comps if len(c) >= 2]\n",
    "        if not comps:\n",
    "            continue\n",
    "\n",
    "        # map pair -> cluster id (within bench) by membership\n",
    "        for cid, members in enumerate(comps, start=1):\n",
    "            mset = set(members)\n",
    "            sub_pairs = pairs_df_b[pairs_df_b[\"a\"].isin(mset) & pairs_df_b[\"b\"].isin(mset)].copy()\n",
    "            if sub_pairs.empty:\n",
    "                continue\n",
    "\n",
    "            # cluster-level divergence stats: sum over pairs (not unique events; that's ok for ranking)\n",
    "            div_events = int(sub_pairs[\"pair_div_events\"].sum())\n",
    "            div_conv = int(sub_pairs[\"pair_div_converged\"].sum())\n",
    "            div_fail = int(sub_pairs[\"pair_div_not_converged\"].sum())\n",
    "\n",
    "            if div_events < min_div_events_cluster:\n",
    "                continue\n",
    "\n",
    "            succ_rate = (div_conv / div_events) if div_events > 0 else np.nan\n",
    "            ratio = (div_conv / div_fail) if div_fail > 0 else (np.inf if div_conv > 0 else np.nan)\n",
    "\n",
    "            betas = [meta_map.get(t, {}).get(beta_col, np.nan) for t in members]\n",
    "            sigmas = [meta_map.get(t, {}).get(sigma_col, np.nan) for t in members]\n",
    "\n",
    "            all_clusters_rows.append({\n",
    "                \"bench\": bench,\n",
    "                \"cluster_id_in_bench\": cid,\n",
    "                \"n_members\": len(members),\n",
    "                \"members\": \",\".join(members),\n",
    "\n",
    "                \"pairs_count\": int(sub_pairs.shape[0]),\n",
    "                \"mean_pair_corr\": float(sub_pairs[\"corr\"].mean()),\n",
    "                \"mean_pair_sign_agree\": float(sub_pairs[\"sign_agree\"].mean()),\n",
    "                \"mean_pair_score\": float(sub_pairs[\"pair_score\"].mean()),\n",
    "\n",
    "                \"beta_mean\": float(np.nanmean(betas)),\n",
    "                \"beta_std\": float(np.nanstd(betas)),\n",
    "                \"sigma_mean\": float(np.nanmean(sigmas)),\n",
    "                \"sigma_std\": float(np.nanstd(sigmas)),\n",
    "\n",
    "                \"div_events\": div_events,\n",
    "                \"div_converged\": div_conv,\n",
    "                \"div_not_converged\": div_fail,\n",
    "                \"div_success_rate\": float(succ_rate) if np.isfinite(succ_rate) else np.nan,\n",
    "                \"succ_to_fail_ratio\": float(ratio) if np.isfinite(ratio) else np.nan,\n",
    "\n",
    "                \"div_strength_avg_max_abs_z\": float(sub_pairs[\"pair_strength_avg_max_abs_z\"].mean()),\n",
    "                \"div_strength_median_max_abs_z\": float(sub_pairs[\"pair_strength_median_max_abs_z\"].median()),\n",
    "                \"div_strength_p90_max_abs_z\": float(sub_pairs[\"pair_strength_p90_max_abs_z\"].mean()),\n",
    "            })\n",
    "\n",
    "    pairs_df = pd.concat(all_pairs_rows, ignore_index=True) if all_pairs_rows else pd.DataFrame()\n",
    "    clusters_df = pd.DataFrame(all_clusters_rows)\n",
    "\n",
    "    # ---- sorting ----\n",
    "    if not pairs_df.empty:\n",
    "        pairs_df = pairs_df.sort_values(\n",
    "            [\"bench\", \"pair_succ_to_fail_ratio\", \"pair_success_rate\", \"pair_score\", \"corr\"],\n",
    "            ascending=[True, False, False, False, False],\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    if not clusters_df.empty:\n",
    "        clusters_df = clusters_df.sort_values(\n",
    "            [\"bench\", \"succ_to_fail_ratio\", \"div_converged\", \"mean_pair_score\", \"mean_pair_corr\"],\n",
    "            ascending=[True, False, False, False, False],\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    return clusters_df, pairs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af038a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def save_cluster_pair_reports(clusters_df: pd.DataFrame, pairs_df: pd.DataFrame, output_dir: str, *, top_n: int = 200):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     clusters_path = os.path.join(output_dir, \"clusters_top.csv\")\n",
    "#     pairs_full_path = os.path.join(output_dir, \"pairs_full.csv\")\n",
    "#     pairs_top_path = os.path.join(output_dir, \"pairs_top.csv\")\n",
    "\n",
    "#     if clusters_df is not None and not clusters_df.empty:\n",
    "#         clusters_df.to_csv(clusters_path, index=False)\n",
    "#     else:\n",
    "#         pd.DataFrame().to_csv(clusters_path, index=False)\n",
    "\n",
    "#     if pairs_df is not None and not pairs_df.empty:\n",
    "#         pairs_df.to_csv(pairs_full_path, index=False)\n",
    "\n",
    "#         top = pairs_df.head(top_n).copy()\n",
    "#         top.to_csv(pairs_top_path, index=False)\n",
    "#     else:\n",
    "#         pd.DataFrame().to_csv(pairs_full_path, index=False)\n",
    "#         pd.DataFrame().to_csv(pairs_top_path, index=False)\n",
    "\n",
    "#     print(\"Saved:\")\n",
    "#     print(\" \", clusters_path)\n",
    "#     print(\" \", pairs_full_path)\n",
    "#     print(\" \", pairs_top_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d69fb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clusters_df, pairs_df = find_stack_clusters_and_pairs_full_stats(\n",
    "#     path=\"ARBITRAGE/final_filtered.parquet\",\n",
    "#     file_type=\"parquet\",\n",
    "\n",
    "#     t_from=\"04:00\",\n",
    "#     t_to=\"09:30\",\n",
    "#     bucket_minutes=5,\n",
    "\n",
    "#     corr_threshold=0.70,\n",
    "#     sign_agree_threshold=0.60,\n",
    "#     beta_tol=0.50,\n",
    "#     sigma_tol=0.80,\n",
    "\n",
    "#     min_points_per_ticker=800,\n",
    "#     min_overlap_points_pair=400,\n",
    "\n",
    "#     z_enter=2.0,\n",
    "#     z_exit=0.5,\n",
    "#     min_div_events_pair=3,\n",
    "#     min_div_events_cluster=10,\n",
    "# )\n",
    "\n",
    "# print(\"clusters:\", len(clusters_df))\n",
    "# print(\"pairs:\", len(pairs_df))\n",
    "\n",
    "# # топ кластерів\n",
    "# print(clusters_df.head(20))\n",
    "\n",
    "# # топ пар (вже відсортовано всередині bench по ratio/success/score)\n",
    "# print(pairs_df.head(30))\n",
    "\n",
    "# save_cluster_pair_reports(\n",
    "#     clusters_df,\n",
    "#     pairs_df,\n",
    "#     output_dir=\"ARBITRAGE/open_analysis_fast/cluster_pairs_report\",\n",
    "#     top_n=300,\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.153715,
   "end_time": "2026-01-28T18:00:39.845231",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\datum-api-examples-main\\OriON\\STRATEGIES\\notebooks\\CoupleDating.ipynb",
   "output_path": "C:\\datum-api-examples-main\\OriON\\status\\last_CoupleDating_out.ipynb",
   "parameters": {
    "output_dir": "C:\\datum-api-examples-main\\OriON\\signals",
    "run_date": "2026-01-28"
   },
   "start_time": "2026-01-28T18:00:39.691516",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}