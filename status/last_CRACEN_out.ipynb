{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ba80b0",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [21]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a9d4d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:37.306449Z",
     "iopub.status.busy": "2026-01-27T21:13:37.306449Z",
     "iopub.status.idle": "2026-01-27T21:13:37.326428Z",
     "shell.execute_reply": "2026-01-27T21:13:37.326428Z"
    },
    "papermill": {
     "duration": 0.031868,
     "end_time": "2026-01-27T21:13:37.326428",
     "exception": false,
     "start_time": "2026-01-27T21:13:37.294560",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-27\"\n",
    "output_dir = \"C:\\\\Users\\\\sergi\\\\OneDrive\\\\\\u0420\\u0430\\u0431\\u043e\\u0447\\u0438\\u0439 \\u0441\\u0442\\u043e\\u043b\\\\ORION_MAIN\\\\OriON\\\\signals\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8fcad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:37.343299Z",
     "iopub.status.busy": "2026-01-27T21:13:37.343299Z",
     "iopub.status.idle": "2026-01-27T21:13:37.358541Z",
     "shell.execute_reply": "2026-01-27T21:13:37.357669Z"
    },
    "papermill": {
     "duration": 0.033241,
     "end_time": "2026-01-27T21:13:37.359669",
     "exception": false,
     "start_time": "2026-01-27T21:13:37.326428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-01\"  # papermill replacement\n",
    "import os\n",
    "output_dir = os.environ.get(\"ORION_SIGNALS_DIR\", \"../signals\")\n",
    "config_path = os.environ.get(\"DATUM_API_CONFIG_PATH\", \"../ops/datum_api_config.json\")\n",
    "dry_run = False\n",
    "\n",
    "# ensure output exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0381d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:37.371637Z",
     "iopub.status.busy": "2026-01-27T21:13:37.371637Z",
     "iopub.status.idle": "2026-01-27T21:13:38.326931Z",
     "shell.execute_reply": "2026-01-27T21:13:38.326931Z"
    },
    "papermill": {
     "duration": 0.960755,
     "end_time": "2026-01-27T21:13:38.326931",
     "exception": false,
     "start_time": "2026-01-27T21:13:37.366176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import pandas as pd\n",
    "from datum_api_client import DatumApi\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pip install xlrd\n",
    "# pip install openpyxl\n",
    "\n",
    "# Download excel with ticker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14223c43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:38.344019Z",
     "iopub.status.busy": "2026-01-27T21:13:38.344019Z",
     "iopub.status.idle": "2026-01-27T21:13:38.371854Z",
     "shell.execute_reply": "2026-01-27T21:13:38.371854Z"
    },
    "papermill": {
     "duration": 0.044923,
     "end_time": "2026-01-27T21:13:38.371854",
     "exception": false,
     "start_time": "2026-01-27T21:13:38.326931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORION_HOME: C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\n",
      "CRACEN_DIR : C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\CRACEN\n",
      "WORK_DIR  : C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\CRACEN\\work\n",
      "FINAL_PATH: C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\CRACEN\\final.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_orion_home() -> Path:\n",
    "    # 1) –Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–µ ‚Äî env var\n",
    "    env = os.environ.get(\"ORION_HOME\")\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"ORION_HOME points to missing path: {p}\")\n",
    "        return p\n",
    "\n",
    "    # 2) fallback: —à—É–∫–∞—î–º–æ –ø–∞–ø–∫—É OriON –≤–≥–æ—Ä—É –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó\n",
    "    here = Path.cwd().resolve()\n",
    "    for parent in [here] + list(here.parents):\n",
    "        if parent.name.lower() == \"orion\":\n",
    "            return parent\n",
    "        cand = parent / \"OriON\"\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            return cand.resolve()\n",
    "\n",
    "    raise RuntimeError(\"Cannot locate OriON. Set ORION_HOME env var.\")\n",
    "\n",
    "ORION_HOME = find_orion_home()\n",
    "\n",
    "CRACEN_DIR = ORION_HOME / \"CRACEN\"\n",
    "WORK_DIR   = CRACEN_DIR / \"work\"     # –±–∞—Ç—á—ñ/–º–∞–Ω—ñ—Ñ–µ—Å—Ç–∏/—Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏\n",
    "FINAL_PATH = CRACEN_DIR / \"final.parquet\"\n",
    "\n",
    "CRACEN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_DIR = WORK_DIR / \"CRACEN_batch\"\n",
    "BATCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ETF_DIR = WORK_DIR / \"ETF\"\n",
    "ETF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"ORION_HOME:\", ORION_HOME)\n",
    "print(\"CRACEN_DIR :\", CRACEN_DIR)\n",
    "print(\"WORK_DIR  :\", WORK_DIR)\n",
    "print(\"FINAL_PATH:\", FINAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1445ad42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:38.387749Z",
     "iopub.status.busy": "2026-01-27T21:13:38.387749Z",
     "iopub.status.idle": "2026-01-27T21:13:38.404573Z",
     "shell.execute_reply": "2026-01-27T21:13:38.403680Z"
    },
    "papermill": {
     "duration": 0.032719,
     "end_time": "2026-01-27T21:13:38.404573",
     "exception": false,
     "start_time": "2026-01-27T21:13:38.371854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïì –ü–æ–≤–Ω—ñ –¥–∞—Ç–∏ (–∑ —á–∞—Å–æ–º):\n",
      "start = 2025-10-26 00:00:00\n",
      "end   = 2026-01-26 23:59:59\n",
      "\n",
      "üìÖ –ö–æ—Ä–æ—Ç–∫—ñ –¥–∞—Ç–∏ (–±–µ–∑ —á–∞—Å—É):\n",
      "start_date_str = 2025-10-26\n",
      "end_date_str   = 2026-01-26\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, date, time\n",
    "from calendar import monthrange\n",
    "\n",
    "# === –¢–ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: Europe/Kyiv ===\n",
    "tz = None  # —è–∫—â–æ –Ω–µ–º–∞ zoneinfo, –≤—Å–µ –æ–¥–Ω–æ –ø—Ä–∞—Ü—é—î –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "\n",
    "now_local = datetime.now(tz) if tz else datetime.now()\n",
    "yesterday = (now_local - timedelta(days=1)).date()\n",
    "\n",
    "# —Ç—Ä–∏ –º—ñ—Å—è—Ü—ñ –Ω–∞–∑–∞–¥ –≤—ñ–¥ —É—á–æ—Ä–∞ (–∫–æ—Ä–µ–∫—Ç–Ω–æ –¥–ª—è —Ä—ñ–∑–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ –º—ñ—Å—è—Ü—ñ–≤)\n",
    "try:\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    start_date = (datetime.combine(yesterday, time(0, 0, 0)) - relativedelta(months=3)).date()\n",
    "except Exception:\n",
    "    # fallback –±–µ–∑ dateutil\n",
    "    y, m, d = yesterday.year, yesterday.month - 3, yesterday.day\n",
    "    while m <= 0:\n",
    "        m += 12\n",
    "        y -= 1\n",
    "    d = min(d, monthrange(y, m)[1])\n",
    "    start_date = date(y, m, d)\n",
    "\n",
    "# —Ñ–æ—Ä–º–∞—Ç–∏ –∑ —á–∞—Å–æ–º\n",
    "start = f\"{start_date:%Y-%m-%d} 00:00:00\"\n",
    "end   = f\"{yesterday:%Y-%m-%d} 23:59:59\"\n",
    "\n",
    "# —Ñ–æ—Ä–º–∞—Ç–∏ –±–µ–∑ —á–∞—Å—É\n",
    "start_date_str = f\"{start_date:%Y-%m-%d}\"\n",
    "end_date_str   = f\"{yesterday:%Y-%m-%d}\"\n",
    "\n",
    "# –≤–∏–≤—ñ–¥\n",
    "print(\"üïì –ü–æ–≤–Ω—ñ –¥–∞—Ç–∏ (–∑ —á–∞—Å–æ–º):\")\n",
    "print(\"start =\", start)\n",
    "print(\"end   =\", end)\n",
    "print(\"\\nüìÖ –ö–æ—Ä–æ—Ç–∫—ñ –¥–∞—Ç–∏ (–±–µ–∑ —á–∞—Å—É):\")\n",
    "print(\"start_date_str =\", start_date_str)\n",
    "print(\"end_date_str   =\", end_date_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d55475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:38.419543Z",
     "iopub.status.busy": "2026-01-27T21:13:38.419543Z",
     "iopub.status.idle": "2026-01-27T21:13:42.310060Z",
     "shell.execute_reply": "2026-01-27T21:13:42.310060Z"
    },
    "papermill": {
     "duration": 3.905487,
     "end_time": "2026-01-27T21:13:42.310060",
     "exception": false,
     "start_time": "2026-01-27T21:13:38.404573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature has expired\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker       value\n",
      "0      A    53276.79\n",
      "1     AA  1196849.64\n",
      "2    AAA        0.00\n",
      "3   AAAA        0.00\n",
      "4   AAAC        0.00\n",
      "–†—è–¥–∫—ñ–≤: 12520\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {\n",
    "    \"date_as_of\": yesterday,\n",
    "    \"format\": \"json_records\"\n",
    "}\n",
    "\n",
    "df = DatumApi.data_request(\"/calculations/median_premarket_value_traded_ex_finr_3m\", params)\n",
    "\n",
    "print(df.head())\n",
    "print(\"–†—è–¥–∫—ñ–≤:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc041014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:42.327407Z",
     "iopub.status.busy": "2026-01-27T21:13:42.327407Z",
     "iopub.status.idle": "2026-01-27T21:13:43.637960Z",
     "shell.execute_reply": "2026-01-27T21:13:43.637960Z"
    },
    "papermill": {
     "duration": 1.318562,
     "end_time": "2026-01-27T21:13:43.637960",
     "exception": false,
     "start_time": "2026-01-27T21:13:42.319398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª–∏—à–∏–ª–æ—Å—å: 6,698 —Ä—è–¥–∫—ñ–≤\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>value</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>country</th>\n",
       "      <th>exchange</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>53276.79</td>\n",
       "      <td>Medical Equipment &amp; Devices</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>New York</td>\n",
       "      <td>38456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>1196849.64</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>New York</td>\n",
       "      <td>15162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>1291460.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>36.46</td>\n",
       "      <td>Software</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NASDAQ CM</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACI</td>\n",
       "      <td>16875.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       value                         lvl3        country   exchange  \\\n",
       "0      A    53276.79  Medical Equipment & Devices  UNITED STATES   New York   \n",
       "1     AA  1196849.64              Metals & Mining  UNITED STATES   New York   \n",
       "2   AAAU  1291460.13                          NaN            NaN        NaN   \n",
       "3   AACG       36.46                     Software  UNITED STATES  NASDAQ CM   \n",
       "4   AACI    16875.54                          NaN            NaN        NaN   \n",
       "\n",
       "   market_cap  \n",
       "0     38456.0  \n",
       "1     15162.0  \n",
       "2         NaN  \n",
       "3        34.0  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_nonzero(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î –∫–æ–ø—ñ—é –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É –ª–∏—à–µ –∑ —Ç–∏–º–∏ —Ä—è–¥–∫–∞–º–∏,\n",
    "    –¥–µ –∫–æ–ª–æ–Ω–∫–∞ 'value' –Ω–µ –¥–æ—Ä—ñ–≤–Ω—é—î –Ω—É–ª—é.\n",
    "    \"\"\"\n",
    "    if \"value\" not in df.columns:\n",
    "        raise ValueError(\"–£ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ 'value'\")\n",
    "    return df[df[\"value\"].fillna(0) != 0].copy()\n",
    "\n",
    "\n",
    "# --- –∫—Ä–æ–∫ 1: —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ df (–¥–µ 'value' != 0)\n",
    "df_filtered = filter_nonzero(df)\n",
    "\n",
    "# --- –∫—Ä–æ–∫ 2: —Ç—è–≥–Ω–µ–º–æ –¥–∞–Ω—ñ –ø–æ —Ç—ñ–∫–µ—Ä–∞—Ö\n",
    "ticker_params = {\n",
    "    'fields': 'lvl3,country,exchange,market_cap',\n",
    "    'active': True,\n",
    "    'us_exchange': True,\n",
    "    'listed': True\n",
    "}\n",
    "tickers_df = DatumApi.data_request('/tickers', ticker_params)\n",
    "tickers_df = tickers_df.dropna()\n",
    "\n",
    "# --- –∫—Ä–æ–∫ 3: –¥–æ–¥–∞—î–º–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —É df_filtered\n",
    "top1500 = df_filtered.merge(\n",
    "    tickers_df,\n",
    "    how='left',\n",
    "    on='ticker'\n",
    ")\n",
    "\n",
    "print(f\"–ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª–∏—à–∏–ª–æ—Å—å: {len(top1500):,} —Ä—è–¥–∫—ñ–≤\")\n",
    "top1500.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911fc18c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.655995Z",
     "iopub.status.busy": "2026-01-27T21:13:43.655995Z",
     "iopub.status.idle": "2026-01-27T21:13:43.670932Z",
     "shell.execute_reply": "2026-01-27T21:13:43.670148Z"
    },
    "papermill": {
     "duration": 0.032972,
     "end_time": "2026-01-27T21:13:43.670932",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.637960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>value</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>country</th>\n",
       "      <th>exchange</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>JD</td>\n",
       "      <td>3277606.26</td>\n",
       "      <td>E-Commerce Discretionary</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NASDAQ GS</td>\n",
       "      <td>40531.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker       value                      lvl3        country   exchange  \\\n",
       "3215     JD  3277606.26  E-Commerce Discretionary  UNITED STATES  NASDAQ GS   \n",
       "\n",
       "      market_cap  \n",
       "3215     40531.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1500[top1500[\"ticker\"] == \"JD\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a5e7d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.688692Z",
     "iopub.status.busy": "2026-01-27T21:13:43.687183Z",
     "iopub.status.idle": "2026-01-27T21:13:43.716742Z",
     "shell.execute_reply": "2026-01-27T21:13:43.716742Z"
    },
    "papermill": {
     "duration": 0.04581,
     "end_time": "2026-01-27T21:13:43.716742",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.670932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_lvl2(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –∑ –∫–æ–ª–æ–Ω–∫–∏ 'lvl3' —É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ñ.\n",
    "    –ü—Ä–æ–ø—É—Å–∫–∏ (NaN) —ñ–≥–Ω–æ—Ä—É—é—Ç—å—Å—è.\n",
    "    \"\"\"\n",
    "    if \"lvl3\" not in df.columns:\n",
    "        raise ValueError(\"–£ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ 'lvl3'\")\n",
    "    \n",
    "    unique_vals = sorted(df[\"lvl3\"].dropna().unique().tolist())\n",
    "    print(f\"üîπ –ó–Ω–∞–π–¥–µ–Ω–æ {len(unique_vals)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É –∫–æ–ª–æ–Ω—Ü—ñ 'lvl3'\")\n",
    "    return unique_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50851126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.732384Z",
     "iopub.status.busy": "2026-01-27T21:13:43.732384Z",
     "iopub.status.idle": "2026-01-27T21:13:43.764905Z",
     "shell.execute_reply": "2026-01-27T21:13:43.764405Z"
    },
    "papermill": {
     "duration": 0.048163,
     "end_time": "2026-01-27T21:13:43.764905",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.716742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ –ó–Ω–∞–π–¥–µ–Ω–æ 58 —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É –∫–æ–ª–æ–Ω—Ü—ñ 'lvl3'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Advertising & Marketing',\n",
       " 'Aerospace & Defense',\n",
       " 'Apparel & Textile Products',\n",
       " 'Asset Management',\n",
       " 'Automotive',\n",
       " 'Banking',\n",
       " 'Beverages',\n",
       " 'Biotech & Pharma',\n",
       " 'Cable & Satellite',\n",
       " 'Chemicals',\n",
       " 'Commercial Support Services',\n",
       " 'Construction Materials',\n",
       " 'Consumer Services',\n",
       " 'Containers & Packaging',\n",
       " 'Diversified Industrials',\n",
       " 'E-Commerce Discretionary',\n",
       " 'Electric Utilities',\n",
       " 'Electrical Equipment',\n",
       " 'Engineering & Construction',\n",
       " 'Entertainment Content',\n",
       " 'Food',\n",
       " 'Forestry, Paper & Wood Products',\n",
       " 'Gas & Water Utilities',\n",
       " 'Health Care Facilities & Svcs',\n",
       " 'Home & Office Products',\n",
       " 'Home Construction',\n",
       " 'Household Products',\n",
       " 'IT Services',\n",
       " 'Industrial Intermediate Prod',\n",
       " 'Industrial Support Services',\n",
       " 'Institutional Financial Svcs',\n",
       " 'Insurance',\n",
       " 'Internet Media & Services',\n",
       " 'Leisure Facilities & Services',\n",
       " 'Leisure Products',\n",
       " 'Machinery',\n",
       " 'Medical Equipment & Devices',\n",
       " 'Metals & Mining',\n",
       " 'Oil & Gas Services & Equip',\n",
       " 'Oil & Gas Supply Chain',\n",
       " 'Publishing & Broadcasting',\n",
       " 'REIT',\n",
       " 'Real Estate Owners & Developers',\n",
       " 'Real Estate Services',\n",
       " 'Renewable Energy',\n",
       " 'Retail - Consumer Staples',\n",
       " 'Retail - Discretionary',\n",
       " 'Semiconductors',\n",
       " 'Software',\n",
       " 'Specialty Finance',\n",
       " 'Steel',\n",
       " 'Technology Hardware',\n",
       " 'Telecommunications',\n",
       " 'Tobacco & Cannabis',\n",
       " 'Transportation & Logistics',\n",
       " 'Transportation Equipment',\n",
       " 'Wholesale - Consumer Staples',\n",
       " 'Wholesale - Discretionary']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_lvl2 = get_unique_lvl2(top1500)\n",
    "unique_lvl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455cf91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.780316Z",
     "iopub.status.busy": "2026-01-27T21:13:43.780316Z",
     "iopub.status.idle": "2026-01-27T21:13:43.796965Z",
     "shell.execute_reply": "2026-01-27T21:13:43.796134Z"
    },
    "papermill": {
     "duration": 0.033165,
     "end_time": "2026-01-27T21:13:43.798070",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.764905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sector_to_etf = {\n",
    "    # --- Finance ---\n",
    "    \"Asset Management\": [\"XLF\", \"SPY\", \"BITO\"],\n",
    "    \"Banking\": [\"KRE\", \"XLF\", \"SPY\", \"BITO\"],\n",
    "    \"Institutional Financial Svcs\": [\"XLF\", \"SPY\", \"BITO\"],\n",
    "    \"Insurance\": [\"XLF\", \"SPY\", \"BITO\"],\n",
    "    \"Specialty Finance\": [\"XLF\", \"SPY\", \"BITO\"],\n",
    "\n",
    "    # --- Tech / Internet / Growth ---\n",
    "    \"Software\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "    \"IT Services\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "    \"Semiconductors\": [\"SOXL\", \"QQQ\", \"IWM\", \"SPY\"],\n",
    "    \"Technology Hardware\": [\"QQQ\", \"SOXL\", \"IWM\", \"SPY\"],\n",
    "    \"Internet Media & Services\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"E-Commerce Discretionary\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"Cable & Satellite\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"Telecommunications\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"Publishing & Broadcasting\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"Entertainment Content\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "    \"Advertising & Marketing\": [\"QQQ\", \"KWEB\", \"IWM\", \"SPY\"],\n",
    "\n",
    "    # --- Industrials ---\n",
    "    \"Aerospace & Defense\": [\"IWM\", \"SPY\", \"QQQ\"],\n",
    "    \"Diversified Industrials\": [\"SPY\", \"IWM\"],\n",
    "    \"Industrial Intermediate Prod\": [\"IWM\", \"SPY\"],\n",
    "    \"Industrial Support Services\": [\"IWM\", \"SPY\"],\n",
    "    \"Engineering & Construction\": [\"IWM\", \"SPY\"],\n",
    "    \"Machinery\": [\"IWM\", \"SPY\"],\n",
    "    \"Transportation & Logistics\": [\"IWM\", \"SPY\"],\n",
    "    \"Transportation Equipment\": [\"IWM\", \"SPY\"],\n",
    "    \"Commercial Support Services\": [\"IWM\", \"SPY\"],\n",
    "    \"Containers & Packaging\": [\"IWM\", \"SPY\"],\n",
    "    \"Electrical Equipment\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "\n",
    "    # --- Energy / Materials / Commodities ---\n",
    "    \"Oil & Gas Services & Equip\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Oil & Gas Supply Chain\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Elec & Gas Marketing & Trading\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Electric Utilities\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Gas & Water Utilities\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Renewable Energy\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Chemicals\": [\"XLE\", \"IWM\", \"SPY\"],\n",
    "    \"Metals & Mining\": [\"GDX\", \"IWM\", \"SPY\"],\n",
    "    \"Steel\": [\"GDX\", \"IWM\", \"SPY\"],\n",
    "    \"Construction Materials\": [\"IWM\", \"SPY\"],\n",
    "    \"Forestry, Paper & Wood Products\": [\"IWM\", \"SPY\"],\n",
    "\n",
    "    # --- Health Care ---\n",
    "    \"Biotech & Pharma\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "    \"Medical Equipment & Devices\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "    \"Health Care Facilities & Svcs\": [\"QQQ\", \"IWM\", \"SPY\"],\n",
    "\n",
    "    # --- Consumer Staples / Discretionary ---\n",
    "    \"Food\": [\"SPY\", \"IWM\"],\n",
    "    \"Beverages\": [\"SPY\", \"IWM\"],\n",
    "    \"Household Products\": [\"SPY\", \"IWM\"],\n",
    "    \"Tobacco & Cannabis\": [\"SPY\", \"IWM\"],\n",
    "    \"Retail - Consumer Staples\": [\"SPY\", \"IWM\"],\n",
    "    \"Wholesale - Consumer Staples\": [\"SPY\", \"IWM\"],\n",
    "\n",
    "    \"Retail - Discretionary\": [\"SPY\", \"IWM\", \"QQQ\"],\n",
    "    \"Wholesale - Discretionary\": [\"SPY\", \"IWM\", \"QQQ\"],\n",
    "    \"Consumer Services\": [\"SPY\", \"IWM\"],\n",
    "    \"Leisure Facilities & Services\": [\"SPY\", \"IWM\"],\n",
    "    \"Leisure Products\": [\"SPY\", \"IWM\"],\n",
    "    \"Apparel & Textile Products\": [\"SPY\", \"IWM\", \"QQQ\"],\n",
    "    \"Automotive\": [\"SPY\", \"IWM\", \"QQQ\"],\n",
    "    \"Home & Office Products\": [\"SPY\", \"IWM\"],\n",
    "    \"Home Construction\": [\"IWM\", \"SPY\"],\n",
    "\n",
    "    # --- Real Estate ---\n",
    "    \"REIT\": [\"IWM\", \"SPY\"],\n",
    "    \"Real Estate Owners & Developers\": [\"IWM\", \"SPY\"],\n",
    "    \"Real Estate Services\": [\"IWM\", \"SPY\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb43ed69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.809142Z",
     "iopub.status.busy": "2026-01-27T21:13:43.809142Z",
     "iopub.status.idle": "2026-01-27T21:13:43.825093Z",
     "shell.execute_reply": "2026-01-27T21:13:43.825093Z"
    },
    "papermill": {
     "duration": 0.019001,
     "end_time": "2026-01-27T21:13:43.825093",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.806092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7381c8dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.841119Z",
     "iopub.status.busy": "2026-01-27T21:13:43.841119Z",
     "iopub.status.idle": "2026-01-27T21:13:43.857126Z",
     "shell.execute_reply": "2026-01-27T21:13:43.857126Z"
    },
    "papermill": {
     "duration": 0.022751,
     "end_time": "2026-01-27T21:13:43.857126",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.834375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def corr_beta_best_match_batched(\n",
    "    x_tickers,\n",
    "    y_tickers,\n",
    "    period=\"6 months\",\n",
    "    as_pivot=False,\n",
    "    fmt=\"json_records\",\n",
    "    batch_size=150,\n",
    "    pause=0.15,\n",
    "    max_retries=3,\n",
    "    output_path=None,          # üîπ —Ç–µ–ø–µ—Ä –¥–µ—Ñ–æ–ª—Ç = None\n",
    "):\n",
    "    \"\"\"\n",
    "    –í–∏–∫–ª–∏–∫–∞—î /calculations/corr_beta/v2 –±–∞—Ç—á–∞–º–∏, –æ–±'—î–¥–Ω—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ –æ–¥–∏–Ω DataFrame,\n",
    "    –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ X-—Ç—ñ–∫–µ—Ä–∞ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å Y –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é.\n",
    "\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î (df_all, df_pairs):\n",
    "      - df_all   : —Å–∏—Ä–∏–π –æ–±'—î–¥–Ω–∞–Ω–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∑ API\n",
    "      - df_pairs : –º–∞–ø—ñ–Ω–≥ X -> –Ω–∞–π–∫—Ä–∞—â–∏–π Y\n",
    "    \"\"\"\n",
    "    # --- –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó ---\n",
    "    if not x_tickers:\n",
    "        empty_pairs = pd.DataFrame(\n",
    "            columns=[\"x_ticker\", \"best_y_ticker\", \"best_corr\", \"beta_with_best\"]\n",
    "        )\n",
    "        if output_path:\n",
    "            empty_pairs.to_csv(output_path, index=False)\n",
    "        return pd.DataFrame(), empty_pairs\n",
    "\n",
    "    if not y_tickers:\n",
    "        raise ValueError(\"y_tickers –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º\")\n",
    "\n",
    "    # --- –±–∞—Ç—á–∏–Ω–≥ (—ñ–Ω–¥–µ–∫—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥, —â–æ–± –Ω–µ –≥—É–±–∏—Ç–∏/–¥—É–±–ª—é–≤–∞—Ç–∏ X) ---\n",
    "    y_str = \",\".join(map(str, y_tickers))\n",
    "    results = []\n",
    "    i = 0\n",
    "    n = len(x_tickers)\n",
    "    bs = max(1, int(batch_size))\n",
    "\n",
    "    while i < n:\n",
    "        xs = list(map(str, x_tickers[i : i + bs]))\n",
    "        x_str = \",\".join(xs)\n",
    "\n",
    "        params = {\n",
    "            \"x_tickers\": x_str,\n",
    "            \"y_tickers\": y_str,\n",
    "            \"period\": period,\n",
    "            \"as_pivot\": as_pivot,\n",
    "            \"format\": fmt,\n",
    "        }\n",
    "\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                df_chunk = DatumApi.data_request(\"/calculations/corr_beta/v2\", params)\n",
    "\n",
    "                # –ø—ñ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ —Ç–∏–ø—ñ–≤\n",
    "                if df_chunk is None:\n",
    "                    fetched_ok = False\n",
    "                elif isinstance(df_chunk, pd.DataFrame):\n",
    "                    fetched_ok = not df_chunk.empty\n",
    "                else:\n",
    "                    df_chunk = pd.DataFrame(df_chunk)\n",
    "                    fetched_ok = not df_chunk.empty\n",
    "\n",
    "                if fetched_ok:\n",
    "                    results.append(df_chunk)\n",
    "\n",
    "                i += len(xs)\n",
    "                print(f\"[{i}/{n}] fetched batch of {len(xs)} X —Ç–∏–∫–µ—Ä—ñ–≤\")\n",
    "                time.sleep(pause)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                msg = str(e).lower()\n",
    "                attempt += 1\n",
    "\n",
    "                if \"414\" in msg or \"uri too large\" in msg or \"request-uri too large\" in msg:\n",
    "                    new_bs = max(1, bs // 2)\n",
    "                    if new_bs == bs:\n",
    "                        if attempt >= max_retries:\n",
    "                            print(\n",
    "                                f\"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—é {len(xs)} X-—Ç–∏–∫–µ—Ä—ñ–≤ (i={i}) \"\n",
    "                                f\"–ø—ñ—Å–ª—è {max_retries} 414-–ø–æ–º–∏–ª–æ–∫. –ü–æ–º–∏–ª–∫–∞: {e}\"\n",
    "                            )\n",
    "                            i += len(xs)\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"‚ö†Ô∏è  414 –Ω–∞ i={i}. –ó–º–µ–Ω—à—É—é batch_size {bs} ‚Üí {new_bs} \"\n",
    "                            f\"—ñ —Ñ–æ—Ä–º—É—é –ø–∞—Ä—Ç—ñ—é –∑–∞–Ω–æ–≤–æ.\"\n",
    "                        )\n",
    "                        bs = new_bs\n",
    "                        break\n",
    "                else:\n",
    "                    if attempt >= max_retries:\n",
    "                        print(\n",
    "                            f\"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—é {len(xs)} X-—Ç–∏–∫–µ—Ä—ñ–≤ (i={i}) \"\n",
    "                            f\"–ø—ñ—Å–ª—è {max_retries} —Å–ø—Ä–æ–±. –ü–æ–º–∏–ª–∫–∞: {e}\"\n",
    "                        )\n",
    "                        i += len(xs)\n",
    "                        break\n",
    "                    time.sleep(0.5 * attempt)\n",
    "\n",
    "    df_all = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "    # --- –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫ ---\n",
    "    def _col(options, cols_lower):\n",
    "        for c in options:\n",
    "            if c.lower() in cols_lower:\n",
    "                return next(cc for cc in df_all.columns if cc.lower() == c.lower())\n",
    "        return None\n",
    "\n",
    "    if df_all.empty:\n",
    "        empty_pairs = pd.DataFrame(\n",
    "            columns=[\"x_ticker\", \"best_y_ticker\", \"best_corr\", \"beta_with_best\"]\n",
    "        )\n",
    "        if output_path:\n",
    "            empty_pairs.to_csv(output_path, index=False)\n",
    "        return df_all, empty_pairs\n",
    "\n",
    "    cols_lower = {c.lower() for c in df_all.columns}\n",
    "\n",
    "    x_col    = _col([\"x_ticker\", \"x\", \"ticker_x\", \"subject\", \"symbol_x\"], cols_lower)\n",
    "    y_col    = _col([\"y_ticker\", \"y\", \"ticker_y\", \"benchmark\", \"symbol_y\"], cols_lower)\n",
    "    corr_col = _col([\"corr\", \"correlation\", \"pearson_corr\"], cols_lower)\n",
    "    beta_col = _col([\"beta\", \"beta_coeff\"], cols_lower)\n",
    "\n",
    "    missing = [\n",
    "        name\n",
    "        for name, val in [(\"x_ticker\", x_col), (\"y_ticker\", y_col), (\"corr\", corr_col)]\n",
    "        if val is None\n",
    "    ]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"–í—ñ–¥ API –±—Ä–∞–∫—É—î –æ—á—ñ–∫—É–≤–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫: {', '.join(missing)}. \"\n",
    "            f\"–Ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df_all.columns)}\"\n",
    "        )\n",
    "\n",
    "    # --- –≤–∏–±—ñ—Ä –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ Y –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ X –∑–∞ corr ---\n",
    "    df_sorted = df_all.sort_values(by=corr_col, ascending=False)\n",
    "    idx_best = df_sorted.groupby(x_col, as_index=False)[corr_col].idxmax()[corr_col]\n",
    "    df_best = df_sorted.loc[\n",
    "        idx_best, [x_col, y_col, corr_col] + ([beta_col] if beta_col else [])\n",
    "    ].copy()\n",
    "\n",
    "    rename_map = {\n",
    "        x_col: \"x_ticker\",\n",
    "        y_col: \"best_y_ticker\",\n",
    "        corr_col: \"best_corr\",\n",
    "    }\n",
    "    if beta_col:\n",
    "        rename_map[beta_col] = \"beta_with_best\"\n",
    "    df_pairs = df_best.rename(columns=rename_map)\n",
    "\n",
    "    if \"beta_with_best\" not in df_pairs.columns:\n",
    "        df_pairs[\"beta_with_best\"] = pd.NA\n",
    "\n",
    "    df_pairs = df_pairs[[\"x_ticker\", \"best_y_ticker\", \"best_corr\", \"beta_with_best\"]]\n",
    "\n",
    "    if output_path:\n",
    "        df_pairs.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(df_pairs)} –ø–∞—Ä —É —Ñ–∞–π–ª: {output_path}\")\n",
    "\n",
    "    return df_all, df_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb998a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.873196Z",
     "iopub.status.busy": "2026-01-27T21:13:43.873196Z",
     "iopub.status.idle": "2026-01-27T21:13:43.889274Z",
     "shell.execute_reply": "2026-01-27T21:13:43.889274Z"
    },
    "papermill": {
     "duration": 0.032148,
     "end_time": "2026-01-27T21:13:43.889274",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.857126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def corr_beta_best_etf_by_lvl3(\n",
    "    tickers_df,\n",
    "    sector_to_etf,\n",
    "    sector_col=\"lvl3\",\n",
    "    ticker_col=\"ticker\",\n",
    "    period=\"6 months\",\n",
    "    as_pivot=False,\n",
    "    fmt=\"json_records\",\n",
    "    batch_size=150,\n",
    "    pause=0.15,\n",
    "    max_retries=3,\n",
    "    output_path=\"CRACEN/work/x_to_best_etf_by_lvl3.csv\",\n",
    "    bench_path=\"ticker_bench.csv\",\n",
    "    fallback_etfs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    –°—Ç–≤–æ—Ä—é—î –¥–≤–∞ —Ñ–∞–π–ª–∏:\n",
    "      1) output_path (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º x_to_best_etf_by_lvl3.csv):\n",
    "         x_ticker, best_y_ticker, best_corr, beta_with_best, lvl3\n",
    "\n",
    "      2) bench_path (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º ticker_bench.csv):\n",
    "         ticker, benchmark\n",
    "\n",
    "    –õ–æ–≥—ñ–∫–∞:\n",
    "      - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ lvl3-—Å–µ–∫—Ç–æ—Ä–∞ –∑–±–∏—Ä–∞—î–º–æ X-—Å–ø–∏—Å–æ–∫ —Ç—ñ–∫–µ—Ä—ñ–≤.\n",
    "      - Y-—Å–ø–∏—Å–æ–∫ ETF-—ñ–≤ –±–µ—Ä–µ–º–æ –∑ sector_to_etf[lvl3].\n",
    "      - –Ø–∫—â–æ —Å–µ–∫—Ç–æ—Ä—É –Ω–µ–º–∞—î –≤ –º–∞–ø—ñ –∞–±–æ —Å–ø–∏—Å–æ–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ fallback_etfs\n",
    "        (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º [\"SPY\", \"QQQ\", \"IWM\"]).\n",
    "      - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –∑–∞–ª–∏—à–∞—î–º–æ ETF –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é.\n",
    "      - –¢—ñ–∫–µ—Ä–∏ –±–µ–∑ lvl3 (NaN) –æ–±—Ä–æ–±–ª—è—é—Ç—å—Å—è —è–∫ —Å–µ–∫—Ç–æ—Ä 'UNKNOWN' –∑ fallback ETF-–∞–º–∏.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd  # –Ω–∞ –≤–∏–ø–∞–¥–æ–∫, —è–∫—â–æ –Ω–µ —ñ–º–ø–æ—Ä—Ç–Ω—É—Ç–æ –≤–∏—â–µ\n",
    "\n",
    "    if fallback_etfs is None:\n",
    "        fallback_etfs = [\"SPY\", \"QQQ\", \"IWM\"]\n",
    "\n",
    "    if sector_col not in tickers_df.columns:\n",
    "        raise ValueError(f\"–£ tickers_df –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ '{sector_col}'\")\n",
    "    if ticker_col not in tickers_df.columns:\n",
    "        raise ValueError(f\"–£ tickers_df –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ '{ticker_col}'\")\n",
    "\n",
    "    # –ë–µ—Ä–µ–º–æ –ª–∏—à–µ —Ç–∏–∫–µ—Ä —ñ —Å–µ–∫—Ç–æ—Ä, –∞–ª–µ –ù–ï –¥—Ä–æ–ø–∞—î–º–æ NaN ‚Äî –∑–∞–º—ñ–Ω–∏–º–æ —ó—Ö –Ω–∞ 'UNKNOWN'\n",
    "    df_clean = (\n",
    "        tickers_df[[ticker_col, sector_col]]\n",
    "        .drop_duplicates()\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # üî• –¢—ñ–∫–µ—Ä–∏ –±–µ–∑ —Å–µ–∫—Ç–æ—Ä–∞ ‚Üí 'UNKNOWN'\n",
    "    df_clean[sector_col] = df_clean[sector_col].fillna(\"UNKNOWN\").astype(str)\n",
    "    df_clean[ticker_col] = df_clean[ticker_col].astype(str)\n",
    "\n",
    "    all_sectors = sorted(df_clean[sector_col].unique().tolist())\n",
    "\n",
    "    all_chunks = []\n",
    "    all_pairs = []\n",
    "    sectors_with_fallback = []\n",
    "\n",
    "    for sector in all_sectors:\n",
    "        etfs = sector_to_etf.get(sector)\n",
    "\n",
    "        if not etfs:\n",
    "            etfs = list(fallback_etfs)\n",
    "            sectors_with_fallback.append(sector)\n",
    "            print(\n",
    "                f\"‚ö†Ô∏è  –î–ª—è —Å–µ–∫—Ç–æ—Ä–∞ '{sector}' –Ω–µ–º–∞—î ETF-—ñ–≤ —É sector_to_etf ‚Äî \"\n",
    "                f\"–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é fallback: {etfs}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚úÖ lvl3: {sector} ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: {etfs}\")\n",
    "\n",
    "        x_tickers = (\n",
    "            df_clean.loc[df_clean[sector_col] == sector, ticker_col]\n",
    "            .astype(str)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        if not x_tickers:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== lvl3: {sector} | X-—Ç–∏–∫–µ—Ä—ñ–≤: {len(x_tickers)} | ETF-–∏: {etfs} ===\")\n",
    "\n",
    "        df_all_sec, df_pairs_sec = corr_beta_best_match_batched(\n",
    "            x_tickers=x_tickers,\n",
    "            y_tickers=etfs,\n",
    "            period=period,\n",
    "            as_pivot=as_pivot,\n",
    "            fmt=fmt,\n",
    "            batch_size=batch_size,\n",
    "            pause=pause,\n",
    "            max_retries=max_retries,\n",
    "            output_path=None,   # —Ç—É—Ç –Ω–µ –ø–∏—à–µ–º–æ –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏\n",
    "        )\n",
    "\n",
    "        if not df_all_sec.empty:\n",
    "            df_all_sec = df_all_sec.copy()\n",
    "            df_all_sec[sector_col] = sector\n",
    "            all_chunks.append(df_all_sec)\n",
    "\n",
    "        if not df_pairs_sec.empty:\n",
    "            df_pairs_sec = df_pairs_sec.copy()\n",
    "            df_pairs_sec[sector_col] = sector\n",
    "            all_pairs.append(df_pairs_sec)\n",
    "\n",
    "    df_all = pd.concat(all_chunks, ignore_index=True) if all_chunks else pd.DataFrame()\n",
    "\n",
    "    if all_pairs:\n",
    "        df_pairs = pd.concat(all_pairs, ignore_index=True)\n",
    "\n",
    "        # –Ω–∞ –≤–∏–ø–∞–¥–æ–∫ –¥—É–±–ª—ñ–≤ –ø–æ x_ticker ‚Äî –ª–∏—à–∞—î–º–æ –Ω–∞–π–±—ñ–ª—å—à—É –∫–æ—Ä–µ–ª—è—Ü—ñ—é\n",
    "        df_pairs_sorted = df_pairs.sort_values(by=\"best_corr\", ascending=False)\n",
    "        idx_best = (\n",
    "            df_pairs_sorted.groupby(\"x_ticker\", as_index=False)[\"best_corr\"]\n",
    "            .idxmax()[\"best_corr\"]\n",
    "        )\n",
    "        df_pairs = df_pairs_sorted.loc[idx_best].reset_index(drop=True)\n",
    "\n",
    "        # ‚öôÔ∏è –¥–µ—Ç–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª\n",
    "        df_pairs.to_csv(output_path, index=False)\n",
    "        print(f\"\\n‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(df_pairs)} X‚ÜíETF –ø–∞—Ä —É —Ñ–∞–π–ª: {output_path}\")\n",
    "\n",
    "        # ‚öôÔ∏è —Å–ø—Ä–æ—â–µ–Ω–∏–π —Ñ–∞–π–ª –≤–∏–≥–ª—è–¥—É ticker,benchmark\n",
    "        df_bench = df_pairs.rename(\n",
    "            columns={\"x_ticker\": \"ticker\", \"best_y_ticker\": \"benchmark\"}\n",
    "        )[[\"ticker\", \"benchmark\"]]\n",
    "\n",
    "        df_bench.to_csv(bench_path, index=False)\n",
    "        print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(df_bench)} –∑–∞–ø–∏—Å—ñ–≤ —É —Ñ–∞–π–ª: {bench_path}\")\n",
    "\n",
    "    else:\n",
    "        df_pairs = pd.DataFrame(\n",
    "            columns=[\"x_ticker\", \"best_y_ticker\", \"best_corr\", \"beta_with_best\", sector_col]\n",
    "        )\n",
    "        df_pairs.to_csv(output_path, index=False)\n",
    "        pd.DataFrame(columns=[\"ticker\", \"benchmark\"]).to_csv(bench_path, index=False)\n",
    "        print(\"\\n‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –∂–æ–¥–Ω–æ—ó –ø–∞—Ä–∏ X‚ÜíETF. –°—Ç–≤–æ—Ä–µ–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ñ–∞–π–ª–∏.\")\n",
    "\n",
    "    if sectors_with_fallback:\n",
    "        print(\"\\n‚ö†Ô∏è –î–ª—è —Ü–∏—Ö lvl3 –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ fallback ETF-–∏ (SPY, QQQ, IWM):\")\n",
    "        for s in sectors_with_fallback:\n",
    "            print(\"   -\", s)\n",
    "\n",
    "    return df_all, df_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab75608f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:13:43.908549Z",
     "iopub.status.busy": "2026-01-27T21:13:43.908549Z",
     "iopub.status.idle": "2026-01-27T21:14:11.997724Z",
     "shell.execute_reply": "2026-01-27T21:14:11.997724Z"
    },
    "papermill": {
     "duration": 28.10845,
     "end_time": "2026-01-27T21:14:11.997724",
     "exception": false,
     "start_time": "2026-01-27T21:13:43.889274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ lvl3: Advertising & Marketing ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Advertising & Marketing | X-—Ç–∏–∫–µ—Ä—ñ–≤: 37 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/37] fetched batch of 37 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Aerospace & Defense ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY', 'QQQ']\n",
      "\n",
      "=== lvl3: Aerospace & Defense | X-—Ç–∏–∫–µ—Ä—ñ–≤: 69 | ETF-–∏: ['IWM', 'SPY', 'QQQ'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69/69] fetched batch of 69 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Apparel & Textile Products ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM', 'QQQ']\n",
      "\n",
      "=== lvl3: Apparel & Textile Products | X-—Ç–∏–∫–µ—Ä—ñ–≤: 36 | ETF-–∏: ['SPY', 'IWM', 'QQQ'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/36] fetched batch of 36 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Asset Management ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLF', 'SPY', 'BITO']\n",
      "\n",
      "=== lvl3: Asset Management | X-—Ç–∏–∫–µ—Ä—ñ–≤: 194 | ETF-–∏: ['XLF', 'SPY', 'BITO'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/194] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194/194] fetched batch of 44 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Automotive ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM', 'QQQ']\n",
      "\n",
      "=== lvl3: Automotive | X-—Ç–∏–∫–µ—Ä—ñ–≤: 73 | ETF-–∏: ['SPY', 'IWM', 'QQQ'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73/73] fetched batch of 73 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Banking ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['KRE', 'XLF', 'SPY', 'BITO']\n",
      "\n",
      "=== lvl3: Banking | X-—Ç–∏–∫–µ—Ä—ñ–≤: 182 | ETF-–∏: ['KRE', 'XLF', 'SPY', 'BITO'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/182] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182/182] fetched batch of 32 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Beverages ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Beverages | X-—Ç–∏–∫–µ—Ä—ñ–≤: 38 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/38] fetched batch of 38 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Biotech & Pharma ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Biotech & Pharma | X-—Ç–∏–∫–µ—Ä—ñ–≤: 670 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/670] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300/670] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450/670] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600/670] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[670/670] fetched batch of 70 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Cable & Satellite ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Cable & Satellite | X-—Ç–∏–∫–µ—Ä—ñ–≤: 12 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/12] fetched batch of 12 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Chemicals ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Chemicals | X-—Ç–∏–∫–µ—Ä—ñ–≤: 83 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83/83] fetched batch of 83 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Commercial Support Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Commercial Support Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 110 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/110] fetched batch of 110 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Construction Materials ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Construction Materials | X-—Ç–∏–∫–µ—Ä—ñ–≤: 20 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] fetched batch of 20 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Consumer Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Consumer Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 40 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/40] fetched batch of 40 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Containers & Packaging ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Containers & Packaging | X-—Ç–∏–∫–µ—Ä—ñ–≤: 20 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] fetched batch of 20 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Diversified Industrials ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Diversified Industrials | X-—Ç–∏–∫–µ—Ä—ñ–≤: 7 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/7] fetched batch of 7 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: E-Commerce Discretionary ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: E-Commerce Discretionary | X-—Ç–∏–∫–µ—Ä—ñ–≤: 51 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/51] fetched batch of 51 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Electric Utilities ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Electric Utilities | X-—Ç–∏–∫–µ—Ä—ñ–≤: 68 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/68] fetched batch of 68 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Electrical Equipment ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Electrical Equipment | X-—Ç–∏–∫–µ—Ä—ñ–≤: 90 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90/90] fetched batch of 90 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Engineering & Construction ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Engineering & Construction | X-—Ç–∏–∫–µ—Ä—ñ–≤: 51 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/51] fetched batch of 51 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Entertainment Content ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Entertainment Content | X-—Ç–∏–∫–µ—Ä—ñ–≤: 44 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/44] fetched batch of 44 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Food ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Food | X-—Ç–∏–∫–µ—Ä—ñ–≤: 67 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67/67] fetched batch of 67 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Forestry, Paper & Wood Products ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Forestry, Paper & Wood Products | X-—Ç–∏–∫–µ—Ä—ñ–≤: 10 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] fetched batch of 10 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Gas & Water Utilities ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Gas & Water Utilities | X-—Ç–∏–∫–µ—Ä—ñ–≤: 24 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/24] fetched batch of 24 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Health Care Facilities & Svcs ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Health Care Facilities & Svcs | X-—Ç–∏–∫–µ—Ä—ñ–≤: 96 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96/96] fetched batch of 96 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Home & Office Products ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Home & Office Products | X-—Ç–∏–∫–µ—Ä—ñ–≤: 20 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] fetched batch of 20 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Home Construction ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Home Construction | X-—Ç–∏–∫–µ—Ä—ñ–≤: 35 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/35] fetched batch of 35 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Household Products ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Household Products | X-—Ç–∏–∫–µ—Ä—ñ–≤: 38 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/38] fetched batch of 38 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: IT Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: IT Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 68 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/68] fetched batch of 68 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Industrial Intermediate Prod ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Industrial Intermediate Prod | X-—Ç–∏–∫–µ—Ä—ñ–≤: 20 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] fetched batch of 20 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Industrial Support Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Industrial Support Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 29 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/29] fetched batch of 29 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Institutional Financial Svcs ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLF', 'SPY', 'BITO']\n",
      "\n",
      "=== lvl3: Institutional Financial Svcs | X-—Ç–∏–∫–µ—Ä—ñ–≤: 40 | ETF-–∏: ['XLF', 'SPY', 'BITO'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/40] fetched batch of 40 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Insurance ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLF', 'SPY', 'BITO']\n",
      "\n",
      "=== lvl3: Insurance | X-—Ç–∏–∫–µ—Ä—ñ–≤: 89 | ETF-–∏: ['XLF', 'SPY', 'BITO'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89/89] fetched batch of 89 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Internet Media & Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Internet Media & Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 107 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107/107] fetched batch of 107 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Leisure Facilities & Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Leisure Facilities & Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 118 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/118] fetched batch of 118 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Leisure Products ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Leisure Products | X-—Ç–∏–∫–µ—Ä—ñ–≤: 31 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/31] fetched batch of 31 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Machinery ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Machinery | X-—Ç–∏–∫–µ—Ä—ñ–≤: 77 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77/77] fetched batch of 77 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Medical Equipment & Devices ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Medical Equipment & Devices | X-—Ç–∏–∫–µ—Ä—ñ–≤: 203 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/203] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203/203] fetched batch of 53 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Metals & Mining ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['GDX', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Metals & Mining | X-—Ç–∏–∫–µ—Ä—ñ–≤: 148 | ETF-–∏: ['GDX', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148/148] fetched batch of 148 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Oil & Gas Services & Equip ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Oil & Gas Services & Equip | X-—Ç–∏–∫–µ—Ä—ñ–≤: 58 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58/58] fetched batch of 58 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Oil & Gas Supply Chain ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Oil & Gas Supply Chain | X-—Ç–∏–∫–µ—Ä—ñ–≤: 140 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140/140] fetched batch of 140 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Publishing & Broadcasting ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Publishing & Broadcasting | X-—Ç–∏–∫–µ—Ä—ñ–≤: 19 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/19] fetched batch of 19 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: REIT ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: REIT | X-—Ç–∏–∫–µ—Ä—ñ–≤: 133 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133/133] fetched batch of 133 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Real Estate Owners & Developers ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Real Estate Owners & Developers | X-—Ç–∏–∫–µ—Ä—ñ–≤: 14 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/14] fetched batch of 14 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Real Estate Services ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Real Estate Services | X-—Ç–∏–∫–µ—Ä—ñ–≤: 19 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/19] fetched batch of 19 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Renewable Energy ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLE', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Renewable Energy | X-—Ç–∏–∫–µ—Ä—ñ–≤: 57 | ETF-–∏: ['XLE', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/57] fetched batch of 57 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Retail - Consumer Staples ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Retail - Consumer Staples | X-—Ç–∏–∫–µ—Ä—ñ–≤: 24 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/24] fetched batch of 24 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Retail - Discretionary ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM', 'QQQ']\n",
      "\n",
      "=== lvl3: Retail - Discretionary | X-—Ç–∏–∫–µ—Ä—ñ–≤: 102 | ETF-–∏: ['SPY', 'IWM', 'QQQ'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102/102] fetched batch of 102 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Semiconductors ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SOXL', 'QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Semiconductors | X-—Ç–∏–∫–µ—Ä—ñ–≤: 103 | ETF-–∏: ['SOXL', 'QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103/103] fetched batch of 103 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Software ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Software | X-—Ç–∏–∫–µ—Ä—ñ–≤: 327 | ETF-–∏: ['QQQ', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/327] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300/327] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[327/327] fetched batch of 27 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Specialty Finance ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['XLF', 'SPY', 'BITO']\n",
      "\n",
      "=== lvl3: Specialty Finance | X-—Ç–∏–∫–µ—Ä—ñ–≤: 168 | ETF-–∏: ['XLF', 'SPY', 'BITO'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/168] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168/168] fetched batch of 18 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Steel ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['GDX', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Steel | X-—Ç–∏–∫–µ—Ä—ñ–≤: 20 | ETF-–∏: ['GDX', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] fetched batch of 20 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Technology Hardware ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'SOXL', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Technology Hardware | X-—Ç–∏–∫–µ—Ä—ñ–≤: 134 | ETF-–∏: ['QQQ', 'SOXL', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134/134] fetched batch of 134 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Telecommunications ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['QQQ', 'KWEB', 'IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Telecommunications | X-—Ç–∏–∫–µ—Ä—ñ–≤: 41 | ETF-–∏: ['QQQ', 'KWEB', 'IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/41] fetched batch of 41 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Tobacco & Cannabis ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Tobacco & Cannabis | X-—Ç–∏–∫–µ—Ä—ñ–≤: 21 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/21] fetched batch of 21 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Transportation & Logistics ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Transportation & Logistics | X-—Ç–∏–∫–µ—Ä—ñ–≤: 125 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125/125] fetched batch of 125 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Transportation Equipment ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['IWM', 'SPY']\n",
      "\n",
      "=== lvl3: Transportation Equipment | X-—Ç–∏–∫–µ—Ä—ñ–≤: 21 | ETF-–∏: ['IWM', 'SPY'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/21] fetched batch of 21 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚ö†Ô∏è  –î–ª—è —Å–µ–∫—Ç–æ—Ä–∞ 'UNKNOWN' –Ω–µ–º–∞—î ETF-—ñ–≤ —É sector_to_etf ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é fallback: ['SPY', 'QQQ', 'IWM']\n",
      "\n",
      "=== lvl3: UNKNOWN | X-—Ç–∏–∫–µ—Ä—ñ–≤: 2031 | ETF-–∏: ['SPY', 'QQQ', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1350/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1650/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1950/2031] fetched batch of 150 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2031/2031] fetched batch of 81 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Wholesale - Consumer Staples ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM']\n",
      "\n",
      "=== lvl3: Wholesale - Consumer Staples | X-—Ç–∏–∫–µ—Ä—ñ–≤: 12 | ETF-–∏: ['SPY', 'IWM'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/12] fetched batch of 12 X —Ç–∏–∫–µ—Ä—ñ–≤\n",
      "‚úÖ lvl3: Wholesale - Discretionary ‚Üí ETF-–∏ –∑ –º–∞–ø–∏: ['SPY', 'IWM', 'QQQ']\n",
      "\n",
      "=== lvl3: Wholesale - Discretionary | X-—Ç–∏–∫–µ—Ä—ñ–≤: 14 | ETF-–∏: ['SPY', 'IWM', 'QQQ'] ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/14] fetched batch of 14 X —Ç–∏–∫–µ—Ä—ñ–≤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ 6510 X‚ÜíETF –ø–∞—Ä —É —Ñ–∞–π–ª: CRACEN/work/x_to_best_etf_by_lvl3.csv\n",
      "‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ 6510 –∑–∞–ø–∏—Å—ñ–≤ —É —Ñ–∞–π–ª: C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\CRACEN\\work\\ticker_bench_.csv\n",
      "\n",
      "‚ö†Ô∏è –î–ª—è —Ü–∏—Ö lvl3 –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ fallback ETF-–∏ (SPY, QQQ, IWM):\n",
      "   - UNKNOWN\n",
      "  x_ticker y_ticker  corr  beta                     lvl3\n",
      "0      ADV      SPY  0.60  0.62  Advertising & Marketing\n",
      "1      ADV      IWM  0.56  0.50  Advertising & Marketing\n",
      "2      ADV      QQQ  0.51  0.00  Advertising & Marketing\n",
      "3      ADV     KWEB  0.13  0.00  Advertising & Marketing\n",
      "4      APP      SPY  0.95  2.84  Advertising & Marketing\n",
      "–†—è–¥–∫—ñ–≤ —É —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö: 19154\n",
      "  x_ticker best_y_ticker  best_corr  beta_with_best  \\\n",
      "0        A           IWM       0.75            0.55   \n",
      "1       AA           SPY       0.70            1.93   \n",
      "2     AAAU           IWM       0.34            0.23   \n",
      "3     AACG           IWM       0.18            0.00   \n",
      "4      AAL           SPY       0.86            1.50   \n",
      "\n",
      "                          lvl3  \n",
      "0  Medical Equipment & Devices  \n",
      "1              Metals & Mining  \n",
      "2                      UNKNOWN  \n",
      "3                     Software  \n",
      "4   Transportation & Logistics  \n"
     ]
    }
   ],
   "source": [
    "# –Ω–æ–≤–µ: –ø—Ä–∞—Ü—é—î–º–æ –Ω–∞–ø—Ä—è–º—É –∑ —Ç–∞–±–ª–∏—Ü–µ—é top1500 (ticker + lvl3)\n",
    "df_all, df_pairs = corr_beta_best_etf_by_lvl3(\n",
    "    tickers_df=top1500,          # DataFrame –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏ ticker, lvl3, ...\n",
    "    sector_to_etf=sector_to_etf, # –º–∞–ø–∞ lvl3 -> —Å–ø–∏—Å–æ–∫ ETF-—ñ–≤\n",
    "    sector_col=\"lvl3\",           # Y–∞–∑–≤–∞ –∫–æ–ª–æ–Ω–∫–∏ —Å–µ–∫—Ç–æ—Ä–∞ –≤ top1500\n",
    "    ticker_col=\"ticker\",         # –Ω–∞–∑–≤–∞ –∫–æ–ª–æ–Ω–∫–∏ –∑ —Ç–∏–∫–µ—Ä–æ–º\n",
    "    period=\"3 months\",\n",
    "    as_pivot=False,\n",
    "    fmt=\"json_records\",\n",
    "    batch_size=150,\n",
    "    pause=0.1,\n",
    "    max_retries=3,\n",
    "    output_path=\"CRACEN/work/x_to_best_etf_by_lvl3.csv\",  # –¥–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–∞\n",
    "    bench_path=str(WORK_DIR / \"ticker_bench_.csv\")\n",
    "            # –ø—Ä–æ—Å—Ç–∏–π —Ñ–∞–π–ª ticker,benchmark\n",
    ")\n",
    "\n",
    "print(df_all.head())\n",
    "print(\"–†—è–¥–∫—ñ–≤ —É —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö:\", len(df_all))\n",
    "print(df_pairs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f010a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.013654Z",
     "iopub.status.busy": "2026-01-27T21:14:12.013654Z",
     "iopub.status.idle": "2026-01-27T21:14:12.092128Z",
     "shell.execute_reply": "2026-01-27T21:14:12.091335Z"
    },
    "papermill": {
     "duration": 0.094404,
     "end_time": "2026-01-27T21:14:12.092128",
     "exception": false,
     "start_time": "2026-01-27T21:14:11.997724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================== FAST INTRADAY SUITE w/ BLUE OCEAN (updated + rolling prune) ==================\n",
    "import os, gc, time, glob, re, csv, uuid, random\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# ---------- utils ----------\n",
    "def _p(*args, **kwargs):\n",
    "    kwargs.setdefault(\"flush\", True)\n",
    "    print(*args, **kwargs)\n",
    "\n",
    "_PSUTIL = False\n",
    "try:\n",
    "    import psutil\n",
    "    _PSUTIL = True\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "def _proc_rss_mb() -> float:\n",
    "    if _PSUTIL:\n",
    "        try:\n",
    "            return psutil.Process(os.getpid()).memory_info().rss / (1024**2)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import resource\n",
    "        ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        if ru > 10**9:\n",
    "            return ru / (1024**2)\n",
    "        return ru / 1024.0\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import tracemalloc\n",
    "        if not tracemalloc.is_tracing():\n",
    "            tracemalloc.start()\n",
    "        cur, _ = tracemalloc.get_traced_memory()\n",
    "        return cur / (1024**2)\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "def _mem_str(prefix: str = \"\") -> str:\n",
    "    mb = _proc_rss_mb()\n",
    "    if mb != mb:\n",
    "        return f\"{prefix}üß† RAM: n/a\"\n",
    "    return f\"{prefix}üß† RAM: {mb:,.1f} MB\"\n",
    "\n",
    "def _ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _fmt_eta(sec: float) -> str:\n",
    "    sec = max(0, int(sec))\n",
    "    d, r = divmod(sec, 86400)\n",
    "    h, r = divmod(r, 3600)\n",
    "    m, s = divmod(r, 60)\n",
    "    if d > 0: return f\"{d}d {h:02d}:{m:02d}:{s:02d}\"\n",
    "    if h > 0: return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "    return f\"{m:02d}:{s:02d}\"\n",
    "\n",
    "def _next_batch_index(out_dir: str, out_prefix: str) -> int:\n",
    "    pat_csv = os.path.join(out_dir, f\"{out_prefix}_*.csv\")\n",
    "    pat_gz  = os.path.join(out_dir, f\"{out_prefix}_*.csv.gz\")\n",
    "    files = glob.glob(pat_csv) + glob.glob(pat_gz)\n",
    "    mx = 0\n",
    "    rx = re.compile(rf'^{re.escape(out_prefix)}_(\\d+)\\.csv(?:\\.gz)?$')\n",
    "    for f in files:\n",
    "        m = rx.match(os.path.basename(f))\n",
    "        if m:\n",
    "            mx = max(mx, int(m.group(1)))\n",
    "    return mx + 1 if mx > 0 else 1\n",
    "\n",
    "# ---------- manifest helpers (old ticker-level) ----------\n",
    "def _manifest_path(out_dir: str) -> str:\n",
    "    return os.path.join(out_dir, \"done_tickers.txt\")\n",
    "\n",
    "def _load_done_tickers_manifest(out_dir: str) -> Optional[set]:\n",
    "    path = _manifest_path(out_dir)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    done = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            t = line.strip()\n",
    "            if t:\n",
    "                done.add(t)\n",
    "    return done\n",
    "\n",
    "def _append_done_tickers_manifest(out_dir: str, tickers: List[str]):\n",
    "    if not tickers:\n",
    "        return\n",
    "    path = _manifest_path(out_dir)\n",
    "    existing = set()\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    existing.add(s)\n",
    "    new_ones = [t for t in set(tickers) if t not in existing]\n",
    "    if not new_ones:\n",
    "        return\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for t in new_ones:\n",
    "            f.write(t + \"\\n\")\n",
    "\n",
    "# ---------- day-manifest helpers (NEW) ----------\n",
    "def _manifest_days_path(out_dir: str) -> str:\n",
    "    return os.path.join(out_dir, \"done_days.csv\")\n",
    "\n",
    "def _load_done_days_manifest(out_dir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Return dict: ticker -> set({'YYYY-MM-DD', ...})\n",
    "    \"\"\"\n",
    "    path = _manifest_days_path(out_dir)\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    done = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        rdr = csv.reader(f)\n",
    "        header = next(rdr, None)\n",
    "        col_idx = {\"ticker\": 0, \"date\": 1}\n",
    "        if header and \"ticker\" in header and \"date\" in header:\n",
    "            col_idx[\"ticker\"] = header.index(\"ticker\")\n",
    "            col_idx[\"date\"]   = header.index(\"date\")\n",
    "        else:\n",
    "            f.seek(0); rdr = csv.reader(f)\n",
    "        for row in rdr:\n",
    "            if not row:\n",
    "                continue\n",
    "            tk = row[col_idx[\"ticker\"]].strip()\n",
    "            dt = row[col_idx[\"date\"]].strip()\n",
    "            if tk and dt:\n",
    "                done.setdefault(tk, set()).add(dt)\n",
    "    return done\n",
    "\n",
    "def _append_done_days_manifest(out_dir: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    From df uses 'ticker' and 'dt' (YYYY-MM-DD HH:MM:SS) to append (ticker,date) to done_days.csv\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"dt\" not in df.columns or \"ticker\" not in df.columns:\n",
    "        return\n",
    "    path = _manifest_days_path(out_dir)\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    tmp = df[[\"ticker\",\"dt\"]].copy()\n",
    "    tmp[\"date\"] = tmp[\"dt\"].astype(str).str.slice(0,10)\n",
    "    tmp = tmp.drop_duplicates(subset=[\"ticker\",\"date\"])\n",
    "\n",
    "    need_header = not os.path.exists(path)\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        if need_header:\n",
    "            wr.writerow([\"ticker\",\"date\"])\n",
    "        for _, r in tmp.iterrows():\n",
    "            wr.writerow([str(r[\"ticker\"]).strip(), str(r[\"date\"]).strip()])\n",
    "\n",
    "def prune_done_days_manifest(out_dir: str, start_date_str: str, end_date_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Rolling-window prune for done_days.csv:\n",
    "    - keep only rows where start_date_str <= date <= end_date_str\n",
    "    - returns refreshed dict: ticker -> set(dates)\n",
    "    Safe to call even if file doesn't exist.\n",
    "    \"\"\"\n",
    "    path = _manifest_days_path(out_dir)\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "\n",
    "    # normalize inputs\n",
    "    start_date_str = str(start_date_str)[:10]\n",
    "    end_date_str   = str(end_date_str)[:10]\n",
    "    if start_date_str > end_date_str:\n",
    "        # swap just in case\n",
    "        start_date_str, end_date_str = end_date_str, start_date_str\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        rdr = csv.reader(f)\n",
    "        header = next(rdr, None)\n",
    "        col_idx = {\"ticker\": 0, \"date\": 1}\n",
    "        has_header = bool(header and \"ticker\" in header and \"date\" in header)\n",
    "        if has_header:\n",
    "            col_idx[\"ticker\"] = header.index(\"ticker\")\n",
    "            col_idx[\"date\"]   = header.index(\"date\")\n",
    "        else:\n",
    "            # no header; treat first row as data\n",
    "            if header:\n",
    "                rdr = [header] + list(rdr)  # include first row back as data\n",
    "            else:\n",
    "                rdr = []\n",
    "        for row in rdr:\n",
    "            if not row:\n",
    "                continue\n",
    "            try:\n",
    "                tk = str(row[col_idx[\"ticker\"]]).strip().upper()\n",
    "                dt = str(row[col_idx[\"date\"]]).strip()[:10]\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not tk or not dt:\n",
    "                continue\n",
    "            if start_date_str <= dt <= end_date_str:\n",
    "                rows.append((tk, dt))\n",
    "\n",
    "    # rewrite file (dedup)\n",
    "    _ensure_dir(out_dir)\n",
    "    tmp_path = path + \".tmp\"\n",
    "    seen = set()\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"ticker\",\"date\"])\n",
    "        for tk, dt in rows:\n",
    "            key = (tk, dt)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            wr.writerow([tk, dt])\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "    # build dict cache\n",
    "    done = {}\n",
    "    for tk, dt in seen:\n",
    "        done.setdefault(tk, set()).add(dt)\n",
    "    return done\n",
    "\n",
    "def _dates_between(start: str, end: str) -> List[pd.Timestamp]:\n",
    "    s = pd.to_datetime(start)\n",
    "    e = pd.to_datetime(end)\n",
    "    if pd.isna(s) or pd.isna(e) or s > e:\n",
    "        return []\n",
    "    days = pd.date_range(start=s.normalize(), end=e.normalize(), freq=\"D\")\n",
    "    return list(days)\n",
    "\n",
    "def _group_consecutive_dates(dates: List[pd.Timestamp], max_span_days: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Build [start_dt, end_dt] ranges from a list of dates, merging consecutive days,\n",
    "    capped by max_span_days per range. Datetimes are ISO to seconds.\n",
    "    \"\"\"\n",
    "    if not dates:\n",
    "        return []\n",
    "    dates = sorted(pd.to_datetime(dates))\n",
    "    out = []\n",
    "    run_start = dates[0]\n",
    "    run_end = dates[0]\n",
    "    for d in dates[1:]:\n",
    "        consecutive = (d - run_end).days == 1\n",
    "        span_len = (d - run_start).days + 1\n",
    "        if consecutive and span_len <= max_span_days:\n",
    "            run_end = d\n",
    "        else:\n",
    "            a = run_start.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "            b = (run_end + pd.Timedelta(hours=23, minutes=59, seconds=59)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            out.append((a, b))\n",
    "            run_start = d\n",
    "            run_end = d\n",
    "    a = run_start.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    b = (run_end + pd.Timedelta(hours=23, minutes=59, seconds=59)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    out.append((a, b))\n",
    "    return out\n",
    "\n",
    "# ================== SMALL (low-level) FUNCTIONS ==================\n",
    "def _build_ranges(start: str, end: str, chunk_days: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"–†–æ–∑–±–∏–≤–∞—î [start,end] –Ω–∞ –Ω–µ–ø–µ—Ä–µ–∫—Ä–∏–≤–Ω—ñ –≤—ñ–¥—Ä—ñ–∑–∫–∏ –ø–æ chunk_days –¥–Ω—ñ–≤.\"\"\"\n",
    "    chunk_days = max(1, int(chunk_days))\n",
    "    s = pd.to_datetime(start)\n",
    "    e = pd.to_datetime(end)\n",
    "    if pd.isna(s) or pd.isna(e) or s > e:\n",
    "        return []\n",
    "    out: List[Tuple[str, str]] = []\n",
    "    cur = s\n",
    "    step = timedelta(days=chunk_days) - timedelta(seconds=1)\n",
    "    while cur <= e:\n",
    "        r_end = min(cur + step, e)\n",
    "        out.append((cur.strftime(\"%Y-%m-%d %H:%M:%S\"), r_end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        cur = r_end + timedelta(seconds=1)\n",
    "    return out\n",
    "\n",
    "def _one_call_v3_dt_fast(\n",
    "    ticker: str,\n",
    "    start_dt: str,\n",
    "    end_dt: str,\n",
    "    interval: int,\n",
    "    *,\n",
    "    chart_type: str = \"ohlcv\",\n",
    "    include_boats: bool = False,\n",
    "    max_retries: int = 2,\n",
    "    retry_backoff: float = 0.5,     # —Å–µ–∫\n",
    "    normalize_ohlcv: bool = False,  # –¥–µ—Ñ–æ–ª—Ç: –í–ò–ú–ö–ù–ï–ù–û (—à–≤–∏–¥–∫—ñ—Å—Ç—å)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç: /intraday/v3.\"\"\"\n",
    "    params = {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"interval\": int(interval),\n",
    "        \"chart_type\": chart_type,\n",
    "        \"start_datetime\": start_dt,\n",
    "        \"end_datetime\": end_dt,\n",
    "        \"include_boats\": bool(include_boats),\n",
    "        \"format\": \"json_records\",\n",
    "    }\n",
    "    attempts = max(0, int(max_retries)) + 1\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            df = DatumApi.data_request(\"/intraday/v3\", params)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                if \"ticker\" not in df.columns:\n",
    "                    df.insert(0, \"ticker\", ticker.upper())\n",
    "                if normalize_ohlcv:\n",
    "                    for col in (\"o\",\"h\",\"l\",\"c\",\"v\"):\n",
    "                        if col in df.columns:\n",
    "                            df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "                return df\n",
    "            return pd.DataFrame()\n",
    "        except Exception:\n",
    "            if attempt == attempts - 1:\n",
    "                return pd.DataFrame()\n",
    "            sleep_s = retry_backoff * (2 ** attempt)\n",
    "            sleep_s *= (0.9 + 0.2 * random.random())\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "def _one_call_blue_ocean_v3_dt_fast(\n",
    "    ticker: str,\n",
    "    start_dt: str,\n",
    "    end_dt: str,\n",
    "    interval: int,\n",
    "    *,\n",
    "    chart_type: str = \"ohlcv\",\n",
    "    max_retries: int = 2,\n",
    "    retry_backoff: float = 0.5,\n",
    "    normalize_ohlcv: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Blue Ocean –µ–Ω–¥–ø–æ—ñ–Ω—Ç: /intraday/v3/blue_ocean (20:00‚Äì04:00).\"\"\"\n",
    "    params = {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"interval\": int(interval),\n",
    "        \"chart_type\": chart_type,\n",
    "        \"start_datetime\": start_dt,\n",
    "        \"end_datetime\": end_dt,\n",
    "        \"format\": \"json_records\",\n",
    "    }\n",
    "    attempts = max(0, int(max_retries)) + 1\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            df = DatumApi.data_request(\"/intraday/v3/blue_ocean\", params)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                if \"ticker\" not in df.columns:\n",
    "                    df.insert(0, \"ticker\", ticker.upper())\n",
    "                if normalize_ohlcv:\n",
    "                    for col in (\"o\",\"h\",\"l\",\"c\",\"v\"):\n",
    "                        if col in df.columns:\n",
    "                            df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "                return df\n",
    "            return pd.DataFrame()\n",
    "        except Exception:\n",
    "            if attempt == attempts - 1:\n",
    "                return pd.DataFrame()\n",
    "            sleep_s = retry_backoff * (2 ** attempt)\n",
    "            sleep_s *= (0.9 + 0.2 * random.random())\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "def fetch_intraday_v3_datetime_fast(\n",
    "    ticker: str,\n",
    "    start: str,\n",
    "    end: str,\n",
    "    interval: int = 1,\n",
    "    *,\n",
    "    chunk_days: int = 14,\n",
    "    parallel: bool = True,\n",
    "    max_workers: int = 12,\n",
    "    chart_type: str = \"ohlcv\",\n",
    "    include_boats: bool = False,\n",
    "    sort_by_dt: bool = False,   # –¥–µ—Ñ–æ–ª—Ç: –≤–∏–∫–ª (—à–≤–∏–¥–∫—ñ—Å—Ç—å)\n",
    "    dedup_dt: bool = False,     # –¥–µ—Ñ–æ–ª—Ç: –≤–∏–∫–ª (—à–≤–∏–¥–∫—ñ—Å—Ç—å)\n",
    "    normalize_ohlcv: bool = False,  # –¥–µ—Ñ–æ–ª—Ç: –≤–∏–∫–ª (—à–≤–∏–¥–∫—ñ—Å—Ç—å)\n",
    "    include_blue_ocean: bool = False,   # ‚¨ÖÔ∏è –ù–û–í–ï\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –¢—è–≥–Ω–µ —ñ–Ω—Ç—Ä–∞–¥—ñ–π–Ω—ñ –¥–∞–Ω—ñ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤—ñ–∫–Ω–∞ —ñ, –æ–ø—Ü—ñ–π–Ω–æ, Blue Ocean (20:00‚Äì04:00),\n",
    "    –∑—à–∏–≤–∞—î –≤ –æ–¥–∏–Ω DF. –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –±–µ–∑ –∑–∞–π–≤–∏—Ö –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å.\n",
    "    \"\"\"\n",
    "    ranges = _build_ranges(start, end, chunk_days)\n",
    "    if not ranges:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    worker_count = max(1, min(int(max_workers), len(ranges) * (2 if include_blue_ocean else 1))) if parallel else 1\n",
    "    parts: List[pd.DataFrame] = []\n",
    "\n",
    "    if parallel and (len(ranges) > 1 or include_blue_ocean):\n",
    "        with ThreadPoolExecutor(max_workers=worker_count) as ex:\n",
    "            futs = []\n",
    "            for (a, b) in ranges:\n",
    "                futs.append(ex.submit(\n",
    "                    _one_call_v3_dt_fast, ticker, a, b, interval,\n",
    "                    chart_type=chart_type, include_boats=include_boats,\n",
    "                    normalize_ohlcv=normalize_ohlcv\n",
    "                ))\n",
    "                if include_blue_ocean:\n",
    "                    futs.append(ex.submit(\n",
    "                        _one_call_blue_ocean_v3_dt_fast, ticker, a, b, interval,\n",
    "                        chart_type=chart_type, normalize_ohlcv=normalize_ohlcv\n",
    "                    ))\n",
    "            for f in as_completed(futs):\n",
    "                df = f.result()\n",
    "                if df is not None and not df.empty:\n",
    "                    parts.append(df)\n",
    "    else:\n",
    "        for (a, b) in ranges:\n",
    "            df_main = _one_call_v3_dt_fast(\n",
    "                ticker, a, b, interval,\n",
    "                chart_type=chart_type, include_boats=include_boats,\n",
    "                normalize_ohlcv=normalize_ohlcv\n",
    "            )\n",
    "            if df_main is not None and not df_main.empty:\n",
    "                parts.append(df_main)\n",
    "            if include_blue_ocean:\n",
    "                df_bo = _one_call_blue_ocean_v3_dt_fast(\n",
    "                    ticker, a, b, interval,\n",
    "                    chart_type=chart_type, normalize_ohlcv=normalize_ohlcv\n",
    "                )\n",
    "                if df_bo is not None and not df_bo.empty:\n",
    "                    parts.append(df_bo)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # –Ø–∫—â–æ –ø—ñ–¥–º—ñ—à—É–≤–∞–ª–∏ Blue Ocean ‚Äî –≤—ñ–¥—Å–æ—Ä—Ç—É—î–º–æ –ø–æ 'dt'\n",
    "    if \"dt\" in out.columns and (include_blue_ocean or sort_by_dt or dedup_dt):\n",
    "        if dedup_dt:\n",
    "            out = out.drop_duplicates(subset=[\"ticker\", \"dt\"], keep=\"last\")\n",
    "        out = out.sort_values(\"dt\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ================== BIG (all tickers, batch writer) [UPDATED + rolling prune] ==================\n",
    "def fetch_intraday_v3_for_all_tickers(\n",
    "    tickers_df: pd.DataFrame,\n",
    "    start: str,\n",
    "    end: str,\n",
    "    *,\n",
    "    interval: int = 1,\n",
    "    chunk_days: int = 30,\n",
    "    parallel_chunks: bool = True,\n",
    "    max_workers_chunks: int = 12,\n",
    "    parallel_tickers: bool = True,\n",
    "    max_workers_tickers: int = 6,\n",
    "    save_every: int = 50,\n",
    "    out_dir: str = \"intraday_batches\",\n",
    "    out_prefix: str = \"batch\",\n",
    "    gzip: bool = True,\n",
    "    keep_columns: Optional[List[str]] = None,\n",
    "    verbose: bool = True,\n",
    "    progress_every: int = 10,\n",
    "    mem_log_every: int = 50,\n",
    "    use_manifest: bool = False,            # old ticker-level manifest (off by default)\n",
    "    use_day_manifest: bool = True,         # NEW: day-level manifest (on by default)\n",
    "    flush_when_rows: Optional[int] = None,\n",
    "    flush_every_seconds: Optional[int] = None,\n",
    "    include_blue_ocean: bool = False,      # pass-through\n",
    ") -> pd.DataFrame:\n",
    "    assert \"ticker\" in tickers_df.columns, \"tickers_df –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –∫–æ–ª–æ–Ω–∫—É 'ticker'\"\n",
    "\n",
    "    # ‚¨ÜÔ∏è –ù–û–†–ú–ê–õ–Ü–ó–ê–¶–Ü–Ø: —Ç—ñ–∫–µ—Ä–∏ —É –≤–µ—Ä—Ö–Ω—ñ–π —Ä–µ–≥—ñ—Å—Ç—Ä –æ–¥—Ä–∞–∑—É –Ω–∞ –≤—Ö–æ–¥—ñ\n",
    "    tickers: List[str] = [str(t).strip().upper() for t in tickers_df[\"ticker\"].dropna().unique()]\n",
    "    if not tickers:\n",
    "        return pd.DataFrame(columns=[\"ticker\",\"error\"])\n",
    "\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    # --- rolling window bounds as YYYY-MM-DD for manifest prune ---\n",
    "    start_date_str = str(start)[:10]\n",
    "    end_date_str   = str(end)[:10]\n",
    "\n",
    "    has_batches = bool(glob.glob(os.path.join(out_dir, f\"{out_prefix}_*.csv*\")))\n",
    "    done_tickers = set()\n",
    "    if use_manifest:\n",
    "        m = _load_done_tickers_manifest(out_dir)\n",
    "        if m and has_batches:\n",
    "            done_tickers = m\n",
    "        elif m and not has_batches and verbose:\n",
    "            _p(\"‚ö†Ô∏è –ó–Ω–∞–π—à–æ–≤ done_tickers.txt, –∞–ª–µ –Ω–µ–º–∞—î batch-—Ñ–∞–π–ª—ñ–≤ ‚Üí —ñ–≥–Ω–æ—Ä—É—é –º–∞–Ω—ñ—Ñ–µ—Å—Ç.\")\n",
    "        # apply old manifest only if explicitly enabled\n",
    "        tickers = [t for t in tickers if t not in done_tickers]\n",
    "\n",
    "    # NEW: day manifest load (+ rolling prune)\n",
    "    if use_day_manifest:\n",
    "        # prune done_days.csv to the current rolling window and get fresh dict\n",
    "        done_days_by_ticker = prune_done_days_manifest(out_dir, start_date_str, end_date_str)\n",
    "    else:\n",
    "        done_days_by_ticker = {}\n",
    "\n",
    "    # ‚¨ÜÔ∏è –ù–û–†–ú–ê–õ–Ü–ó–ê–¶–Ü–Ø: –∫–ª—é—á—ñ –º–∞–Ω—ñ—Ñ–µ—Å—Ç—É —É –≤–µ—Ä—Ö–Ω—ñ–π —Ä–µ–≥—ñ—Å—Ç—Ä\n",
    "    if use_day_manifest and done_days_by_ticker:\n",
    "        done_days_by_ticker = {str(k).upper(): set(v) for k, v in done_days_by_ticker.items()}\n",
    "\n",
    "    # --- –ø–æ–ø–µ—Ä–µ–¥–Ω—î –≤—ñ–¥—Å—ñ–∫–∞–Ω–Ω—è –ø–æ–≤–Ω—ñ—Å—Ç—é –≥–æ—Ç–æ–≤–∏—Ö —Ç—ñ–∫–µ—Ä—ñ–≤ ---\n",
    "    all_days_list = _dates_between(start, end)\n",
    "    all_days_str = [d.strftime(\"%Y-%m-%d\") for d in all_days_list]\n",
    "\n",
    "    def _ticker_fully_done(tk: str) -> bool:\n",
    "        if not use_day_manifest:\n",
    "            return False\n",
    "        s = done_days_by_ticker.get(str(tk).upper(), set())\n",
    "        return bool(s) and all(dd in s for dd in all_days_str)\n",
    "\n",
    "    fully_done = [t for t in tickers if _ticker_fully_done(t)]\n",
    "    tickers = [t for t in tickers if t not in fully_done]\n",
    "\n",
    "    total = len(tickers)\n",
    "    batch_idx = _next_batch_index(out_dir, out_prefix)\n",
    "    run_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "    errors, buffer = [], []\n",
    "    written_total = 0\n",
    "    start_time = time.time()\n",
    "    last_flush_at = start_time\n",
    "    rows_in_buffer = 0\n",
    "\n",
    "    if verbose:\n",
    "        psutil_note = \"psutil\" if _PSUTIL else \"fallback\"\n",
    "\n",
    "        # –ª–æ–≥—ñ–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ–≤ –¥–∞—Ç –¥–ª—è –ª–æ–≥—ñ–≤\n",
    "        if use_day_manifest and done_days_by_ticker:\n",
    "            # —É—Å—ñ –¥–Ω—ñ –≤ –º–∞–Ω—ñ—Ñ–µ—Å—Ç—ñ\n",
    "            all_done_dates = sorted({d for s in done_days_by_ticker.values() for d in s})\n",
    "            skipped_days_str = (\n",
    "                f\"{all_done_dates[0]} ‚Üí {all_done_dates[-1]} ({len(all_done_dates)} –¥–Ω—ñ–≤)\"\n",
    "                if all_done_dates else \"‚Äì\"\n",
    "            )\n",
    "            # —É—Å—ñ –¥–Ω—ñ, —è–∫—ñ —â–µ –Ω–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ (–ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –ø–æ–≤–Ω–∏–º –¥—ñ–∞–ø–∞–∑–æ–Ω–æ–º)\n",
    "            all_days_full = [d.strftime(\"%Y-%m-%d\") for d in _dates_between(start, end)]\n",
    "            remaining_days = sorted(set(all_days_full) - set(all_done_dates))\n",
    "            remaining_days_str = (\n",
    "                f\"{remaining_days[0]} ‚Üí {remaining_days[-1]} ({len(remaining_days)} –¥–Ω—ñ–≤)\"\n",
    "                if remaining_days else \"‚Äì\"\n",
    "            )\n",
    "        else:\n",
    "            skipped_days_str = \"‚Äì\"\n",
    "            remaining_days_str = f\"{start[:10]} ‚Üí {end[:10]}\"\n",
    "\n",
    "        _p(f\"‚ñ∂ –ü–æ—á–∏–Ω–∞—î–º–æ: {total} —Ç—ñ–∫–µ—Ä—ñ–≤ (–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(done_tickers)} –ø–æ —Å—Ç–∞—Ä–æ–º—É –º–∞–Ω—ñ—Ñ–µ—Å—Ç—É) | {start} ‚Üí {end} | interval={interval} | next batch #{batch_idx:04d} | run={run_id}\")\n",
    "        _p(f\"   workers: tickers={max_workers_tickers if parallel_tickers else 1}, chunks={max_workers_chunks if parallel_chunks else 0} | blue_ocean={include_blue_ocean} | day-manifest={use_day_manifest}\")\n",
    "        _p(f\"   day-manifest: –ø–æ–≤–Ω—ñ—Å—Ç—é –≥–æ—Ç–æ–≤–∏—Ö —Ç—ñ–∫–µ—Ä—ñ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ = {len(fully_done)}\")\n",
    "        _p(f\"   üìâ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –¥–Ω—ñ (–≤ –º–µ–∂–∞—Ö window): {skipped_days_str}\")\n",
    "        _p(f\"   üìà –æ–±—Ä–æ–±–ª—è—é—Ç—å—Å—è –¥–Ω—ñ: {remaining_days_str}\")\n",
    "        _p(_mem_str(f\"   {psutil_note} ¬∑ —Å—Ç–∞—Ä—Ç ¬∑ \"))\n",
    "\n",
    "    def _make_batch_path(idx: int) -> str:\n",
    "        fname = f\"{out_prefix}_{idx:04d}.csv\" + (\".gz\" if gzip else \"\")\n",
    "        return os.path.join(out_dir, fname)\n",
    "\n",
    "    def _append_df_for_buffer(dfi: pd.DataFrame):\n",
    "        nonlocal rows_in_buffer\n",
    "        if dfi is None or dfi.empty:\n",
    "            return\n",
    "        buffer.append(dfi)\n",
    "        rows_in_buffer += len(dfi)\n",
    "\n",
    "    # NEW: –º–∏—Ç—Ç—î–≤–æ –æ–Ω–æ–≤–ª—é—î–º–æ in-memory –∫–µ—à –¥–Ω—ñ–≤ —ñ–∑ –Ω–æ–≤–∏—Ö —Ä—è–¥–∫—ñ–≤\n",
    "    def _update_day_cache_from_df(dfi: pd.DataFrame):\n",
    "        if not use_day_manifest or dfi is None or dfi.empty:\n",
    "            return\n",
    "        if \"ticker\" not in dfi.columns or \"dt\" not in dfi.columns:\n",
    "            return\n",
    "        tmp = dfi[[\"ticker\", \"dt\"]].copy()\n",
    "        tmp[\"date\"] = tmp[\"dt\"].astype(str).str.slice(0, 10)\n",
    "        for tk, grp in tmp.groupby(\"ticker\"):\n",
    "            done_days_by_ticker.setdefault(str(tk).upper(), set()).update(grp[\"date\"].astype(str).tolist())\n",
    "\n",
    "    def _flush():\n",
    "        nonlocal buffer, batch_idx, written_total, rows_in_buffer, last_flush_at\n",
    "        if not buffer:\n",
    "            return None\n",
    "        path = _make_batch_path(batch_idx)\n",
    "        while os.path.exists(path):\n",
    "            batch_idx += 1\n",
    "            path = _make_batch_path(batch_idx)\n",
    "\n",
    "        if verbose:\n",
    "            _p(_mem_str(\"   ‚è≥ –ø–µ—Ä–µ–¥ flush ¬∑ \"))\n",
    "\n",
    "        out = pd.concat(buffer, ignore_index=True)\n",
    "\n",
    "        # Stable order & dedup\n",
    "        if \"dt\" in out.columns:\n",
    "            out = out.drop_duplicates(subset=[\"ticker\",\"dt\"], keep=\"last\")\n",
    "            out = out.sort_values([\"ticker\",\"dt\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "        tmp_path = path + \".tmp\"\n",
    "        if gzip:\n",
    "            out.to_csv(tmp_path, index=False, compression={\"method\": \"gzip\", \"compresslevel\": 1})\n",
    "        else:\n",
    "            out.to_csv(tmp_path, index=False, compression=None)\n",
    "        os.replace(tmp_path, path)\n",
    "\n",
    "        written_total += len(out)\n",
    "        if verbose:\n",
    "            _p(f\"üíæ batch {batch_idx} ‚Üí {len(out):,} —Ä—è–¥–∫—ñ–≤ (–≤—Å—å–æ–≥–æ {written_total:,} —É —Ü—å–æ–º—É –∑–∞–ø—É—Å–∫—É) ‚Üí {path}\")\n",
    "\n",
    "        # NEW: update day-manifest (—Ç–∞ –æ–¥—Ä–∞–∑—É –æ–Ω–æ–≤–∏—Ç–∏ –∫–µ—à —É –ø–∞–º'—è—Ç—ñ)\n",
    "        if use_day_manifest:\n",
    "            if \"dt\" not in out.columns:\n",
    "                for alt in (\"datetime\", \"time\", \"t\"):\n",
    "                    if alt in out.columns:\n",
    "                        out = out.rename(columns={alt: \"dt\"})\n",
    "                        break\n",
    "            _append_done_days_manifest(out_dir, out)\n",
    "            _update_day_cache_from_df(out)\n",
    "        else:\n",
    "            if \"ticker\" in out.columns:\n",
    "                _append_done_tickers_manifest(out_dir, out[\"ticker\"].astype(str).unique().tolist())\n",
    "\n",
    "        # (–æ–ø—Ü—ñ–π–Ω–æ) –≤—ñ–¥–º—ñ—á–∞—Ç–∏ –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–∞–∫—Ä–∏—Ç—ñ —Ç—ñ–∫–µ—Ä–∏ —É —Å—Ç–∞—Ä–æ–º—É –º–∞–Ω—ñ—Ñ–µ—Å—Ç—ñ, —è–∫—â–æ –≤—ñ–Ω —É–≤—ñ–º–∫–Ω–µ–Ω–∏–π\n",
    "        if use_manifest and use_day_manifest:\n",
    "            def _ticker_fully_done_local(tk: str) -> bool:\n",
    "                s = done_days_by_ticker.get(str(tk).upper(), set())\n",
    "                return bool(s) and all(dd in s for dd in all_days_str)\n",
    "            fully_done_now = []\n",
    "            for tk in out.get(\"ticker\", pd.Series(dtype=str)).astype(str).str.upper().unique():\n",
    "                if _ticker_fully_done_local(tk):\n",
    "                    fully_done_now.append(tk)\n",
    "            _append_done_tickers_manifest(out_dir, fully_done_now)\n",
    "\n",
    "        batch_idx += 1\n",
    "        buffer.clear()\n",
    "        rows_in_buffer = 0\n",
    "        del out\n",
    "        gc.collect()\n",
    "        last_flush_at = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            _p(_mem_str(\"   ‚úÖ –ø—ñ—Å–ª—è flush ¬∑ \"))\n",
    "        return path\n",
    "\n",
    "    def _progress_line(done: int) -> str:\n",
    "        pct = (done / total * 100) if total else 100.0\n",
    "        elapsed = time.time() - start_time\n",
    "        per_item = elapsed / done if done > 0 else 0.0\n",
    "        remaining = (total - done) * per_item if done > 0 else 0.0\n",
    "        return f\"{pct:6.2f}% | ETA {_fmt_eta(remaining)}\"\n",
    "\n",
    "    def _should_print(idx: int, is_error: bool, is_last: bool) -> bool:\n",
    "        if is_error or is_last:\n",
    "            return True\n",
    "        if progress_every and progress_every > 0:\n",
    "            return (idx % progress_every == 0)\n",
    "        return True\n",
    "\n",
    "    def _maybe_log_mem(idx: int):\n",
    "        if not verbose:\n",
    "            return\n",
    "        if mem_log_every and mem_log_every > 0 and idx % mem_log_every == 0:\n",
    "            _p(_mem_str(\"   ‚Ä¢ RAM ¬∑ \"))\n",
    "\n",
    "    def _maybe_flush_by_thresholds(processed: int):\n",
    "        if not buffer:\n",
    "            return\n",
    "        need_rows = (flush_when_rows is not None and rows_in_buffer >= flush_when_rows)\n",
    "        need_time = (flush_every_seconds is not None and (time.time() - last_flush_at) >= flush_every_seconds)\n",
    "        need_periodic = (save_every and processed % save_every == 0)\n",
    "        if need_rows or need_time or need_periodic:\n",
    "            _flush()\n",
    "\n",
    "    # NEW: —è–∫—â–æ –∑–∞ –¥—ñ–∞–ø–∞–∑–æ–Ω –≤–∑–∞–≥–∞–ª—ñ –Ω–µ –ø—Ä–∏–π—à–ª–∏ –¥–∞–Ω—ñ ‚Äî –ø–æ–∑–Ω–∞—á–∞—î–º–æ –¥–Ω—ñ —è–∫ –≤–∏–∫–æ–Ω–∞–Ω—ñ\n",
    "    def _mark_days_done(tk: str, days_list: List[pd.Timestamp]):\n",
    "        if not use_day_manifest or not days_list:\n",
    "            return\n",
    "        df_mark = pd.DataFrame({\n",
    "            \"ticker\": str(tk).upper(),\n",
    "            \"dt\": [pd.Timestamp(d).strftime(\"%Y-%m-%d 00:00:00\") for d in days_list],\n",
    "        })\n",
    "        _append_done_days_manifest(out_dir, df_mark)\n",
    "        done_days_by_ticker.setdefault(str(tk).upper(), set()).update(\n",
    "            [pd.Timestamp(d).strftime(\"%Y-%m-%d\") for d in days_list]\n",
    "        )\n",
    "\n",
    "    # --- per-ticker worker (respects day-manifest gaps) ---\n",
    "    def _one(tk: str):\n",
    "        try:\n",
    "            # determine which days to fetch\n",
    "            all_days = all_days_list\n",
    "            if use_day_manifest:\n",
    "                done_set = done_days_by_ticker.get(str(tk).upper(), set())\n",
    "                todo_days = [d for d in all_days if d.strftime(\"%Y-%m-%d\") not in done_set]\n",
    "            else:\n",
    "                todo_days = all_days\n",
    "\n",
    "            if not todo_days:\n",
    "                return tk, pd.DataFrame(), None\n",
    "\n",
    "            # group missing days into ranges (capped by chunk_days)\n",
    "            todo_ranges = _group_consecutive_dates(todo_days, max_span_days=max(1, int(chunk_days)))\n",
    "\n",
    "            parts: List[pd.DataFrame] = []\n",
    "            if parallel_chunks and len(todo_ranges) > 1:\n",
    "                w = max(1, min(int(max_workers_chunks), len(todo_ranges) * (2 if include_blue_ocean else 1)))\n",
    "                with ThreadPoolExecutor(max_workers=w) as ex:\n",
    "                    futs = []\n",
    "                    for (a, b) in todo_ranges:\n",
    "                        futs.append(ex.submit(\n",
    "                            _one_call_v3_dt_fast, tk, a, b, interval,\n",
    "                            chart_type=\"ohlcv\", include_boats=False, normalize_ohlcv=False\n",
    "                        ))\n",
    "                        if include_blue_ocean:\n",
    "                            futs.append(ex.submit(\n",
    "                                _one_call_blue_ocean_v3_dt_fast, tk, a, b, interval,\n",
    "                                chart_type=\"ohlcv\", normalize_ohlcv=False\n",
    "                            ))\n",
    "                    for f in as_completed(futs):\n",
    "                        df = f.result()\n",
    "                        if df is not None and not df.empty:\n",
    "                            parts.append(df)\n",
    "            else:\n",
    "                for (a, b) in todo_ranges:\n",
    "                    df_main = _one_call_v3_dt_fast(tk, a, b, interval, chart_type=\"ohlcv\", include_boats=False, normalize_ohlcv=False)\n",
    "                    if df_main is not None and not df_main.empty:\n",
    "                        parts.append(df_main)\n",
    "                    if include_blue_ocean:\n",
    "                        df_bo = _one_call_blue_ocean_v3_dt_fast(tk, a, b, interval, chart_type=\"ohlcv\", normalize_ohlcv=False)\n",
    "                        if df_bo is not None and not df_bo.empty:\n",
    "                            parts.append(df_bo)\n",
    "\n",
    "            if not parts:\n",
    "                # –Ω–µ –ø—Ä–∏–π—à–ª–æ –¥–∞–Ω–∏—Ö ‚Äî –ø–æ–∑–Ω–∞—á–∞—î–º–æ —Ü—ñ –¥–Ω—ñ –≤—ñ–¥–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–∏–º–∏, —â–æ–± –±—ñ–ª—å—à–µ –Ω–µ —Ö–æ–¥–∏—Ç–∏\n",
    "                _mark_days_done(tk, todo_days)  # NEW\n",
    "                return tk, pd.DataFrame(), None\n",
    "\n",
    "            dfo = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "            if \"dt\" in dfo.columns:\n",
    "                dfo = dfo.drop_duplicates(subset=[\"ticker\",\"dt\"], keep=\"last\")\n",
    "                dfo = dfo.sort_values([\"ticker\",\"dt\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "            # –∑–∞–≤–∂–¥–∏ –∑–∞–ª–∏—à–∞—î–º–æ 'ticker' —Ç–∞ 'dt' (—â–æ–± –º–∞–Ω—ñ—Ñ–µ—Å—Ç –ø—Ä–∞—Ü—é–≤–∞–≤)\n",
    "            if keep_columns:\n",
    "                base = [\"ticker\", \"dt\"]\n",
    "                cols = base + [c for c in keep_columns if c in dfo.columns and c not in base]\n",
    "                cols = [c for c in cols if c in dfo.columns]\n",
    "                dfo = dfo[cols]\n",
    "\n",
    "            return tk, dfo, None\n",
    "        except Exception as e:\n",
    "            return tk, None, str(e)\n",
    "\n",
    "    if parallel_tickers:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers_tickers) as ex:\n",
    "            futs = {ex.submit(_one, tk): tk for tk in tickers}\n",
    "            processed = 0\n",
    "            for f in as_completed(futs):\n",
    "                tk = futs[f]\n",
    "                processed += 1\n",
    "                is_last = (processed == total)\n",
    "                try:\n",
    "                    tk_, df, err = f.result()\n",
    "                except Exception as e:\n",
    "                    errors.append({\"ticker\": tk, \"error\": str(e)})\n",
    "                    if _should_print(processed, True, is_last) and verbose:\n",
    "                        _p(f\"[{processed}/{total}] ‚úó {tk}: {e} | {_progress_line(processed)}\")\n",
    "                    _maybe_log_mem(processed)\n",
    "                    continue\n",
    "\n",
    "                if err:\n",
    "                    errors.append({\"ticker\": tk_, \"error\": err})\n",
    "                    if _should_print(processed, True, is_last) and verbose:\n",
    "                        _p(f\"[{processed}/{total}] ‚úó {tk_}: {err} | {_progress_line(processed)}\")\n",
    "                else:\n",
    "                    if df is not None and not df.empty:\n",
    "                        _append_df_for_buffer(df)\n",
    "                        _update_day_cache_from_df(df)\n",
    "                        if _should_print(processed, False, is_last) and verbose:\n",
    "                            _p(f\"[{processed}/{total}] ‚úì {tk_}: {len(df):,} —Ä—è–¥–∫—ñ–≤ | {_progress_line(processed)}\")\n",
    "                    else:\n",
    "                        if _should_print(processed, False, is_last) and verbose:\n",
    "                            _p(f\"[{processed}/{total}] ‚Äì {tk_}: empty | {_progress_line(processed)}\")\n",
    "\n",
    "                del df\n",
    "                gc.collect()\n",
    "                _maybe_log_mem(processed)\n",
    "                _maybe_flush_by_thresholds(processed)\n",
    "    else:\n",
    "        for i, tk in enumerate(tickers, 1):\n",
    "            is_last = (i == total)\n",
    "            tk_, df, err = _one(tk)\n",
    "            if err:\n",
    "                errors.append({\"ticker\": tk_, \"error\": err})\n",
    "                if _should_print(i, True, is_last) and verbose:\n",
    "                    _p(f\"[{i}/{total}] ‚úó {tk_}: {err} | {_progress_line(i)}\")\n",
    "            else:\n",
    "                if df is not None and not df.empty:\n",
    "                    _append_df_for_buffer(df)\n",
    "                    _update_day_cache_from_df(df)\n",
    "                    if _should_print(i, False, is_last) and verbose:\n",
    "                        _p(f\"[{i}/{total}] ‚úì {tk_}: {len(df):,} —Ä—è–¥–∫—ñ–≤ | {_progress_line(i)}\")\n",
    "                else:\n",
    "                    if _should_print(i, False, is_last) and verbose:\n",
    "                        _p(f\"[{i}/{total}] ‚Äì {tk_}: empty | {_progress_line(i)}\")\n",
    "\n",
    "            del df\n",
    "            gc.collect()\n",
    "            _maybe_log_mem(i)\n",
    "            _maybe_flush_by_thresholds(i)\n",
    "\n",
    "    _flush()\n",
    "    total_elapsed = time.time() - start_time\n",
    "    if verbose:\n",
    "        _p(_mem_str(\"   üîö —Ñ—ñ–Ω–∞–ª ¬∑ \"))\n",
    "        _p(f\"üèÅ –ì–æ—Ç–æ–≤–æ. –ù–æ–≤–∏—Ö —Ä—è–¥–∫—ñ–≤: {written_total:,} ‚Üí {out_dir} | –ß–∞—Å: {_fmt_eta(total_elapsed)}\")\n",
    "    return pd.DataFrame(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27a981",
   "metadata": {
    "papermill": {
     "duration": 0.015183,
     "end_time": "2026-01-27T21:14:12.107311",
     "exception": false,
     "start_time": "2026-01-27T21:14:12.092128",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f393af7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.123434Z",
     "iopub.status.busy": "2026-01-27T21:14:12.123434Z",
     "iopub.status.idle": "2026-01-27T21:14:12.156352Z",
     "shell.execute_reply": "2026-01-27T21:14:12.155215Z"
    },
    "papermill": {
     "duration": 0.034144,
     "end_time": "2026-01-27T21:14:12.157578",
     "exception": false,
     "start_time": "2026-01-27T21:14:12.123434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>value</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>country</th>\n",
       "      <th>exchange</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>53276.79</td>\n",
       "      <td>Medical Equipment &amp; Devices</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>New York</td>\n",
       "      <td>38456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>1196849.64</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>New York</td>\n",
       "      <td>15162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>1291460.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>36.46</td>\n",
       "      <td>Software</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NASDAQ CM</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACI</td>\n",
       "      <td>16875.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>ZYBT</td>\n",
       "      <td>71.57</td>\n",
       "      <td>Biotech &amp; Pharma</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NASDAQ CM</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>49039.53</td>\n",
       "      <td>Biotech &amp; Pharma</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>NASDAQ GS</td>\n",
       "      <td>1727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>ZYN</td>\n",
       "      <td>412.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>4482.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>ZYXIQ</td>\n",
       "      <td>4482.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6698 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker       value                         lvl3        country  \\\n",
       "0         A    53276.79  Medical Equipment & Devices  UNITED STATES   \n",
       "1        AA  1196849.64              Metals & Mining  UNITED STATES   \n",
       "2      AAAU  1291460.13                          NaN            NaN   \n",
       "3      AACG       36.46                     Software  UNITED STATES   \n",
       "4      AACI    16875.54                          NaN            NaN   \n",
       "...     ...         ...                          ...            ...   \n",
       "6693   ZYBT       71.57             Biotech & Pharma  UNITED STATES   \n",
       "6694   ZYME    49039.53             Biotech & Pharma  UNITED STATES   \n",
       "6695    ZYN      412.24                          NaN            NaN   \n",
       "6696   ZYXI     4482.24                          NaN            NaN   \n",
       "6697  ZYXIQ     4482.24                          NaN            NaN   \n",
       "\n",
       "       exchange  market_cap  \n",
       "0      New York     38456.0  \n",
       "1      New York     15162.0  \n",
       "2           NaN         NaN  \n",
       "3     NASDAQ CM        34.0  \n",
       "4           NaN         NaN  \n",
       "...         ...         ...  \n",
       "6693  NASDAQ CM        48.0  \n",
       "6694  NASDAQ GS      1727.0  \n",
       "6695        NaN         NaN  \n",
       "6696        NaN         NaN  \n",
       "6697        NaN         NaN  \n",
       "\n",
       "[6698 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9937d869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.168540Z",
     "iopub.status.busy": "2026-01-27T21:14:12.168540Z",
     "iopub.status.idle": "2026-01-27T21:14:12.200985Z",
     "shell.execute_reply": "2026-01-27T21:14:12.200217Z"
    },
    "papermill": {
     "duration": 0.032445,
     "end_time": "2026-01-27T21:14:12.200985",
     "exception": false,
     "start_time": "2026-01-27T21:14:12.168540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker\n",
      "0    SPY\n",
      "1    IWM\n",
      "2    GDX\n",
      "3    XLF\n",
      "4    QQQ\n",
      "5    XLE\n",
      "6    KRE\n",
      "7   BITO\n",
      "8   KWEB\n",
      "9   SOXL\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# —Å–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä—ñ–≤\n",
    "tickers = ['SPY','IWM','GDX','XLF','QQQ','XLE','KRE','BITO','KWEB','SOXL']\n",
    "\n",
    "# —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame\n",
    "df = pd.DataFrame(tickers, columns=['ticker'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ae4d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.216216Z",
     "iopub.status.busy": "2026-01-27T21:14:12.216216Z",
     "iopub.status.idle": "2026-01-27T21:14:12.233182Z",
     "shell.execute_reply": "2026-01-27T21:14:12.232309Z"
    },
    "papermill": {
     "duration": 0.032197,
     "end_time": "2026-01-27T21:14:12.233182",
     "exception": false,
     "start_time": "2026-01-27T21:14:12.200985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep = [\"ticker\",\"dt\",\"o\",\"h\",\"l\",\"c\",\"v\"]\n",
    "\n",
    "# errs = fetch_intraday_v3_for_all_tickers(\n",
    "#     tickers_df=top1500,\n",
    "#     start= start,\n",
    "#     end= end,\n",
    "#     interval=1,\n",
    "#     chunk_days=30,\n",
    "#     parallel_chunks=True,\n",
    "#     max_workers_chunks=6,\n",
    "#     parallel_tickers=True,\n",
    "#     max_workers_tickers=1,\n",
    "#     save_every=50,\n",
    "#     out_dir=str(BATCH_DIR),\n",
    "#     out_prefix=\"intraday\",\n",
    "#     gzip=True,\n",
    "#     keep_columns=keep,\n",
    "#     verbose=True,\n",
    "#     include_blue_ocean=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c898e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.248108Z",
     "iopub.status.busy": "2026-01-27T21:14:12.248108Z",
     "iopub.status.idle": "2026-01-27T21:14:12.280459Z",
     "shell.execute_reply": "2026-01-27T21:14:12.279718Z"
    },
    "papermill": {
     "duration": 0.04838,
     "end_time": "2026-01-27T21:14:12.281562",
     "exception": false,
     "start_time": "2026-01-27T21:14:12.233182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob, gc, time\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pa = pq = None\n",
    "\n",
    "\n",
    "def collect_premarket_by_day_stream(\n",
    "    folder: str,\n",
    "    out_path: str = \"CRACEN/work/premarket_rows.parquet\",\n",
    "    pattern: str = \"intraday_*.csv.gz\",\n",
    "    start_time: str = \"00:00:00\",\n",
    "    end_time: str   = \"23:59:00\",\n",
    "    *,\n",
    "    date_min: Optional[str] = None,     # 'YYYY-MM-DD' inclusive\n",
    "    date_max: Optional[str] = None,     # 'YYYY-MM-DD' inclusive\n",
    "    aggregate: bool = False,            # False ‚Üí –≤—Å—ñ —Ä—è–¥–∫–∏, True ‚Üí –∞–≥—Ä–µ–≥–∞—Ü—ñ—è –ø–æ (ticker,date)\n",
    "    chunksize: int = 200_000,           # —è–∫—â–æ –º–∞–ª–æ RAM ‚Äî —â–µ –∑–º–µ–Ω—à\n",
    "    compression: str = \"snappy\",\n",
    "    log_every: int = 5,\n",
    "    sort_chunk: bool = True,            # sort within each chunk (ticker,dt)\n",
    "):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ {folder}/{pattern}, —Ñ—ñ–ª—å—Ç—Ä—É—î —Ä—è–¥–∫–∏:\n",
    "      - –∑–∞ —á–∞—Å–æ–≤–∏–º –≤—ñ–∫–Ω–æ–º [start_time..end_time] –ø–æ –ø—ñ–¥—Ä—è–¥–∫—É —á–∞—Å—É –≤ 'dt'\n",
    "      - –æ–ø—Ü—ñ–π–Ω–æ –∑–∞ –¥–∞—Ç–æ—é [date_min..date_max] –ø–æ –ø—ñ–¥—Ä—è–¥–∫—É –¥–∞—Ç–∏ –≤ 'dt'\n",
    "    —ñ —Å—Ç—Ä—ñ–º–æ–≤–æ –ø–∏—à–µ —É –æ–¥–∏–Ω Parquet.\n",
    "\n",
    "    –û—á—ñ–∫—É–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —É CSV: 'ticker','dt','o','h','l','c','v'.\n",
    "    dt –º–∞—î —Ñ–æ—Ä–º–∞—Ç 'YYYY-MM-DD HH:MM:SS' (string).\n",
    "    \"\"\"\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow: pip install pyarrow\")\n",
    "\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤ {pattern} —É {folder}\")\n",
    "\n",
    "    # normalize dates (string compare works for YYYY-MM-DD)\n",
    "    if date_min is not None:\n",
    "        date_min = str(date_min)[:10]\n",
    "    if date_max is not None:\n",
    "        date_max = str(date_max)[:10]\n",
    "    if date_min is not None and date_max is not None and date_min > date_max:\n",
    "        date_min, date_max = date_max, date_min\n",
    "\n",
    "    # —á–∏—Ç–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–µ, –¥–∞—î–º–æ –≤—É–∑—å–∫—ñ —Ç–∏–ø–∏\n",
    "    usecols = [\"ticker\", \"dt\", \"o\", \"h\", \"l\", \"c\", \"v\"]\n",
    "    dtypes = {\n",
    "        \"ticker\": \"string\",\n",
    "        \"dt\": \"string\",         # –≤–∞–∂–ª–∏–≤–æ: —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ –ø–æ –ø—ñ–¥—Ä—è–¥–∫—É, –±–µ–∑ datetime-–ø–∞—Ä—Å–∏–Ω–≥—É\n",
    "        \"o\": \"float32\",\n",
    "        \"h\": \"float32\",\n",
    "        \"l\": \"float32\",\n",
    "        \"c\": \"float32\",\n",
    "        \"v\": \"float32\",\n",
    "    }\n",
    "\n",
    "    # –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∏—Ö–æ–¥—É\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    writer = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_in = total_out = 0\n",
    "    file_idx = 0\n",
    "\n",
    "    rng_str = \"\"\n",
    "    if date_min or date_max:\n",
    "        rng_str = f\"  date=[{date_min or '‚Ä¶'}..{date_max or '‚Ä¶'}]\"\n",
    "    print(f\"‚ñ∂Ô∏è START: {folder}/{pattern} ‚Üí {out_path}  chunksize={chunksize:,}{rng_str}\")\n",
    "\n",
    "    try:\n",
    "        for path in files:\n",
    "            file_idx += 1\n",
    "            chunk_idx = 0\n",
    "\n",
    "            reader = pd.read_csv(\n",
    "                path,\n",
    "                compression=\"infer\",\n",
    "                usecols=usecols,\n",
    "                dtype=dtypes,\n",
    "                chunksize=chunksize,\n",
    "                low_memory=True,\n",
    "                # engine=\"pyarrow\",\n",
    "            )\n",
    "\n",
    "            for chunk in reader:\n",
    "                chunk_idx += 1\n",
    "                total_in += len(chunk)\n",
    "\n",
    "                # —à–≤–∏–¥–∫–µ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞–Ω–Ω—è –ø–æ —á–∞—Å—É: 'YYYY-MM-DD HH:MM:SS'\n",
    "                t = chunk[\"dt\"].str.slice(11, 19)\n",
    "                mask = (t >= start_time) & (t <= end_time)\n",
    "\n",
    "                # —Ñ—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç—ñ, —Ç–∞–∫–æ–∂ —Å—Ç—Ä–æ–∫–∞–º–∏\n",
    "                if date_min is not None or date_max is not None:\n",
    "                    d = chunk[\"dt\"].str.slice(0, 10)\n",
    "                    if date_min is not None:\n",
    "                        mask &= (d >= date_min)\n",
    "                    if date_max is not None:\n",
    "                        mask &= (d <= date_max)\n",
    "\n",
    "                cut = chunk.loc[mask].copy()\n",
    "\n",
    "                if cut.empty:\n",
    "                    del chunk, t, mask, cut\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "\n",
    "                if aggregate:\n",
    "                    # –¥–∞—Ç–∞ —è–∫ —Ä—è–¥–æ–∫ 'YYYY-MM-DD'\n",
    "                    cut[\"date\"] = cut[\"dt\"].str.slice(0, 10)\n",
    "                    # —Å–æ—Ä—Ç –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ\n",
    "                    if sort_chunk and {\"ticker\", \"date\"}.issubset(cut.columns):\n",
    "                        cut = cut.sort_values([\"ticker\", \"date\"], kind=\"mergesort\")\n",
    "                    grp = (cut.groupby([\"ticker\", \"date\"], as_index=False, observed=True)\n",
    "                              .agg({\"v\": \"sum\"}))  # –ø—Ä–∏–∫–ª–∞–¥: —Å—É–º–∞ –æ–±—Å—è–≥—É; –∑–º—ñ–Ω—é–π –∞–≥—Ä–µ–≥–∞—Ü—ñ—é –∑–∞ –ø–æ—Ç—Ä–µ–±–∏\n",
    "                    out_df = grp\n",
    "                else:\n",
    "                    # —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫ –≤ –º–µ–∂–∞—Ö —á–∞–Ω–∫–∞\n",
    "                    if sort_chunk and {\"ticker\", \"dt\"}.issubset(cut.columns):\n",
    "                        cut = cut.sort_values([\"ticker\", \"dt\"], kind=\"mergesort\")\n",
    "                    out_df = cut\n",
    "\n",
    "                table = pa.Table.from_pandas(out_df, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(out_path, table.schema, compression=compression)\n",
    "                writer.write_table(table)\n",
    "                total_out += len(out_df)\n",
    "\n",
    "                if (chunk_idx % log_every) == 0:\n",
    "                    elapsed = time.time() - t0\n",
    "                    print(f\"  [{file_idx}/{len(files)}] {os.path.basename(path)} \"\n",
    "                          f\"chunk {chunk_idx}  in={total_in:,}  out={total_out:,}  \"\n",
    "                          f\"elapsed={elapsed:,.1f}s\")\n",
    "\n",
    "                del chunk, t, mask, cut, out_df, table\n",
    "                gc.collect()\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE  in={total_in:,} ‚Üí out={total_out:,}  to {out_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda8fd4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5079d600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T21:14:12.293522Z",
     "iopub.status.busy": "2026-01-27T21:14:12.293522Z",
     "iopub.status.idle": "2026-01-27T21:14:12.670898Z",
     "shell.execute_reply": "2026-01-27T21:14:12.670898Z"
    },
    "papermill": {
     "duration": 0.378374,
     "end_time": "2026-01-27T21:14:12.670898",
     "exception": true,
     "start_time": "2026-01-27T21:14:12.292524",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow: pip install pyarrow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcollect_premarket_by_day_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBATCH_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWORK_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpremarket_rows.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m23:59:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 38\u001b[0m, in \u001b[0;36mcollect_premarket_by_day_stream\u001b[1;34m(folder, out_path, pattern, start_time, end_time, date_min, date_max, aggregate, chunksize, compression, log_every, sort_chunk)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m–ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ {folder}/{pattern}, —Ñ—ñ–ª—å—Ç—Ä—É—î —Ä—è–¥–∫–∏:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m  - –∑–∞ —á–∞—Å–æ–≤–∏–º –≤—ñ–∫–Ω–æ–º [start_time..end_time] –ø–æ –ø—ñ–¥—Ä—è–¥–∫—É —á–∞—Å—É –≤ 'dt'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mdt –º–∞—î —Ñ–æ—Ä–º–∞—Ç 'YYYY-MM-DD HH:MM:SS' (string).\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow: pip install pyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, pattern)))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: –ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow: pip install pyarrow"
     ]
    }
   ],
   "source": [
    "collect_premarket_by_day_stream(\n",
    "    folder=str(BATCH_DIR),\n",
    "    out_path=str(WORK_DIR / \"premarket_rows.parquet\"),\n",
    "    start_time=\"00:00:00\",\n",
    "    end_time=\"23:59:00\",\n",
    "    date_min=start_date_str,      # NEW\n",
    "    date_max=end_date_str,        # NEW\n",
    "    aggregate=False,\n",
    "    chunksize=150_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34efb9a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "def build_oc_from_intraday_folder(\n",
    "    folder: str,\n",
    "    out_path: Optional[str] = None,\n",
    "    chunksize: int = 250_000,\n",
    ") -> pd.DataFrame:\n",
    "    files = sorted(\n",
    "        glob.glob(os.path.join(folder, \"intraday_*.csv\")) +\n",
    "        glob.glob(os.path.join(folder, \"intraday_*.csv.gz\"))\n",
    "    )\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ intraday_*.csv[.gz] —É: {folder}\")\n",
    "\n",
    "    OPEN_FROM, OPEN_TO   = \"09:30\", \"09:35\"\n",
    "    CLOSE_FROM, CLOSE_TO = \"15:55\", \"15:59\"\n",
    "\n",
    "    best_open:  Dict[Tuple[str, str], Tuple[str, float]] = {}\n",
    "    best_close: Dict[Tuple[str, str], Tuple[str, float]] = {}\n",
    "\n",
    "    usecols = [\"ticker\", \"dt\", \"o\", \"c\"]\n",
    "\n",
    "    for path in files:\n",
    "        for chunk in pd.read_csv(path, compression=\"infer\", low_memory=False,\n",
    "                                 chunksize=chunksize, usecols=usecols):\n",
    "            cols = set(map(str, chunk.columns))\n",
    "            missing = set(usecols) - cols\n",
    "            if missing:\n",
    "                raise KeyError(f\"{os.path.basename(path)}: –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {missing}\")\n",
    "\n",
    "            dt = chunk[\"dt\"].astype(str)\n",
    "            if \"o\" in chunk.columns:\n",
    "                chunk[\"o\"] = pd.to_numeric(chunk[\"o\"], errors=\"coerce\")\n",
    "            if \"c\" in chunk.columns:\n",
    "                chunk[\"c\"] = pd.to_numeric(chunk[\"c\"], errors=\"coerce\")\n",
    "\n",
    "            hhmm = dt.str.slice(11, 16)\n",
    "            date = dt.str.slice(0, 10)\n",
    "\n",
    "            # OPEN\n",
    "            mask_o = (hhmm >= OPEN_FROM) & (hhmm <= OPEN_TO) & chunk[\"o\"].notna()\n",
    "            if mask_o.any():\n",
    "                cut = chunk.loc[mask_o, [\"ticker\", \"o\"]].copy()\n",
    "                cut[\"date\"] = date[mask_o].values\n",
    "                cut[\"dt\"]   = dt[mask_o].values\n",
    "                cut = cut.sort_values(\"dt\").drop_duplicates([\"ticker\",\"date\"], keep=\"first\")\n",
    "                for r in cut.itertuples(index=False):\n",
    "                    key = (r.ticker, r.date)\n",
    "                    cur = best_open.get(key)\n",
    "                    if (cur is None) or (r.dt < cur[0]):\n",
    "                        best_open[key] = (r.dt, float(r.o))\n",
    "\n",
    "            # CLOSE\n",
    "            mask_c = (hhmm >= CLOSE_FROM) & (hhmm <= CLOSE_TO) & chunk[\"c\"].notna()\n",
    "            if mask_c.any():\n",
    "                cut = chunk.loc[mask_c, [\"ticker\", \"c\"]].copy()\n",
    "                cut[\"date\"] = date[mask_c].values\n",
    "                cut[\"dt\"]   = dt[mask_c].values\n",
    "                cut = cut.sort_values(\"dt\").drop_duplicates([\"ticker\",\"date\"], keep=\"last\")\n",
    "                for r in cut.itertuples(index=False):\n",
    "                    key = (r.ticker, r.date)\n",
    "                    cur = best_close.get(key)\n",
    "                    if (cur is None) or (r.dt > cur[0]):\n",
    "                        best_close[key] = (r.dt, float(r.c))\n",
    "\n",
    "    opens_df = pd.DataFrame(\n",
    "        [(t, d, v) for (t, d), (_, v) in best_open.items()],\n",
    "        columns=[\"ticker\", \"date\", \"open\"]\n",
    "    )\n",
    "    closes_df = pd.DataFrame(\n",
    "        [(t, d, v) for (t, d), (_, v) in best_close.items()],\n",
    "        columns=[\"ticker\", \"date\", \"close\"]\n",
    "    )\n",
    "\n",
    "    out = pd.merge(opens_df, closes_df, on=[\"ticker\",\"date\"], how=\"outer\") \\\n",
    "            .sort_values([\"ticker\",\"date\"], kind=\"stable\") \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "    # ‚¨áÔ∏è –û–ö–†–£–ì–õ–ï–ù–ù–Ø –î–û 2 –ó–ù–ê–ö–Ü–í\n",
    "    if \"open\" in out.columns:\n",
    "        out[\"open\"] = pd.to_numeric(out[\"open\"], errors=\"coerce\").round(2)\n",
    "    if \"close\" in out.columns:\n",
    "        out[\"close\"] = pd.to_numeric(out[\"close\"], errors=\"coerce\").round(2)\n",
    "\n",
    "    # –∑–∞–≤–∂–¥–∏ Parquet\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(folder, \"oc.parquet\")\n",
    "    if not out_path.endswith(\".parquet\"):\n",
    "        out_path += \".parquet\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    out.to_parquet(out_path, index=False)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66365954",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_oc = build_oc_from_intraday_folder(\n",
    "    folder=str(BATCH_DIR),\n",
    "    out_path=str(WORK_DIR / \"oc.parquet\"),\n",
    "    chunksize=250_000\n",
    ")\n",
    "print(df_oc.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986472c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pq = None\n",
    "\n",
    "\n",
    "def enrich_with_day_open_and_prev_close(df_intraday: pd.DataFrame,\n",
    "                                        df_oc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –î–æ–¥–∞—î –¥–æ –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞ —ñ–Ω—Ç—Ä–∞–¥–µ—é 'open' –¥–Ω—è —Ç–∞ 'prev_close' (—É—á–æ—Ä–∞—à–Ω—ñ–π close).\n",
    "    –û—á—ñ–∫—É—î –≤ df_intraday: ['ticker','dt',...]\n",
    "              –≤ df_oc:       ['ticker','date','open','close']\n",
    "    \"\"\"\n",
    "    out = df_intraday.copy()\n",
    "    out[\"dt\"] = pd.to_datetime(out[\"dt\"], errors=\"coerce\")\n",
    "    out = out[out[\"dt\"].notna()].copy()\n",
    "    out[\"date\"] = out[\"dt\"].dt.date.astype(\"string\")\n",
    "\n",
    "    oc = df_oc.copy()\n",
    "    oc[\"date\"] = pd.to_datetime(oc[\"date\"], errors=\"coerce\").dt.date.astype(\"string\")\n",
    "\n",
    "    oc_sorted = (\n",
    "        oc.sort_values([\"ticker\", \"date\"])\n",
    "          .groupby(\"ticker\", as_index=False, group_keys=False)\n",
    "          .apply(lambda d: d.assign(prev_close=d[\"close\"].shift(1)))\n",
    "    )\n",
    "\n",
    "    right = oc_sorted[[\"ticker\", \"date\", \"open\", \"prev_close\"]].copy()\n",
    "    out = out.merge(right, on=[\"ticker\", \"date\"], how=\"left\", validate=\"m:1\")\n",
    "    out = out.sort_values([\"ticker\", \"dt\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_intraday_and_oc_to_file(\n",
    "    intraday_parquet_path: str,\n",
    "    oc_parquet_path: str,\n",
    "    out_path: str,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    log_every_batches: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    –ß–∏—Ç–∞—î –≤–µ–ª–∏–∫–∏–π Parquet –±–∞—Ç—á–∞–º–∏ (—á–µ—Ä–µ–∑ pyarrow.parquet.ParquetFile.iter_batches),\n",
    "    –º–µ—Ä–¥–∂–∏—Ç—å —ñ–∑ –¥–µ–Ω–Ω–∏–º–∏ open/prev_close –∑ oc.parquet —ñ –ø–∏—à–µ —É out_path\n",
    "    (.parquet –∞–±–æ .csv/.csv.gz).\n",
    "\n",
    "    intraday_parquet_path  ‚Äî —à–ª—è—Ö –¥–æ CRACEN/premarket_rows.parquet\n",
    "    oc_parquet_path        ‚Äî —à–ª—è—Ö –¥–æ CRACEN/oc.parquet\n",
    "    out_path               ‚Äî —à–ª—è—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (—É —Ç—ñ–π –∂–µ –ø–∞–ø—Ü—ñ)\n",
    "    \"\"\"\n",
    "    if pq is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow.parquet).\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) –î–µ–Ω–Ω–∏–π OC (—É –ø–∞–º'—è—Ç—ñ)\n",
    "    df_oc = pd.read_parquet(oc_parquet_path)\n",
    "    req_oc = {\"ticker\", \"date\", \"open\", \"close\"}\n",
    "    missing = req_oc - set(df_oc.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"OC-—Ñ–∞–π–ª –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∫–æ–ª–æ–Ω–æ–∫: {sorted(missing)}\")\n",
    "\n",
    "    # 2) –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    _, ext = os.path.splitext(out_path.lower())\n",
    "    if ext not in [\".csv\", \".gz\", \".parquet\", \".csv.gz\"]:\n",
    "        raise ValueError(\"–ü—ñ–¥—Ç—Ä–∏–º–∞–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏: .csv, .csv.gz, .parquet\")\n",
    "\n",
    "    parquet_writer = None\n",
    "    total_rows = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    # 3) –ü–æ—Ç–æ–∫–æ–≤–µ —á–∏—Ç–∞–Ω–Ω—è parquet\n",
    "    pf = pq.ParquetFile(intraday_parquet_path)\n",
    "    cols = [\"ticker\", \"dt\", \"o\", \"h\", \"l\", \"c\", \"v\"]\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START: merge ‚Üí {out_path} | batch_rows‚âà{batch_rows:,}\")\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=batch_rows, columns=cols):\n",
    "        batch_idx += 1\n",
    "        df_batch = batch.to_pandas()\n",
    "\n",
    "        df_enriched = enrich_with_day_open_and_prev_close(df_batch, df_oc)\n",
    "\n",
    "        if out_path.endswith(\".parquet\"):\n",
    "            table = pa.Table.from_pandas(df_enriched)\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(out_path, table.schema, compression=\"snappy\")\n",
    "            parquet_writer.write_table(table)\n",
    "        else:\n",
    "            header = not os.path.exists(out_path)\n",
    "            df_enriched.to_csv(\n",
    "                out_path,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "                header=header,\n",
    "                compression=\"infer\" if out_path.endswith(\".gz\") else None,\n",
    "            )\n",
    "\n",
    "        total_rows += len(df_batch)\n",
    "        if batch_idx % log_every_batches == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"[batch {batch_idx:>4}] rows={total_rows:,}  elapsed={elapsed:,.1f}s\")\n",
    "\n",
    "        del df_batch, df_enriched, batch\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer is not None:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE  rows={total_rows:,}  -> {out_path}  in {time.time()-t0:,.1f}s\")\n",
    "\n",
    "\n",
    "def merge_intraday_and_oc_in_folder(\n",
    "    folder: str = \"CRACEN\",\n",
    "    intraday_filename: str = \"premarket_rows.parquet\",\n",
    "    oc_filename: str = \"oc.parquet\",\n",
    "    out_filename: str = \"premarket_rows_enriched.parquet\",\n",
    "    batch_rows: int = 2_000_000,\n",
    "    log_every_batches: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ó—Ä—É—á–Ω–∏–π –æ–±–≥–æ—Ä—Ç–Ω–∏–∫ —Å–∞–º–µ –ø—ñ–¥ —Ç–≤–æ—é –ø–∞–ø–∫—É –∑—ñ —Å–∫—Ä—ñ–Ω—É.\n",
    "    –®—É–∫–∞—î:\n",
    "      - {folder}/{intraday_filename}\n",
    "      - {folder}/{oc_filename}\n",
    "    —ñ –ø–∏—à–µ:\n",
    "      - {folder}/{out_filename}\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î —à–ª—è—Ö –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.\n",
    "    \"\"\"\n",
    "    intraday_path = os.path.join(folder, intraday_filename)\n",
    "    oc_path = os.path.join(folder, oc_filename)\n",
    "    out_path = os.path.join(folder, out_filename)\n",
    "\n",
    "    if not os.path.exists(intraday_path):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ç—Ä–∞–¥–µ–π: {intraday_path}\")\n",
    "    if not os.path.exists(oc_path):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ OC-—Ñ–∞–π–ª: {oc_path}\")\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    merge_intraday_and_oc_to_file(\n",
    "        intraday_parquet_path=intraday_path,\n",
    "        oc_parquet_path=oc_path,\n",
    "        out_path=out_path,\n",
    "        batch_rows=batch_rows,\n",
    "        log_every_batches=log_every_batches,\n",
    "    )\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb5da4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = merge_intraday_and_oc_in_folder(folder=str(WORK_DIR))\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–∞–π–ª—ñ:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9569a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pq = None\n",
    "\n",
    "\n",
    "def add_pct_vs_prev_close_parquet(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    round_to: Optional[int] = None,\n",
    "    log_every: int = 5,\n",
    "    value_col: str = \"o\",        # ‚Üê —â–æ –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ prev_close (–¥–µ—Ñ–æ–ª—Ç: open)\n",
    "    out_col: str = \"Stack%\",     # –Ω–∞–∑–≤–∞ –∫–æ–ª–æ–Ω–∫–∏ –∑ –≤—ñ–¥—Å–æ—Ç–∫–æ–º\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ü–æ—Ç–æ–∫–æ–≤–æ —á–∏—Ç–∞—î –≤–µ–ª–∏–∫–∏–π Parquet, –¥–æ–¥–∞—î –∫–æ–ª–æ–Ω–∫—É:\n",
    "        out_col = (value_col / prev_close - 1) * 100\n",
    "    —ñ –ø–∏—à–µ –Ω–æ–≤–∏–π Parquet.\n",
    "    \"\"\"\n",
    "    if pq is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow.parquet).\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    pf = pq.ParquetFile(input_path)\n",
    "    parquet_writer = None\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START  file={input_path}  ‚Üí  {output_path}  batch_rows‚âà{batch_rows:,}  using={value_col}\")\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=batch_rows):\n",
    "        batches += 1\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫\n",
    "        if \"prev_close\" not in df.columns:\n",
    "            raise KeyError(\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ 'prev_close'.\")\n",
    "        if value_col not in df.columns:\n",
    "            raise KeyError(f\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ '{value_col}'.\")\n",
    "\n",
    "        # —á–∏—Å–ª–æ–≤—ñ —Ç–∏–ø–∏\n",
    "        df[\"prev_close\"] = pd.to_numeric(df[\"prev_close\"], errors=\"coerce\")\n",
    "        df[value_col]    = pd.to_numeric(df[value_col],    errors=\"coerce\")\n",
    "\n",
    "        # –Ω—É–ª—å–æ–≤–∏–π prev_close ‚Üí NaN, —â–æ–± –Ω–µ –¥—ñ–ª–∏—Ç–∏ –Ω–∞ 0\n",
    "        denom = df[\"prev_close\"].replace({0: np.nan})\n",
    "\n",
    "        # —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ % vs prev_close\n",
    "        pct = (df[value_col] / denom - 1.0) * 100.0\n",
    "\n",
    "        # —â–µ —Ä–∞–∑ –≥–∞—Ä–∞–Ω—Ç—É—î–º–æ float dtype (–±–µ–∑ object/pd.NA –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ)\n",
    "        pct = pd.to_numeric(pct, errors=\"coerce\")\n",
    "\n",
    "        if round_to is not None:\n",
    "            pct = pct.round(int(round_to))\n",
    "\n",
    "        df[out_col] = pct\n",
    "\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(\n",
    "                output_path,\n",
    "                table.schema,\n",
    "                compression=\"snappy\",\n",
    "            )\n",
    "        parquet_writer.write_table(table)\n",
    "\n",
    "        total += len(df)\n",
    "        if batches % log_every == 0:\n",
    "            print(f\"[batch {batches:>4}] rows={total:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer is not None:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE  rows={total:,}  ‚Üí  {output_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def add_pct_vs_prev_close_in_folder(\n",
    "    folder: str = \"CRACEN\",\n",
    "    input_filename: str = \"premarket_rows_enriched.parquet\",\n",
    "    output_filename: str = \"premarket_rows_enriched_with_pct.parquet\",\n",
    "    batch_rows: int = 2_000_000,\n",
    "    round_to: Optional[int] = 2,\n",
    "    log_every: int = 5,\n",
    "    value_col: str = \"o\",          # ‚Üê open vs prev_close\n",
    "    out_col: str = \"Stack%\",       # –Ω–∞–∑–≤–∞ –Ω–æ–≤–æ—ó –∫–æ–ª–æ–Ω–∫–∏\n",
    ") -> str:\n",
    "    \"\"\"–û–±–≥–æ—Ä—Ç–∫–∞ –ø—ñ–¥ —Ç–≤–æ—é –ø–∞–ø–∫—É CRACEN.\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    src = os.path.join(folder, input_filename)\n",
    "    dst = os.path.join(folder, output_filename)\n",
    "    if not os.path.exists(src):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª: {src}\")\n",
    "    return add_pct_vs_prev_close_parquet(\n",
    "        input_path=src,\n",
    "        output_path=dst,\n",
    "        batch_rows=batch_rows,\n",
    "        round_to=round_to,\n",
    "        log_every=log_every,\n",
    "        value_col=value_col,\n",
    "        out_col=out_col,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffe676",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = add_pct_vs_prev_close_in_folder(folder=str(WORK_DIR))\n",
    "print(\"–ì–æ—Ç–æ–≤–æ:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d925296",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re, time, math\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pa = pq = None\n",
    "\n",
    "\n",
    "# ---------- –ù–ê–î–Ü–ô–ù–ï –ß–ò–¢–ê–ù–ù–Ø –ú–ê–ü–Ü–ù–ì–£ (ticker, bench) ----------\n",
    "\n",
    "def _read_mapping(mapping_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ß–∏—Ç–∞—î —Ñ–∞–π–ª –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—è–º–∏ —Ç—ñ–∫–µ—Ä‚Üî–±–µ–Ω—á. –ü—ñ–¥—Ç—Ä–∏–º–∫–∞:\n",
    "      - CSV (—Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫ ; –∞–±–æ , –∞–±–æ \\t), –∑/–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "      - XLSX\n",
    "      - –í–∏–ø–∞–¥–æ–∫ \"–≤—Å–µ –≤ –æ–¥–Ω—ñ–π –∫–æ–ª–æ–Ω—Ü—ñ\": 'AAPL;SPY' / 'AAPL,SPY'\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î DF: ['ticker','bench'] (UPPER, –±–µ–∑ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤).\n",
    "    \"\"\"\n",
    "    p = mapping_path\n",
    "    ext = os.path.splitext(p)[1].lower()\n",
    "\n",
    "    if ext in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(p)\n",
    "    else:\n",
    "        # 1) —Å–ø—Ä–æ–±–∞ autodetect\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep=None, engine=\"python\")\n",
    "        except Exception:\n",
    "            # 2) —Å–ø—Ä–æ–±–∏ –∑ —è–≤–Ω–∏–º–∏ —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫–∞–º–∏\n",
    "            for sep_try in (\";\", \",\", \"\\t\", \"|\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(p, sep=sep_try, engine=\"python\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    df = None\n",
    "            if df is None:\n",
    "                # 3) –º–æ–∂–ª–∏–≤–æ, —Ñ–∞–π–ª –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "                try:\n",
    "                    df = pd.read_csv(p, header=None, sep=None, engine=\"python\")\n",
    "                except Exception:\n",
    "                    # 4) –æ—Å—Ç–∞–Ω–Ω—è —Å–ø—Ä–æ–±–∞ ‚Äî —è–≤–Ω—ñ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
    "                    for sep_try in (\";\", \",\", \"\\t\", \"|\"):\n",
    "                        try:\n",
    "                            df = pd.read_csv(p, header=None, sep=sep_try, engine=\"python\")\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            df = None\n",
    "                    if df is None:\n",
    "                        raise\n",
    "\n",
    "    # –Ø–∫—â–æ –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —ñ –≤–∏–≥–ª—è–¥–∞—î —è–∫ \"AAPL;SPY\" ‚Äî —Å–ø–ª—ñ—Ç–∏–º–æ\n",
    "    if df.shape[1] == 1:\n",
    "        col = df.columns[0]\n",
    "        # –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å —Ä–æ–∑–±–∏—Ç–∏ –∑–∞ ; –∞–±–æ , –∞–±–æ \\t\n",
    "        s = df[col].astype(str)\n",
    "        if s.str.contains(\";\").any():\n",
    "            parts = s.str.split(\";\", n=1, expand=True)\n",
    "        elif s.str.contains(\",\").any():\n",
    "            parts = s.str.split(\",\", n=1, expand=True)\n",
    "        elif s.str.contains(\"\\t\").any():\n",
    "            parts = s.str.split(\"\\t\", n=1, expand=True)\n",
    "        else:\n",
    "            raise KeyError(\"–ù–µ –º–æ–∂—É –∑–Ω–∞–π—Ç–∏ —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫ –º—ñ–∂ 'ticker' —ñ 'bench'.\")\n",
    "        df = pd.DataFrame({\"ticker\": parts[0], \"bench\": parts[1]})\n",
    "\n",
    "    # –í–≥–∞–¥—É—î–º–æ –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    tcol = cols.get(\"ticker\") or cols.get(\"symbol\")\n",
    "    bcol = cols.get(\"bench\") or cols.get(\"benchmark\") or cols.get(\"etf\")\n",
    "\n",
    "    # –Ø–∫—â–æ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –Ω–µ–º–∞, –∞–ª–µ 2 –∫–æ–ª–æ–Ω–∫–∏ ‚Äî –ø—Ä–∏–π–º–∞—î–º–æ —ó—Ö —è–∫ ticker/bench\n",
    "    if tcol is None or bcol is None:\n",
    "        if df.shape[1] >= 2:\n",
    "            tcol = df.columns[0]\n",
    "            bcol = df.columns[1]\n",
    "        else:\n",
    "            raise KeyError(\"–ü–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ 'ticker' —ñ 'bench' —É mapping-—Ñ–∞–π–ª—ñ.\")\n",
    "\n",
    "    out = (\n",
    "        df[[tcol, bcol]]\n",
    "        .rename(columns={tcol: \"ticker\", bcol: \"bench\"})\n",
    "        .dropna()\n",
    "    )\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "    out[\"bench\"]  = out[\"bench\"].astype(str).str.strip().str.upper()\n",
    "    out = out[out[\"ticker\"] != \"\"]\n",
    "    out = out[out[\"bench\"]  != \"\"]\n",
    "    return out.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "# ---------- –í–ò–¢–Ø–ì corr/beta –î–õ–Ø –ü–ê–† (X, –§–Ü–ö–°–û–í–ê–ù–ò–ô BENCH) ----------\n",
    "\n",
    "def _fetch_corr_beta_pairs(\n",
    "    x_list: List[str],\n",
    "    bench: str,\n",
    "    *,\n",
    "    period: str = \"6 months\",\n",
    "    fmt: str = \"json_records\",\n",
    "    batch_size: int = 150,\n",
    "    min_batch_size: int = 10,\n",
    "    pause: float = 0.15,\n",
    "    max_retries: int = 3,\n",
    "    retry_backoff: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    results: List[pd.DataFrame] = []\n",
    "    i, n = 0, len(x_list)\n",
    "    b = bench\n",
    "\n",
    "    while i < n:\n",
    "        cur_bs = min(batch_size, n - i)\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            xs = x_list[i : i + cur_bs]\n",
    "            params = {\n",
    "                \"x_tickers\": \",\".join(xs),\n",
    "                \"y_tickers\": b,\n",
    "                \"period\": period,\n",
    "                \"as_pivot\": False,\n",
    "                \"format\": fmt,\n",
    "            }\n",
    "            try:\n",
    "                df_chunk = DatumApi.data_request(\"/calculations/corr_beta/v2\", params)\n",
    "                if df_chunk is not None and not df_chunk.empty:\n",
    "                    # –ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ä—è–¥–∫–∏ –¥–ª—è –ø–æ—Ç—Ä—ñ–±–Ω–æ–≥–æ bench (–Ω–∞ –≤–∏–ø–∞–¥–æ–∫, —è–∫—â–æ API –ø–æ–≤–µ—Ä–Ω–µ –∑–∞–π–≤–µ)\n",
    "                    if \"y_ticker\" in df_chunk.columns:\n",
    "                        df_chunk = df_chunk[df_chunk[\"y_ticker\"].astype(str).str.upper() == b]\n",
    "                    results.append(df_chunk)\n",
    "                i += len(xs)\n",
    "                time.sleep(pause)  # –ø–∞—É–∑–∞ —Ç—ñ–ª—å–∫–∏ –ø—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
    "                break\n",
    "            except Exception as e:\n",
    "                msg = str(e).lower()\n",
    "                # 414: –∑–º–µ–Ω—à—É—î–º–æ –õ–ò–®–ï –ø–æ—Ç–æ—á–Ω–∏–π –±–∞—Ç—á\n",
    "                if \"414\" in msg or \"uri too large\" in msg:\n",
    "                    if cur_bs <= min_batch_size:\n",
    "                        print(f\"‚ö†Ô∏è 414 –Ω–∞–≤—ñ—Ç—å –ø—Ä–∏ batch={cur_bs}. –ü—Ä–æ–ø—É—Å–∫–∞—é X[{i}:{i+cur_bs}] –¥–ª—è {b}.\")\n",
    "                        i += cur_bs\n",
    "                        break\n",
    "                    new_bs = max(min_batch_size, cur_bs // 2)\n",
    "                    print(f\"‚ö†Ô∏è 414 ‚Üí shrink {cur_bs} ‚Üí {new_bs} –¥–ª—è {b} —ñ –ø–æ–≤—Ç–æ—Ä—é—é.\")\n",
    "                    cur_bs = new_bs\n",
    "                    continue\n",
    "\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    print(f\"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫ X[{i}:{i+cur_bs}] –¥–ª—è {b} –ø—ñ—Å–ª—è {max_reTRIES} —Ä–µ—Ç—Ä–∞—ó–≤. –ü–æ–º–∏–ª–∫–∞: {e}\")\n",
    "                    i += cur_bs\n",
    "                    break\n",
    "                sleep_s = retry_backoff * (2 ** (attempt - 1))\n",
    "                print(f\"‚ö†Ô∏è  {e} ‚Üí retry {attempt}/{max_retries} —á–µ—Ä–µ–∑ {sleep_s:.1f}s\")\n",
    "                time.sleep(sleep_s)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "\n",
    "# ---------- –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø ----------\n",
    "\n",
    "def corr_beta_from_mapping_to_parquet(\n",
    "    mapping_path: str,\n",
    "    out_parquet_path: str,\n",
    "    *,\n",
    "    period: str = \"3 months\",\n",
    "    batch_size: int = 150,\n",
    "    min_batch_size: int = 10,\n",
    "    pause: float = 0.15,\n",
    "    max_retries: int = 3,\n",
    "    retry_backoff: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ß–∏—Ç–∞—î mapping (ticker, bench) –±—É–¥—å-—è–∫–æ–≥–æ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É,\n",
    "    –≥—Ä—É–ø—É—î —Ç—ñ–∫–µ—Ä–∏ –ø–æ –±–µ–Ω—á—É, –≤–∏—Ç—è–≥—É—î corr/beta –¥–ª—è –∫–æ–∂–Ω–æ—ó –≥—Ä—É–ø–∏\n",
    "    —ñ –ø–æ—Ç–æ–∫–æ–≤–æ –ø–∏—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É Parquet.\n",
    "    \"\"\"\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pa, pq) –¥–ª—è –∑–∞–ø–∏—Å—É .parquet\")\n",
    "\n",
    "    map_df = _read_mapping(mapping_path)\n",
    "\n",
    "    groups: Dict[str, List[str]] = (\n",
    "        map_df.groupby(\"bench\")[\"ticker\"].apply(lambda s: sorted(set(s.tolist()))).to_dict()\n",
    "    )\n",
    "\n",
    "    if os.path.exists(out_parquet_path):\n",
    "        os.remove(out_parquet_path)\n",
    "\n",
    "    writer: Optional[pq.ParquetWriter] = None\n",
    "    total_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START corr/beta ‚Üí {out_parquet_path} | –ø–µ—Ä—ñ–æ–¥={period} | –±–µ–Ω—á—ñ–≤={len(groups)}\")\n",
    "\n",
    "    for b, xs in sorted(groups.items()):\n",
    "        if not xs:\n",
    "            continue\n",
    "        print(f\"‚Äî Bench {b}: {len(xs)} —Ç–∏–∫–µ—Ä—ñ–≤\")\n",
    "        df_b = _fetch_corr_beta_pairs(\n",
    "            xs, b,\n",
    "            period=period,\n",
    "            batch_size=batch_size,\n",
    "            min_batch_size=min_batch_size,\n",
    "            pause=pause,\n",
    "            max_retries=max_retries,\n",
    "            retry_backoff=retry_backoff,\n",
    "        )\n",
    "        if df_b is None or df_b.empty:\n",
    "            continue\n",
    "\n",
    "        # —Ç–∏–ø–∏\n",
    "        for col in (\"corr\", \"beta\"):\n",
    "            if col in df_b.columns:\n",
    "                df_b[col] = pd.to_numeric(df_b[col], errors=\"coerce\")\n",
    "\n",
    "        table = pa.Table.from_pandas(df_b, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_parquet_path, table.schema, compression=\"snappy\")\n",
    "        writer.write_table(table)\n",
    "        total_rows += len(df_b)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE rows={total_rows:,} ‚Üí {out_parquet_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return out_parquet_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6a986",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = corr_beta_from_mapping_to_parquet(\n",
    "    mapping_path=str(WORK_DIR / \"ticker_bench_.csv\"),\n",
    "    out_parquet_path=str(WORK_DIR / \"corr_beta_pairs.parquet\"),\n",
    "    period=\"3 months\",\n",
    "    batch_size=150,\n",
    "    pause=0.15,\n",
    ")\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a58ca1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "y_list = [\"SPY\", \"IWM\", \"GDX\", \"XLF\", \"QQQ\", \"XLE\", \"KRE\", \"BITO\", \"SOXL\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346f4dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pa = pq = None\n",
    "\n",
    "\n",
    "def _load_corrbeta_mapping_parquet(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ß–∏—Ç–∞—î parquet –∑ –º–∞–ø–æ—é:\n",
    "      A) ['x_ticker','y_ticker','corr','beta'] (API-—Ñ–æ—Ä–º–∞), –∞–±–æ\n",
    "      B) ['ticker','bench','corr','beta'] (–≤–∂–µ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞).\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î DF: ['ticker','bench','corr','beta'] (UPPER, –±–µ–∑ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤; –ø—Ä–∏ –¥—É–±–ª—è—Ö\n",
    "    –ª–∏—à–∞—î–º–æ —Ä—è–¥–æ–∫ —ñ–∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é corr).\n",
    "    \"\"\"\n",
    "    if pq is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è parquet.\")\n",
    "\n",
    "    # –ß–∏—Ç–∞—î–º–æ –ë–ï–ó —Ñ—ñ–ª—å—Ç—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫ ‚Äî —Ç–∞–∫ –Ω–∞–¥—ñ–π–Ω—ñ—à–µ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ö–µ–º\n",
    "    table = pq.read_table(path)\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —ñ–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –ø–æ—à—É–∫—É\n",
    "    name_map = {c: c.strip().lower() for c in df.columns}\n",
    "    inv_map = {v: k for k, v in name_map.items()}\n",
    "\n",
    "    has_api    = (\"x_ticker\" in inv_map) and (\"y_ticker\" in inv_map)\n",
    "    has_direct = (\"ticker\" in inv_map) and (\"bench\" in inv_map)\n",
    "\n",
    "    if not (has_api or has_direct):\n",
    "        # –¥–æ–ø–æ–º—ñ–∂–Ω–∞ –ø—ñ–¥–∫–∞–∑–∫–∞, —â–æ —Å–∞–º–µ —î –≤ —Ñ–∞–π–ª—ñ\n",
    "        available = \", \".join(df.columns.astype(str).tolist())\n",
    "        raise KeyError(\n",
    "            \"Mapping parquet –ø–æ–≤–∏–Ω–µ–Ω –º—ñ—Å—Ç–∏—Ç–∏ –∞–±–æ ['x_ticker','y_ticker','corr','beta'], \"\n",
    "            \"–∞–±–æ ['ticker','bench','corr','beta'].\\n\"\n",
    "            f\"–£ —Ñ–∞–π–ª—ñ –Ω–∞—è–≤–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {available}\"\n",
    "        )\n",
    "\n",
    "    if has_api:\n",
    "        df = df.rename(columns={\n",
    "            inv_map[\"x_ticker\"]: \"ticker\",\n",
    "            inv_map[\"y_ticker\"]: \"bench\"\n",
    "        })\n",
    "    else:\n",
    "        df = df.rename(columns={\n",
    "            inv_map[\"ticker\"]: \"ticker\",\n",
    "            inv_map[\"bench\"]:  \"bench\"\n",
    "        })\n",
    "\n",
    "    # corr/beta –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ ‚Äî —Å—Ç–≤–æ—Ä–∏–º–æ\n",
    "    if \"corr\" not in name_map.values():\n",
    "        df[\"corr\"] = pd.NA\n",
    "    else:\n",
    "        df = df.rename(columns={inv_map[\"corr\"]: \"corr\"})\n",
    "    if \"beta\" not in name_map.values():\n",
    "        df[\"beta\"] = pd.NA\n",
    "    else:\n",
    "        df = df.rename(columns={inv_map[\"beta\"]: \"beta\"})\n",
    "\n",
    "    # –∑–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–µ\n",
    "    keep = [c for c in [\"ticker\", \"bench\", \"corr\", \"beta\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    # –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–Ω–∞—á–µ–Ω—å\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "    df[\"bench\"]  = df[\"bench\"].astype(str).str.strip().str.upper()\n",
    "    if \"corr\" in df.columns:\n",
    "        df[\"corr\"] = pd.to_numeric(df[\"corr\"], errors=\"coerce\")\n",
    "    if \"beta\" in df.columns:\n",
    "        df[\"beta\"] = pd.to_numeric(df[\"beta\"], errors=\"coerce\")\n",
    "\n",
    "    # —è–∫—â–æ —Ä–∞–ø—Ç–æ–º –Ω–∞ –æ–¥–∏–Ω —Ç–∏–∫–µ—Ä –∫—ñ–ª—å–∫–∞ —Ä—è–¥–∫—ñ–≤ ‚Äî –≤—ñ–∑—å–º–µ–º–æ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é corr\n",
    "    if \"corr\" in df.columns:\n",
    "        df = (df.sort_values([\"ticker\", \"corr\"], ascending=[True, False])\n",
    "                .drop_duplicates(subset=[\"ticker\"], keep=\"first\")\n",
    "                .reset_index(drop=True))\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"ticker\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    # –≥–∞—Ä–∞–Ω—Ç—É—î–º–æ –ø–æ–≤–Ω–∏–π –Ω–∞–±—ñ—Ä —Å—Ç–æ–≤–ø—Ü—ñ–≤\n",
    "    for c in (\"corr\", \"beta\"):\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    return df[[\"ticker\", \"bench\", \"corr\", \"beta\"]]\n",
    "\n",
    "\n",
    "def attach_bench_corr_beta_parquet(\n",
    "    input_parquet_path: str,\n",
    "    corrbeta_parquet_path: str,\n",
    "    out_parquet_path: str,\n",
    "    *,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    log_every_batches: int = 5,\n",
    "    round_decimals: Optional[int] = 4,\n",
    "    insert_before: str = \"pct_vs_prev_close\",\n",
    "    compression: str = \"snappy\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ü–æ—Ç–æ–∫–æ–≤–æ —á–∏—Ç–∞—î –≤–µ–ª–∏–∫–∏–π Parquet (—ñ–Ω—Ç—Ä–∞–¥–µ—ó), –º–µ—Ä–¥–∂–∏—Ç—å 'bench','corr','beta' –∑ —ñ–Ω—à–æ–≥–æ\n",
    "    Parquet (–º–∞–ø–∞) –ø–æ 'ticker' —ñ –ø–∏—à–µ –Ω–æ–≤–∏–π Parquet.\n",
    "    \"\"\"\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pa, pq) –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ Parquet.\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    mapping = _load_corrbeta_mapping_parquet(corrbeta_parquet_path)\n",
    "    if round_decimals is not None:\n",
    "        mapping[\"corr\"] = mapping[\"corr\"].round(round_decimals)\n",
    "        mapping[\"beta\"] = mapping[\"beta\"].round(round_decimals)\n",
    "\n",
    "    if os.path.exists(out_parquet_path):\n",
    "        os.remove(out_parquet_path)\n",
    "\n",
    "    pf = pq.ParquetFile(input_parquet_path)\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    batches = 0\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START merge ‚Üí {out_parquet_path}\")\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=batch_rows):  # —á–∏—Ç–∞—î–º–æ –≤—Å—ñ –∫–æ–ª–æ–Ω–∫–∏\n",
    "        batches += 1\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        if \"ticker\" not in df.columns:\n",
    "            raise KeyError(\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É parquet –≤—ñ–¥—Å—É—Ç–Ω—ñ–π —Å—Ç–æ–≤–ø–µ—Ü—å 'ticker'.\")\n",
    "        df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "        merged = df.merge(mapping, on=\"ticker\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "        # –ü–µ—Ä–µ—Å—Ç–∞–≤–∏–º–æ –Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏\n",
    "        if insert_before in merged.columns:\n",
    "            cols = list(merged.columns)\n",
    "            for c in (\"bench\", \"corr\", \"beta\"):\n",
    "                if c in cols: cols.remove(c)\n",
    "            idx = cols.index(insert_before)\n",
    "            new_cols = cols[:idx] + [\"bench\", \"corr\", \"beta\"] + cols[idx:]\n",
    "            merged = merged[new_cols]\n",
    "        else:\n",
    "            base = [c for c in merged.columns if c not in (\"bench\",\"corr\",\"beta\")]\n",
    "            merged = merged[base + [\"bench\",\"corr\",\"beta\"]]\n",
    "\n",
    "        table = pa.Table.from_pandas(merged, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_parquet_path, table.schema, compression=compression)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        total_rows += len(merged)\n",
    "        if batches % log_every_batches == 0:\n",
    "            print(f\"[batch {batches:>4}] rows={total_rows:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, merged, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE rows={total_rows:,} ‚Üí {out_parquet_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return out_parquet_path\n",
    "\n",
    "\n",
    "# –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π –∞–ª—ñ–∞—Å –ø—ñ–¥ —Å—Ç–∞—Ä–µ —ñ–º‚Äô—è (—â–æ–± —ñ—Å–Ω—É—é—á–∏–π –∫–æ–¥ –Ω–µ –º—ñ–Ω—è—Ç–∏)\n",
    "def attach_bench_corr_beta_file(**kwargs) -> str:\n",
    "    return attach_bench_corr_beta_parquet(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acad0cc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = attach_bench_corr_beta_parquet(\n",
    "    input_parquet_path=str(WORK_DIR / \"premarket_rows_enriched_with_pct.parquet\"),\n",
    "    corrbeta_parquet_path=str(WORK_DIR / \"corr_beta_pairs.parquet\"),\n",
    "    out_parquet_path=str(WORK_DIR / \"premarket_rows_final.parquet\"),\n",
    "    batch_rows=2_000_000,\n",
    "    round_decimals=4,\n",
    "    insert_before=\"pct_vs_prev_close\",\n",
    "    compression=\"snappy\",\n",
    ")\n",
    "print(\"–ì–æ—Ç–æ–≤–æ:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02938414",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def peek_parquet(path: str, n: int = 20, columns=None):\n",
    "    \"\"\"\n",
    "    –õ–µ–≥–∫–∏–π –ø–µ—Ä–µ–≥–ª—è–¥ –≤–µ–ª–∏–∫–æ–≥–æ Parquet:\n",
    "      - –¥—Ä—É–∫—É—î —à–ª—è—Ö, –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ (—ñ–∑ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö), –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫ —ñ —ó—Ö –Ω–∞–∑–≤–∏\n",
    "      - –ø–æ—Ç–æ–∫–æ–≤–æ —á–∏—Ç–∞—î —ñ –ø–æ–∫–∞–∑—É—î –ø–µ—Ä—à—ñ n —Ä—è–¥–∫—ñ–≤ (optionally –ª–∏—à–µ –≤–∫–∞–∑–∞–Ω—ñ columns)\n",
    "      - –ø–æ–≤–µ—Ä—Ç–∞—î DataFrame —ñ–∑ –ø—Ä–µ–≤‚Äô—é (n —Ä—è–¥–∫—ñ–≤)\n",
    "    –í–∏–º–∞–≥–∞—î pyarrow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pyarrow.parquet as pq\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"–ü–æ—Ç—Ä—ñ–±–µ–Ω –ø–∞–∫–µ—Ç 'pyarrow' –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è parquet. –í—Å—Ç–∞–Ω–æ–≤–∏: pip install pyarrow\"\n",
    "        ) from e\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    pf = pq.ParquetFile(path)\n",
    "    total_rows = pf.metadata.num_rows if pf.metadata is not None else None\n",
    "    schema_cols = [f.name for f in pf.schema_arrow]\n",
    "    print(f\"–§–∞–π–ª: {path}\")\n",
    "    if total_rows is not None:\n",
    "        print(f\"–†—è–¥–∫—ñ–≤ (meta): {total_rows:,}\")\n",
    "    print(f\"–ö–æ–ª–æ–Ω–æ–∫: {len(schema_cols)}\")\n",
    "    print(\"–°—Ö–µ–º–∞:\", \", \".join(schema_cols))\n",
    "\n",
    "    # –ó–±–∏—Ä–∞—î–º–æ –ø–µ—Ä—à—ñ n —Ä—è–¥–∫—ñ–≤ –ø–æ—Ç–æ–∫–æ–≤–æ\n",
    "    need = n\n",
    "    parts = []\n",
    "    for batch in pf.iter_batches(batch_size=min(100_000, max(1, n)), columns=columns):\n",
    "        df_part = batch.to_pandas()\n",
    "        parts.append(df_part)\n",
    "        need -= len(df_part)\n",
    "        if need <= 0:\n",
    "            break\n",
    "\n",
    "    head_df = pd.concat(parts, ignore_index=True).head(n) if parts else pd.DataFrame()\n",
    "    # –ö–æ—Ä–æ—Ç–∫–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ –∫–ª—é—á–æ–≤–∏—Ö –Ω–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—è—Ö (—è–∫—â–æ —î)\n",
    "    for c in [\"bench\", \"corr\", \"beta\", \"pct_vs_prev_close\"]:\n",
    "        if c in head_df.columns:\n",
    "            na = int(head_df[c].isna().sum())\n",
    "            print(f\"NaN —É '{c}' (—É –ø—Ä–µ–≤'—é): {na}/{len(head_df)}\")\n",
    "\n",
    "    print(\"\\n–ü–µ—Ä—à—ñ —Ä—è–¥–∫–∏:\")\n",
    "    with pd.option_context(\"display.max_rows\", n, \"display.max_columns\", 200, \"display.width\", 200):\n",
    "        print(head_df)\n",
    "\n",
    "    return head_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901c6ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peek_parquet(\"CRACEN/work/premarket_rows_final.parquet\", n=620)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d359f2a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# —Å–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä—ñ–≤\n",
    "tickers = ['SPY','IWM','GDX','XLF','QQQ','XLE','KRE','BITO','KWEB','SOXL']\n",
    "\n",
    "# —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame\n",
    "df_bench = pd.DataFrame(tickers, columns=['ticker'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add48d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep = [\"ticker\",\"dt\",\"o\",\"h\",\"l\",\"c\",\"v\"]\n",
    "\n",
    "errs = fetch_intraday_v3_for_all_tickers(\n",
    "    tickers_df=df_bench,\n",
    "    start= start,\n",
    "    end= end,\n",
    "    interval=1,\n",
    "    chunk_days=30,\n",
    "    parallel_chunks=True,\n",
    "    max_workers_chunks=6,\n",
    "    parallel_tickers=True,\n",
    "    max_workers_tickers=1,\n",
    "    save_every=50,\n",
    "    out_dir=str(ETF_DIR),\n",
    "    out_prefix=\"intraday\",\n",
    "    gzip=True,\n",
    "    keep_columns=keep,\n",
    "    verbose=True,\n",
    "    include_blue_ocean=True,   # ‚¨ÖÔ∏è –¥–æ–¥–∞–π —Ü–µ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838445a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "collect_premarket_by_day_stream(\n",
    "    folder=str(ETF_DIR),\n",
    "    out_path=str(WORK_DIR / \"premarket_etf_rows.parquet\"),\n",
    "    start_time=\"00:00:00\",\n",
    "    end_time=\"23:59:00\",\n",
    "    aggregate=False,\n",
    "    chunksize=150_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251cdde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_oc = build_oc_from_intraday_folder(\n",
    "    folder=str(ETF_DIR),\n",
    "    out_path=str(WORK_DIR / \"oc_etf.parquet\"),\n",
    "    chunksize=250_000\n",
    ")\n",
    "print(df_oc.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2eb47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pq = None\n",
    "\n",
    "\n",
    "def enrich_with_day_open_and_prev_close(df_intraday: pd.DataFrame,\n",
    "                                        df_oc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –î–æ–¥–∞—î –¥–æ –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞ —ñ–Ω—Ç—Ä–∞–¥–µ—é 'open' –¥–Ω—è —Ç–∞ 'prev_close' (—É—á–æ—Ä–∞—à–Ω—ñ–π close).\n",
    "    –û—á—ñ–∫—É—î –≤ df_intraday: ['ticker','dt',...]\n",
    "              –≤ df_oc:       ['ticker','date','open','close']\n",
    "    \"\"\"\n",
    "    out = df_intraday.copy()\n",
    "    out[\"dt\"] = pd.to_datetime(out[\"dt\"], errors=\"coerce\")\n",
    "    out = out[out[\"dt\"].notna()].copy()\n",
    "    out[\"date\"] = out[\"dt\"].dt.date.astype(\"string\")\n",
    "\n",
    "    oc = df_oc.copy()\n",
    "    oc[\"date\"] = pd.to_datetime(oc[\"date\"], errors=\"coerce\").dt.date.astype(\"string\")\n",
    "\n",
    "    oc_sorted = (\n",
    "        oc.sort_values([\"ticker\", \"date\"])\n",
    "          .groupby(\"ticker\", as_index=False, group_keys=False)\n",
    "          .apply(lambda d: d.assign(prev_close=d[\"close\"].shift(1)))\n",
    "    )\n",
    "\n",
    "    right = oc_sorted[[\"ticker\", \"date\", \"open\", \"prev_close\"]].copy()\n",
    "    out = out.merge(right, on=[\"ticker\", \"date\"], how=\"left\", validate=\"m:1\")\n",
    "    out = out.sort_values([\"ticker\", \"dt\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_intraday_and_oc_to_file(\n",
    "    intraday_parquet_path: str,\n",
    "    oc_parquet_path: str,\n",
    "    out_path: str,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    log_every_batches: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    –ß–∏—Ç–∞—î –≤–µ–ª–∏–∫–∏–π Parquet –±–∞—Ç—á–∞–º–∏ (—á–µ—Ä–µ–∑ pyarrow.parquet.ParquetFile.iter_batches),\n",
    "    –º–µ—Ä–¥–∂–∏—Ç—å —ñ–∑ –¥–µ–Ω–Ω–∏–º–∏ open/prev_close –∑ oc.parquet —ñ –ø–∏—à–µ —É out_path\n",
    "    (.parquet –∞–±–æ .csv/.csv.gz).\n",
    "\n",
    "    intraday_parquet_path  ‚Äî —à–ª—è—Ö –¥–æ CRACEN/premarket_rows.parquet\n",
    "    oc_parquet_path        ‚Äî —à–ª—è—Ö –¥–æ CRACEN/oc.parquet\n",
    "    out_path               ‚Äî —à–ª—è—Ö –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (—É —Ç—ñ–π –∂–µ –ø–∞–ø—Ü—ñ)\n",
    "    \"\"\"\n",
    "    if pq is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow.parquet).\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) –î–µ–Ω–Ω–∏–π OC (—É –ø–∞–º'—è—Ç—ñ)\n",
    "    df_oc = pd.read_parquet(oc_parquet_path)\n",
    "    req_oc = {\"ticker\", \"date\", \"open\", \"close\"}\n",
    "    missing = req_oc - set(df_oc.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"OC-—Ñ–∞–π–ª –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∫–æ–ª–æ–Ω–æ–∫: {sorted(missing)}\")\n",
    "\n",
    "    # 2) –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    _, ext = os.path.splitext(out_path.lower())\n",
    "    if ext not in [\".csv\", \".gz\", \".parquet\", \".csv.gz\"]:\n",
    "        raise ValueError(\"–ü—ñ–¥—Ç—Ä–∏–º–∞–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏: .csv, .csv.gz, .parquet\")\n",
    "\n",
    "    parquet_writer = None\n",
    "    total_rows = 0\n",
    "    batch_idx = 0\n",
    "\n",
    "    # 3) –ü–æ—Ç–æ–∫–æ–≤–µ —á–∏—Ç–∞–Ω–Ω—è parquet\n",
    "    pf = pq.ParquetFile(intraday_parquet_path)\n",
    "    cols = [\"ticker\", \"dt\", \"o\", \"h\", \"l\", \"c\", \"v\"]\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START: merge ‚Üí {out_path} | batch_rows‚âà{batch_rows:,}\")\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=batch_rows, columns=cols):\n",
    "        batch_idx += 1\n",
    "        df_batch = batch.to_pandas()\n",
    "\n",
    "        df_enriched = enrich_with_day_open_and_prev_close(df_batch, df_oc)\n",
    "\n",
    "        if out_path.endswith(\".parquet\"):\n",
    "            table = pa.Table.from_pandas(df_enriched)\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(out_path, table.schema, compression=\"snappy\")\n",
    "            parquet_writer.write_table(table)\n",
    "        else:\n",
    "            header = not os.path.exists(out_path)\n",
    "            df_enriched.to_csv(\n",
    "                out_path,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "                header=header,\n",
    "                compression=\"infer\" if out_path.endswith(\".gz\") else None,\n",
    "            )\n",
    "\n",
    "        total_rows += len(df_batch)\n",
    "        if batch_idx % log_every_batches == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"[batch {batch_idx:>4}] rows={total_rows:,}  elapsed={elapsed:,.1f}s\")\n",
    "\n",
    "        del df_batch, df_enriched, batch\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer is not None:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE  rows={total_rows:,}  -> {out_path}  in {time.time()-t0:,.1f}s\")\n",
    "\n",
    "\n",
    "def merge_intraday_and_oc_in_folder(\n",
    "    folder: str = \"CRACEN\",\n",
    "    intraday_filename: str = \"premarket_etf_rows.parquet\",\n",
    "    oc_filename: str = \"oc_etf.parquet\",\n",
    "    out_filename: str = \"premarket_etf_rows_enriched.parquet\",\n",
    "    batch_rows: int = 2_000_000,\n",
    "    log_every_batches: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ó—Ä—É—á–Ω–∏–π –æ–±–≥–æ—Ä—Ç–Ω–∏–∫ —Å–∞–º–µ –ø—ñ–¥ —Ç–≤–æ—é –ø–∞–ø–∫—É –∑—ñ —Å–∫—Ä—ñ–Ω—É.\n",
    "    –®—É–∫–∞—î:\n",
    "      - {folder}/{intraday_filename}\n",
    "      - {folder}/{oc_filename}\n",
    "    —ñ –ø–∏—à–µ:\n",
    "      - {folder}/{out_filename}\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î —à–ª—è—Ö –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.\n",
    "    \"\"\"\n",
    "    intraday_path = os.path.join(folder, intraday_filename)\n",
    "    oc_path = os.path.join(folder, oc_filename)\n",
    "    out_path = os.path.join(folder, out_filename)\n",
    "\n",
    "    if not os.path.exists(intraday_path):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ç—Ä–∞–¥–µ–π: {intraday_path}\")\n",
    "    if not os.path.exists(oc_path):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ OC-—Ñ–∞–π–ª: {oc_path}\")\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    merge_intraday_and_oc_to_file(\n",
    "        intraday_parquet_path=intraday_path,\n",
    "        oc_parquet_path=oc_path,\n",
    "        out_path=out_path,\n",
    "        batch_rows=batch_rows,\n",
    "        log_every_batches=log_every_batches,\n",
    "    )\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7cabf2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = merge_intraday_and_oc_in_folder(folder=str(WORK_DIR))\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–∞–π–ª—ñ:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f3ec5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pq = None\n",
    "\n",
    "def add_pct_vs_prev_close_parquet(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    round_to: Optional[int] = None,\n",
    "    log_every: int = 5,\n",
    "    value_col: str = \"o\",   # ‚Üê —â–æ –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ prev_close (–¥–µ—Ñ–æ–ª—Ç: open)\n",
    "    out_col: str = \"Benchk%\",  # –Ω–∞–∑–≤–∞ –∫–æ–ª–æ–Ω–∫–∏ –∑ –≤—ñ–¥—Å–æ—Ç–∫–æ–º\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ü–æ—Ç–æ–∫–æ–≤–æ —á–∏—Ç–∞—î –≤–µ–ª–∏–∫–∏–π Parquet, –¥–æ–¥–∞—î –∫–æ–ª–æ–Ω–∫—É:\n",
    "        out_col = (value_col / prev_close - 1) * 100\n",
    "    —ñ –ø–∏—à–µ –Ω–æ–≤–∏–π Parquet.\n",
    "    \"\"\"\n",
    "    if pq is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow.parquet).\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    pf = pq.ParquetFile(input_path)\n",
    "    parquet_writer = None\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START  file={input_path}  ‚Üí  {output_path}  batch_rows‚âà{batch_rows:,}  using={value_col}\")\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=batch_rows):\n",
    "        batches += 1\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫\n",
    "        if \"prev_close\" not in df.columns:\n",
    "            raise KeyError(\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ 'prev_close'.\")\n",
    "        if value_col not in df.columns:\n",
    "            raise KeyError(f\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ '{value_col}'.\")\n",
    "\n",
    "        # —á–∏—Å–ª–æ–≤—ñ —Ç–∏–ø–∏\n",
    "        df[\"prev_close\"] = pd.to_numeric(df[\"prev_close\"], errors=\"coerce\")\n",
    "        df[value_col]    = pd.to_numeric(df[value_col],    errors=\"coerce\")\n",
    "\n",
    "        denom = df[\"prev_close\"].replace({0: pd.NA})\n",
    "        pct = (df[value_col] / denom - 1.0) * 100.0\n",
    "        if round_to is not None:\n",
    "            pct = pct.round(round_to)\n",
    "\n",
    "        df[out_col] = pct\n",
    "\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(output_path, table.schema, compression=\"snappy\")\n",
    "        parquet_writer.write_table(table)\n",
    "\n",
    "        total += len(df)\n",
    "        if batches % log_every == 0:\n",
    "            print(f\"[batch {batches:>4}] rows={total:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer is not None:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE  rows={total:,}  ‚Üí  {output_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def add_pct_vs_prev_close_in_folder(\n",
    "    folder: str = \"CRACEN\",\n",
    "    input_filename: str = \"premarket_etf_rows_enriched.parquet\",\n",
    "    output_filename: str = \"premarket_etf_rows_enriched_with_pct.parquet\",\n",
    "    batch_rows: int = 2_000_000,\n",
    "    round_to: Optional[int] = 2,\n",
    "    log_every: int = 5,\n",
    "    value_col: str = \"o\",                 # ‚Üê open vs prev_close\n",
    "    out_col: str = \"Bench%\",   # –Ω–∞–∑–≤–∞ –Ω–æ–≤–æ—ó –∫–æ–ª–æ–Ω–∫–∏\n",
    ") -> str:\n",
    "    \"\"\"–û–±–≥–æ—Ä—Ç–∫–∞ –ø—ñ–¥ —Ç–≤–æ—é –ø–∞–ø–∫—É CRACEN.\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    src = os.path.join(folder, input_filename)\n",
    "    dst = os.path.join(folder, output_filename)\n",
    "    if not os.path.exists(src):\n",
    "        raise FileNotFoundError(f\"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª: {src}\")\n",
    "    return add_pct_vs_prev_close_parquet(\n",
    "        input_path=src,\n",
    "        output_path=dst,\n",
    "        batch_rows=batch_rows,\n",
    "        round_to=round_to,\n",
    "        log_every=log_every,\n",
    "        value_col=value_col,\n",
    "        out_col=out_col,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cad246",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = add_pct_vs_prev_close_in_folder(folder=str(WORK_DIR))\n",
    "print(\"–ì–æ—Ç–æ–≤–æ:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5d9f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gc, time\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception as e:\n",
    "    pa = pq = None\n",
    "\n",
    "\n",
    "def merge_bench_pct_from_etf(\n",
    "    base_path: str = \"CRACEN/work/premarket_rows_enriched_with_pct.parquet\",\n",
    "    etf_path:  str = \"CRACEN/work/premarket_etf_rows_enriched_with_pct.parquet\",\n",
    "    out_path:  str = \"CRACEN/work/premarket_rows_with_bench_pct.parquet\",\n",
    "    *,\n",
    "    batch_rows: int = 2_000_000,\n",
    "    align_to_minute: bool = False,        # —è–∫—â–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è ‚Äî –ø—ñ–¥—Ä—ñ–≤–Ω—è—Ç–∏ –¥–æ —Ö–≤–∏–ª–∏–Ω–∏\n",
    "    round_decimals: Optional[int] = 2,    # –æ–∫—Ä—É–≥–ª–∏—Ç–∏ Bench% –¥–æ 2 –∑–Ω–∞–∫—ñ–≤\n",
    "    insert_before: Optional[str] = None,  # –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, \"pct_vs_prev_close\"; —ñ–Ω–∞–∫—à–µ –≤ –∫—ñ–Ω–µ—Ü—å\n",
    "    compression: str = \"snappy\",\n",
    "    log_every_batches: int = 5,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ú–µ—Ä–¥–∂–∏—Ç—å Bench% —ñ–∑ ETF-—Ñ–∞–π–ª–∞ —É –±–∞–∑–æ–≤–∏–π —Ñ–∞–π–ª –ø–æ –∫–ª—é—á–∞—Ö ('bench'‚Üî'ticker', 'dt') —ñ –ø–∏—à–µ –Ω–æ–≤–∏–π Parquet.\n",
    "    - –£ ETF-—Ñ–∞–π–ª—ñ –∫–æ–ª–æ–Ω–∫–∞ –∑ –≤—ñ–¥—Å–æ—Ç–∫–æ–º –º–æ–∂–µ –Ω–∞–∑–∏–≤–∞—Ç–∏—Å—å 'bench%' –∞–±–æ 'pct_vs_prev_close' ‚Äî –æ–±–∏–¥–≤–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è.\n",
    "    - –ë–∞–∑–æ–≤–∏–π —Ñ–∞–π–ª –Ω–µ –∑–º—ñ–Ω—é—î—Ç—å—Å—è, –¥–æ–¥–∞—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤–∞ –∫–æ–ª–æ–Ω–∫–∞ 'Bench%'.\n",
    "    \"\"\"\n",
    "\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow: pip install pyarrow\")\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        raise FileNotFoundError(base_path)\n",
    "    if not os.path.exists(etf_path):\n",
    "        raise FileNotFoundError(etf_path)\n",
    "\n",
    "    # ---------- 1) –ó–±–∏—Ä–∞—î–º–æ –º–∞–ø—É –∑ ETF-—Ñ–∞–π–ª–∞: (bench, dt) -> Bench% ----------\n",
    "    def _read_etf_mapping(etf_parquet: str) -> pd.DataFrame:\n",
    "        pf = pq.ParquetFile(etf_parquet)\n",
    "        parts = []\n",
    "        # —á–∏—Ç–∞—î–º–æ –≤—Å—ñ –∫–æ–ª–æ–Ω–∫–∏, —â–æ–± –Ω–µ –ª–æ–≤–∏—Ç–∏ —Ä—ñ–∑–Ω—ñ —Å—Ö–µ–º–∏ —É row-groups\n",
    "        for batch in pf.iter_batches():\n",
    "            df = batch.to_pandas()\n",
    "            # –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ\n",
    "            for col in (\"ticker\", \"dt\"):\n",
    "                if col not in df.columns:\n",
    "                    raise KeyError(f\"–£ ETF-—Ñ–∞–π–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—è –∫–æ–ª–æ–Ω–∫–∞ '{col}'\")\n",
    "\n",
    "            # –∫–æ–ª–æ–Ω–∫–∞ –≤—ñ–¥—Å–æ—Ç–∫–∞: bench% –∞–±–æ pct_vs_prev_close\n",
    "            cols_l = {c.lower().strip(): c for c in df.columns}\n",
    "            if \"bench%\" in cols_l:\n",
    "                val_col = cols_l[\"bench%\"]\n",
    "            elif \"pct_vs_prev_close\" in cols_l:\n",
    "                val_col = cols_l[\"pct_vs_prev_close\"]\n",
    "            else:\n",
    "                raise KeyError(\"–£ ETF-—Ñ–∞–π–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ 'bench%' –∞–±–æ 'pct_vs_prev_close'.\")\n",
    "\n",
    "            d = df[[\"ticker\", \"dt\", val_col]].copy()\n",
    "            d[\"ticker\"] = d[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "            d[\"dt\"]     = pd.to_datetime(d[\"dt\"], errors=\"coerce\")\n",
    "            if align_to_minute:\n",
    "                d[\"dt\"] = d[\"dt\"].dt.floor(\"T\")\n",
    "            d = d.dropna(subset=[\"dt\"])\n",
    "            d = d.rename(columns={\"ticker\": \"bench\", val_col: \"Bench%\"})\n",
    "            d[\"Bench%\"] = pd.to_numeric(d[\"Bench%\"], errors=\"coerce\")\n",
    "            parts.append(d)\n",
    "\n",
    "            del df, d, batch\n",
    "            gc.collect()\n",
    "\n",
    "        if not parts:\n",
    "            return pd.DataFrame(columns=[\"bench\",\"dt\",\"Bench%\"])\n",
    "\n",
    "        etf_map = pd.concat(parts, ignore_index=True)\n",
    "        # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ ‚Äî –∑–∞–ª–∏—à–∞—î–º–æ –ø—ñ–∑–Ω—ñ—à–∏–π –∑–∞–ø–∏—Å\n",
    "        etf_map = (etf_map.sort_values([\"bench\",\"dt\"])\n",
    "                           .drop_duplicates(subset=[\"bench\",\"dt\"], keep=\"last\")\n",
    "                           .reset_index(drop=True))\n",
    "        return etf_map\n",
    "\n",
    "    etf_map = _read_etf_mapping(etf_path)\n",
    "    if round_decimals is not None and \"Bench%\" in etf_map.columns:\n",
    "        etf_map[\"Bench%\"] = etf_map[\"Bench%\"].round(round_decimals)\n",
    "\n",
    "    # ---------- 2) –ü–æ—Ç–æ–∫–æ–≤–æ –ø—Ä–æ—Ö–æ–¥–∏–º–æ –±–∞–∑–æ–≤–∏–π —Ñ–∞–π–ª —ñ –ø—ñ–¥–º–µ—Ä–¥–∂—É—î–º–æ Bench% ----------\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "\n",
    "    pf_base = pq.ParquetFile(base_path)\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START merge ‚Üí {out_path}\")\n",
    "    for i, batch in enumerate(pf_base.iter_batches(batch_size=batch_rows), 1):\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # –∫–ª—é—á—ñ\n",
    "        for col in (\"bench\", \"dt\"):\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"–£ –±–∞–∑–æ–≤–æ–º—É —Ñ–∞–π–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—è –∫–æ–ª–æ–Ω–∫–∞ '{col}'\")\n",
    "\n",
    "        df[\"bench\"] = df[\"bench\"].astype(str).str.strip().str.upper()\n",
    "        df[\"dt\"]    = pd.to_datetime(df[\"dt\"], errors=\"coerce\")\n",
    "        if align_to_minute:\n",
    "            df[\"dt\"] = df[\"dt\"].dt.floor(\"T\")\n",
    "\n",
    "        merged = df.merge(etf_map, on=[\"bench\",\"dt\"], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "        # —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è 'Bench%'\n",
    "        if \"Bench%\" in merged.columns and insert_before and insert_before in merged.columns:\n",
    "            cols = list(merged.columns)\n",
    "            cols.remove(\"Bench%\")\n",
    "            pos = cols.index(insert_before)\n",
    "            cols = cols[:pos] + [\"Bench%\"] + cols[pos:]\n",
    "            merged = merged[cols]\n",
    "        # —ñ–Ω–∞–∫—à–µ ‚Äî –∑–∞–ª–∏—à–∞—î–º–æ –≤ –∫—ñ–Ω—Ü—ñ —è–∫ —î\n",
    "\n",
    "        table = pa.Table.from_pandas(merged, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_path, table.schema, compression=compression)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        total_rows += len(merged)\n",
    "        if i % log_every_batches == 0:\n",
    "            print(f\"[batch {i:>4}] rows={total_rows:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, merged, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"‚úÖ DONE rows={total_rows:,} ‚Üí {out_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb665b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_bench_pct_from_etf(\n",
    "    base_path=str(WORK_DIR / \"premarket_rows_final.parquet\"),\n",
    "    etf_path =str(WORK_DIR / \"premarket_etf_rows_enriched_with_pct.parquet\"),\n",
    "    out_path =str(WORK_DIR / \"premarket_rows_with_bench_pct.parquet\"),\n",
    "    batch_rows=2_000_000,\n",
    "    align_to_minute=False,       # –≤–º–∏–∫–∞–π, —è–∫—â–æ —Å–µ–∫—É–Ω–¥–∏/–º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥–∏ ¬´–ø–ª–∏–≤—É—Ç—å¬ª\n",
    "    round_decimals=2,\n",
    "    insert_before=None,          # –∞–±–æ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, \"pct_vs_prev_close\"\n",
    "    compression=\"snappy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c748eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm   # –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä\n",
    "\n",
    "def get_gaps_for_tickers(tickers_df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    –û—Ç—Ä–∏–º—É—î –≥–µ–ø–∏ –¥–ª—è –≤—Å—ñ—Ö —Ç–∏–∫–µ—Ä—ñ–≤ —ñ–∑ DataFrame tickers_df (–∫–æ–ª–æ–Ω–∫–∞ 'ticker').\n",
    "    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –µ–Ω–¥–ø–æ—ñ–Ω—Ç /daily/gaps —ñ –æ–±‚Äô—î–¥–Ω—É—î –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É –æ–¥–∏–Ω DataFrame.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    tickers = tickers_df[\"ticker\"].dropna().astype(str).unique().tolist()\n",
    "\n",
    "    for t in tqdm(tickers, desc=\"Processing tickers\"):\n",
    "        params = {\n",
    "            \"ticker\": t,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"format\": \"json_records\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            df_gap = DatumApi.data_request(\"/daily/gaps\", params)\n",
    "            if df_gap is not None and not df_gap.empty:\n",
    "                all_results.append(df_gap)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ –∑ {t}: {e}\")\n",
    "\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# === –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è ===\n",
    "df_gaps_all = get_gaps_for_tickers(top1500, start_date = start_date_str, end_date = end_date_str)\n",
    "\n",
    "print(df_gaps_all.head())\n",
    "print(\"–ó–∞–≥–∞–ª–æ–º —Ä—è–¥–∫—ñ–≤:\", len(df_gaps_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ee8f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_by_ticker = (\n",
    "    df_gaps_all.groupby(\"ticker\")[\"gap\"]\n",
    "    .std()                 # std –ø–æ –≥—Ä—É–ø—ñ (ddof=1)\n",
    "    .round(2)              # –æ–∫—Ä—É–≥–ª–µ–Ω–Ω—è –¥–æ 2 –∑–Ω–∞–∫—ñ–≤\n",
    "    .reset_index(name=\"sigma\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26485bd2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_by_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ab24e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception:\n",
    "    pa = pq = None\n",
    "\n",
    "\n",
    "def _prepare_sigma_df(\n",
    "    sigma_df: Optional[pd.DataFrame] = None,\n",
    "    sigma_path: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ì–æ—Ç—É—î –º–∞–ø—É ['ticker','sigma'] –∑ DataFrame –∞–±–æ –∑ —Ñ–∞–π–ª—É (.parquet/.csv).\n",
    "    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î —Ä–µ–≥—ñ—Å—Ç—Ä, —Ç–∏–ø–∏ —ñ –ø—Ä–∏–±–∏—Ä–∞—î –¥—É–±–ª—ñ–∫–∞—Ç–∏ –ø–æ 'ticker'.\n",
    "    \"\"\"\n",
    "    if sigma_df is None and sigma_path is None:\n",
    "        raise ValueError(\"–ü–µ—Ä–µ–¥–∞–π –∞–±–æ sigma_df, –∞–±–æ sigma_path.\")\n",
    "\n",
    "    if sigma_df is None:\n",
    "        ext = os.path.splitext(sigma_path)[1].lower()\n",
    "        if ext == \".parquet\":\n",
    "            sigma_df = pd.read_parquet(sigma_path)\n",
    "        else:\n",
    "            sigma_df = pd.read_csv(sigma_path, low_memory=False)\n",
    "\n",
    "    df = sigma_df.copy()\n",
    "    # –∑–Ω–∞–π–¥–µ–º–æ –∫–æ–ª–æ–Ω–∫–∏ –≥–Ω—É—á–∫–æ\n",
    "    cols_lower = {c.lower().strip(): c for c in df.columns}\n",
    "    tcol = cols_lower.get(\"ticker\")\n",
    "    scol = cols_lower.get(\"sigma\")\n",
    "    if tcol is None or scol is None:\n",
    "        raise KeyError(\"–û—á—ñ–∫—É—é –∫–æ–ª–æ–Ω–∫–∏ 'ticker' —ñ 'sigma' —É –¥–∞–Ω–∏—Ö –∑—ñ —Å–∫—Ä—ñ–Ω–∞.\")\n",
    "\n",
    "    df = df[[tcol, scol]].rename(columns={tcol: \"ticker\", scol: \"sigma\"})\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "    df[\"sigma\"]  = pd.to_numeric(df[\"sigma\"], errors=\"coerce\")\n",
    "    df = (df.sort_values([\"ticker\"])\n",
    "            .drop_duplicates(subset=[\"ticker\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return df[[\"ticker\", \"sigma\"]]\n",
    "\n",
    "\n",
    "def add_sigma_by_ticker_to_parquet(\n",
    "    base_path: str = \"CRACEN/work/premarket_rows_with_bench_pct.parquet\",\n",
    "    out_path:  Optional[str] = None,\n",
    "    *,\n",
    "    sigma_df: Optional[pd.DataFrame] = None,   # –Ω–∞–ø—Ä. —Ç–≤—ñ–π std_by_ticker –∑—ñ —Å–∫—Ä—ñ–Ω–∞\n",
    "    sigma_path: Optional[str] = None,          # –∞–±–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª–∞ –∑ —Ü–∏–º–∏ –¥–∞–Ω–∏–º–∏\n",
    "    batch_rows: int = 2_000_000,\n",
    "    compression: str = \"snappy\",\n",
    "    insert_before: Optional[str] = None,       # –Ω–∞–ø—Ä. \"Bench%\" –∞–±–æ \"pct_vs_prev_close\"; —è–∫—â–æ None ‚Äî –≤ –∫—ñ–Ω–µ—Ü—å\n",
    "    log_every_batches: int = 5,\n",
    "    round_decimals: Optional[int] = 6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ü–æ—Ç–æ–∫–æ–≤–æ –¥–æ–¥–∞—î –∫–æ–ª–æ–Ω–∫—É 'sigma' (–∑–∞ 'ticker') –¥–æ –≤–µ–ª–∏–∫–æ–≥–æ Parquet —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –Ω–æ–≤–∏–π —Ñ–∞–π–ª.\n",
    "    \"\"\"\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow, pyarrow.parquet).\")\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        raise FileNotFoundError(base_path)\n",
    "\n",
    "    # –≤–∏—Ö—ñ–¥–Ω–∏–π —à–ª—è—Ö\n",
    "    if out_path is None:\n",
    "        folder, fname = os.path.split(base_path)\n",
    "        name, ext = os.path.splitext(fname)\n",
    "        out_path = os.path.join(folder, f\"{name}_with_sigma{ext or '.parquet'}\")\n",
    "\n",
    "    # –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ sigma\n",
    "    sigma_map = _prepare_sigma_df(sigma_df=sigma_df, sigma_path=sigma_path)\n",
    "    if round_decimals is not None:\n",
    "        sigma_map[\"sigma\"] = sigma_map[\"sigma\"].round(round_decimals)\n",
    "\n",
    "    # –∑–∞–ø–∏—Å —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª ‚Üí –∞—Ç–æ–º–∞—Ä–Ω–∞ –∑–∞–º—ñ–Ω–∞\n",
    "    tmp_path = out_path + f\".tmp_{os.getpid()}_{int(time.time())}\"\n",
    "    pf = pq.ParquetFile(base_path)\n",
    "    writer = None\n",
    "    total = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è START merge sigma ‚Üí {out_path}\")\n",
    "    for i, batch in enumerate(pf.iter_batches(batch_size=batch_rows), 1):\n",
    "        df = batch.to_pandas()\n",
    "        if \"ticker\" not in df.columns:\n",
    "            raise KeyError(\"–£ –±–∞–∑–æ–≤–æ–º—É —Ñ–∞–π–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—è –∫–æ–ª–æ–Ω–∫–∞ 'ticker'.\")\n",
    "\n",
    "        df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "        merged = df.merge(sigma_map, on=\"ticker\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "        # —Ä–æ–∑—Ç–∞—à—É–≤–∞—Ç–∏ 'sigma' –∑–∞ –±–∞–∂–∞–Ω–Ω—è–º\n",
    "        if insert_before and insert_before in merged.columns:\n",
    "            cols = list(merged.columns)\n",
    "            cols.remove(\"sigma\")\n",
    "            idx = cols.index(insert_before)\n",
    "            cols = cols[:idx] + [\"sigma\"] + cols[idx:]\n",
    "            merged = merged[cols]\n",
    "        else:\n",
    "            base_cols = [c for c in merged.columns if c != \"sigma\"]\n",
    "            merged = merged[base_cols + [\"sigma\"]]\n",
    "\n",
    "        table = pa.Table.from_pandas(merged, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(tmp_path, table.schema, compression=compression)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        total += len(merged)\n",
    "        if i % log_every_batches == 0:\n",
    "            print(f\"[batch {i:>4}] rows={total:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, merged, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    # –∞—Ç–æ–º–∞—Ä–Ω–∞ –∑–∞–º—ñ–Ω–∞\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    os.replace(tmp_path, out_path)\n",
    "\n",
    "    print(f\"‚úÖ DONE rows={total:,} ‚Üí {out_path}  in {time.time()-t0:,.1f}s\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fdb4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# std_by_ticker –º–∞—î –∫–æ–ª–æ–Ω–∫–∏: ['ticker','sigma']\n",
    "add_sigma_by_ticker_to_parquet(\n",
    "    base_path=str(WORK_DIR / \"premarket_rows_with_bench_pct.parquet\"),\n",
    "    sigma_df=std_by_ticker,  # <- —Ç–≤—ñ–π DF –∑—ñ —Å–∫—Ä—ñ–Ω–∞\n",
    "    # –∞–±–æ sigma_path=\"CRACEN/std_by_ticker.parquet\"/\".csv\"\n",
    "    insert_before=None,      # –∞–±–æ \"Bench%\" / \"pct_vs_prev_close\" ‚Äî –∫—É–¥–∏ –≤—Å—Ç–∞–≤–∏—Ç–∏\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f11a5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peek_parquet(\"CRACEN/work/premarket_rows_with_bench_pct_with_sigma.parquet\", n=21150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a8a5e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow.dataset as ds\n",
    "except Exception:\n",
    "    pa = pq = ds = None\n",
    "\n",
    "\n",
    "def _write_final_sorted_alphabetical(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    *,\n",
    "    compression: str = \"snappy\",\n",
    "    roundtrip_pandas: bool = True,   # –Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–µ –¥–ª—è sort_values\n",
    "    log_every: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rewrites input parquet into output parquet in strict physical order:\n",
    "      ticker A‚ÜíZ, then (date, dt) within each ticker (or dt only).\n",
    "    Uses dataset scanning per ticker to avoid global in-memory sort.\n",
    "    \"\"\"\n",
    "    if ds is None or pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow.dataset (pyarrow>=6).\")\n",
    "\n",
    "    dataset = ds.dataset(input_path, format=\"parquet\")\n",
    "\n",
    "    # Discover columns\n",
    "    schema_names = [f.name for f in dataset.schema]\n",
    "    if \"ticker\" not in schema_names:\n",
    "        raise KeyError(\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É parquet –Ω–µ–º–∞—î –∫–æ–ª–æ–Ω–∫–∏ 'ticker' ‚Äî –Ω–µ–º–æ–∂–ª–∏–≤–æ –≤—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ —Ç–∏–∫–µ—Ä–∞—Ö.\")\n",
    "\n",
    "    has_date = (\"date\" in schema_names)\n",
    "    has_dt   = (\"dt\" in schema_names)\n",
    "\n",
    "    # get unique tickers (safe, but could be heavy; acceptable for your use-case)\n",
    "    tickers_tbl = dataset.to_table(columns=[\"ticker\"])\n",
    "    tickers = sorted(pd.unique(tickers_tbl.column(\"ticker\").to_pandas().astype(str)))\n",
    "    del tickers_tbl\n",
    "    gc.collect()\n",
    "\n",
    "    tmp_out = output_path + f\".tmp_sorted_{os.getpid()}_{int(time.time())}\"\n",
    "    if os.path.exists(tmp_out):\n",
    "        os.remove(tmp_out)\n",
    "\n",
    "    writer = None\n",
    "    total_written = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"‚ñ∂ SORT: {input_path} ‚Üí {output_path} | tickers={len(tickers):,} | has_date={has_date} has_dt={has_dt}\")\n",
    "\n",
    "    try:\n",
    "        for i, tk in enumerate(tickers, 1):\n",
    "            # Filter rows for this ticker\n",
    "            filt = (ds.field(\"ticker\") == tk)\n",
    "            table = dataset.to_table(filter=filt)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            if roundtrip_pandas:\n",
    "                df = table.to_pandas()\n",
    "                # Ensure date exists if needed\n",
    "                if has_date:\n",
    "                    # keep as-is; assume date is string 'YYYY-MM-DD'\n",
    "                    pass\n",
    "                elif has_dt and (\"date\" not in df.columns):\n",
    "                    # If no date column exists, optionally derive it (not required for sort)\n",
    "                    pass\n",
    "\n",
    "                if has_date and has_dt and {\"date\", \"dt\"}.issubset(df.columns):\n",
    "                    df.sort_values([\"date\", \"dt\"], kind=\"mergesort\", inplace=True)\n",
    "                elif has_dt and \"dt\" in df.columns:\n",
    "                    df.sort_values([\"dt\"], kind=\"mergesort\", inplace=True)\n",
    "                else:\n",
    "                    # fallback: stable sort by all columns\n",
    "                    df.sort_values(list(df.columns), kind=\"mergesort\", inplace=True)\n",
    "\n",
    "                out_table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                del df\n",
    "            else:\n",
    "                # Arrow-only fallback (less flexible)\n",
    "                out_table = table\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(tmp_out, out_table.schema, compression=compression)\n",
    "\n",
    "            writer.write_table(out_table)\n",
    "            total_written += out_table.num_rows\n",
    "\n",
    "            if (i % log_every) == 0:\n",
    "                print(f\"  [{i:>6}/{len(tickers):,}] {tk}  written={total_written:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "            del table, out_table\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "    # atomic replace\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    os.replace(tmp_out, output_path)\n",
    "\n",
    "    print(f\"‚úÖ SORT DONE: {total_written:,} rows ‚Üí {output_path} | time={time.time()-t0:,.1f}s\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def recompute_tp_deviation_devsig(\n",
    "    input_path: str = \"CRACEN/work/premarket_rows_with_bench_pct_with_sigma.parquet\",\n",
    "    output_path: str = \"CRACEN/work/premarket_rows_with_bench_pct_with_sigma_v2.parquet\",\n",
    "    chunksize: int = 2_000_000,   # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ batch_rows –¥–ª—è Parquet\n",
    "    round_tp_dev: Optional[int] = 2,\n",
    "    round_devsig: Optional[int] = 6,\n",
    "    compression: str = \"snappy\",\n",
    "    *,\n",
    "    sort_final_by_ticker: bool = True,      # NEW: rewrite final sorted\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Parquet-–≤–µ—Ä—Å—ñ—è.\n",
    "    –ü–µ—Ä–µ—Ä–∞—Ö—É–Ω–æ–∫:\n",
    "      - t_p       = stack% * beta\n",
    "      - deviation = t_p - bench%\n",
    "      - dev_sig   = deviation / sigma  (0 -> NaN)\n",
    "    –°—Ç–∏—Ä–∞—î —Å—Ç–∞—Ä—ñ t_p / deviation / dev_sig, —è–∫—â–æ –±—É–ª–∏.\n",
    "    –ü—Ä–∞—Ü—é—î –ø–æ—Ç–æ–∫–æ–≤–æ.\n",
    "\n",
    "    NEW:\n",
    "      - —è–∫—â–æ sort_final_by_ticker=True, —Ç–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π output_path –±—É–¥–µ —Ñ—ñ–∑–∏—á–Ω–æ\n",
    "        –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π: ticker A‚ÜíZ, –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ —Ç–∏–∫–µ—Ä–∞ date,dt (–∞–±–æ dt).\n",
    "    \"\"\"\n",
    "    if pq is None or pa is None:\n",
    "        raise RuntimeError(\"–ü–æ—Ç—Ä—ñ–±–µ–Ω pyarrow (pyarrow, pyarrow.parquet).\")\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(input_path)\n",
    "\n",
    "    t0 = time.time()\n",
    "    batch_rows = chunksize\n",
    "\n",
    "    # 1) write unsorted tmp parquet\n",
    "    tmp_unsorted = output_path + f\".tmp_unsorted_{os.getpid()}_{int(time.time())}\"\n",
    "    if os.path.exists(tmp_unsorted):\n",
    "        os.remove(tmp_unsorted)\n",
    "\n",
    "    pf = pq.ParquetFile(input_path)\n",
    "    writer = None\n",
    "    total = 0\n",
    "\n",
    "    print(f\"‚ñ∂ START (Parquet): {input_path}  ‚Üí  {output_path} | batch_rows‚âà{batch_rows:,}\")\n",
    "\n",
    "    for bi, batch in enumerate(pf.iter_batches(batch_size=batch_rows), 1):\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # –≥–Ω—É—á–∫–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫\n",
    "        names = {c.lower().strip(): c for c in df.columns}\n",
    "        stack_col = names.get(\"stack%\") or names.get(\"pct_vs_prev_close\")\n",
    "        bench_col = names.get(\"bench%\")\n",
    "        beta_col  = names.get(\"beta\")\n",
    "        sigma_col = names.get(\"sigma\")\n",
    "\n",
    "        need = {\n",
    "            \"stack%/pct_vs_prev_close\": stack_col,\n",
    "            \"bench%\": bench_col,\n",
    "            \"beta\": beta_col,\n",
    "            \"sigma\": sigma_col,\n",
    "        }\n",
    "        missing = [k for k, v in need.items() if v is None]\n",
    "        if missing:\n",
    "            raise KeyError(f\"–£ –≤—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –±—Ä–∞–∫—É—î –∫–æ–ª–æ–Ω–æ–∫: {missing}\")\n",
    "\n",
    "        # –ø—Ä–∏–±–µ—Ä–µ–º–æ —Å—Ç–∞—Ä—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, —è–∫—â–æ –±—É–ª–∏\n",
    "        for col in (\"t_p\", \"deviation\", \"dev_sig\"):\n",
    "            if col in df.columns:\n",
    "                del df[col]\n",
    "\n",
    "        # —á–∏—Å–ª–æ–≤—ñ —Ç–∏–ø–∏\n",
    "        df[stack_col] = pd.to_numeric(df[stack_col], errors=\"coerce\")\n",
    "        df[bench_col] = pd.to_numeric(df[bench_col], errors=\"coerce\")\n",
    "        df[beta_col]  = pd.to_numeric(df[beta_col],  errors=\"coerce\")\n",
    "        df[sigma_col] = pd.to_numeric(df[sigma_col], errors=\"coerce\")\n",
    "\n",
    "        # –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\n",
    "        tp  = df[stack_col] * df[beta_col]\n",
    "        dev = tp - df[bench_col]\n",
    "        denom = df[sigma_col].replace({0: np.nan})\n",
    "        dsg = dev / denom\n",
    "\n",
    "        # –æ–∫—Ä—É–≥–ª–µ–Ω–Ω—è\n",
    "        if round_tp_dev is not None:\n",
    "            tp  = tp.round(round_tp_dev)\n",
    "            dev = dev.round(round_tp_dev)\n",
    "        if round_devsig is not None:\n",
    "            dsg = dsg.round(round_devsig)\n",
    "\n",
    "        # –¥–æ–¥–∞—Ç–∏ –≤ –∫—ñ–Ω–µ—Ü—å\n",
    "        df[\"t_p\"] = tp\n",
    "        df[\"deviation\"] = dev\n",
    "        df[\"dev_sig\"] = dsg\n",
    "\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(tmp_unsorted, table.schema, compression=compression)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        total += len(df)\n",
    "        if bi % 5 == 0:\n",
    "            print(f\"[batch {bi:>4}] rows={total:,}  elapsed={time.time()-t0:,.1f}s\")\n",
    "\n",
    "        del df, batch, table\n",
    "        gc.collect()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    # 2) sort & finalize\n",
    "    if sort_final_by_ticker:\n",
    "        # rewrite sorted to output_path\n",
    "        _write_final_sorted_alphabetical(\n",
    "            tmp_unsorted,\n",
    "            output_path,\n",
    "            compression=compression,\n",
    "            roundtrip_pandas=True,\n",
    "            log_every=200,\n",
    "        )\n",
    "        # cleanup unsorted tmp\n",
    "        try:\n",
    "            os.remove(tmp_unsorted)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        # atomic replace unsorted as final\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        os.replace(tmp_unsorted, output_path)\n",
    "\n",
    "    print(f\"‚úÖ DONE: {total:,} rows ‚Üí {output_path} | total_time={time.time()-t0:,.1f}s\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8bb909",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recompute_tp_deviation_devsig(\n",
    "    input_path=str(WORK_DIR / \"premarket_rows_with_bench_pct_with_sigma.parquet\"),\n",
    "    output_path=str(FINAL_PATH),\n",
    "    chunksize=2_000_000,\n",
    "    round_tp_dev=2,\n",
    "    round_devsig=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c0fbe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "path = str(FINAL_PATH)\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "\n",
    "dataset = ds.dataset(path, format=\"parquet\")\n",
    "\n",
    "table = dataset.to_table(\n",
    "    filter=(ds.field(\"ticker\") == ticker)\n",
    ")\n",
    "\n",
    "df = table.to_pandas()\n",
    "\n",
    "# —è–∫—â–æ —î date + dt ‚Äî –≤—ñ–¥—Å–æ—Ä—Ç—É—î–º–æ, —â–æ–± —Ü–µ –±—É–ª–∏ —Å–∞–º–µ \"–ø–µ—Ä—à—ñ\" –ø–æ —á–∞—Å—É\n",
    "if {\"date\", \"dt\"}.issubset(df.columns):\n",
    "    df = df.sort_values([\"date\", \"dt\"])\n",
    "\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.243931,
   "end_time": "2026-01-27T21:14:12.922043",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\STRATEGIES\\notebooks\\CRACEN.ipynb",
   "output_path": "C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\status\\last_CRACEN_out.ipynb",
   "parameters": {
    "output_dir": "C:\\Users\\sergi\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ORION_MAIN\\OriON\\signals",
    "run_date": "2026-01-27"
   },
   "start_time": "2026-01-27T21:13:35.678112",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}